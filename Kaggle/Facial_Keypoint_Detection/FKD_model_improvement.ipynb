{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FKD_model_improvement",
      "provenance": [],
      "authorship_tag": "ABX9TyN+u8fzhbZz/j20o4CsomyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangminAhn21/DL_Study/blob/main/Kaggle/Facial_Keypoint_Detection/FKD_model_improvement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg40qszLY2lv",
        "outputId": "e3628c2b-9da1-4ba8-d2e8-bbf60f3e1d78"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bU35sIg4ZBgE",
        "outputId": "923539a1-19e2-473d-ec37-6af81ed0d314"
      },
      "source": [
        "%cd drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS1GVPTjZNyh",
        "outputId": "ef208911-c553-4e96-9241-095c35097bb9"
      },
      "source": [
        "pip install kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81CEmc7uZabl"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "training_pd = pd.read_csv('training.csv')\n",
        "test_pd = pd.read_csv('test.csv')\n",
        "\n",
        "training_pd = training_pd.dropna()\n",
        "# training_pd = training_pd.fillna(method='ffill')\n",
        "\n",
        "training = training_pd.to_numpy()\n",
        "test = test_pd.to_numpy()\n",
        "\n",
        "train_image = training[:, -1]\n",
        "train_key = training[:, :-1].astype('float64')\n",
        "test_image = test[:, 1]\n",
        "\n",
        "train_image = np.array([np.array([int(pixel) for pixel in image.split()]).\\\n",
        "                        reshape(96, 96) for image in train_image])\n",
        "test_image = np.array([np.array([int(pixel) for pixel in image.split()]).\\\n",
        "                       reshape(96, 96) for image in test_image])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9I3Vyj-2ud0",
        "outputId": "253a4e66-ffd9-474e-b655-4f549e17368d"
      },
      "source": [
        "training_pd.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2140, 31)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTz2ugZSaESS"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CN-gp29Zlit"
      },
      "source": [
        "class improved_CNN(nn.Module):\n",
        "    def __init__(self, l1=64, l2=32):\n",
        "        super(improved_CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=3,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(3)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=3,\n",
        "            out_channels=8,\n",
        "            kernel_size=3,\n",
        "            padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(8)\n",
        "        self.pool = nn.MaxPool2d(\n",
        "            kernel_size=2,\n",
        "            stride=2)\n",
        "        self.fc1 = nn.Linear(24 * 24 * 8, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 30)\n",
        "        self.dropout = nn.Dropout()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 96, 96)\n",
        "        x = self.conv1(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = x.view(-1, 24 * 24 * 8)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-aaKBqPcnOj",
        "outputId": "48757142-1357-4233-836f-42dcf7317f3d"
      },
      "source": [
        "pip install ray"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-1.8.0-cp37-cp37m-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.7 MB 169 kB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.2.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (2.6.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.4.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.42.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.2)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.0.2-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 49.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Collecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->redis>=3.5.0->ray) (1.13.3)\n",
            "Installing collected packages: deprecated, redis, ray\n",
            "Successfully installed deprecated-1.2.13 ray-1.8.0 redis-4.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROlbX2NUdJap",
        "outputId": "43da6540-a676-463e-eac5-8a608dcfa0b1"
      },
      "source": [
        "pip install -U tensorboardx"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIKDFh6WcD8z"
      },
      "source": [
        "from utils import FaceDataset, RMSELoss\n",
        "from functools import partial\n",
        "from models import CNN\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from filelock import FileLock\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "\n",
        "def improved_cnn_train(config, data, checkpoint_dir=None, data_dir=None):\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = torch.device('cuda')\n",
        "    else:\n",
        "        DEVICE = torch.device('cpu')\n",
        "    print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)\n",
        "\n",
        "    model = improved_CNN(config['l1'], config['l2']).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'])\n",
        "    criterion = RMSELoss()\n",
        "    print(model)\n",
        "\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    dataset = FaceDataset(data[0], data[1])\n",
        "\n",
        "    lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\n",
        "    train_data, val_data = torch.utils.data.random_split(dataset, lengths)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_data,\n",
        "                          batch_size=config['batch_size'],\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "    val_loader = DataLoader(dataset=val_data,\n",
        "                        batch_size=config['batch_size'],\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)\n",
        "\n",
        "    model.train()\n",
        "    for Epoch in range(50):\n",
        "        # running_loss = 0.0\n",
        "        # epoch_steps = 0\n",
        "        for batch_idx, (image, key) in enumerate(train_loader):\n",
        "            image = image.to(DEVICE)\n",
        "            key = key.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(image)\n",
        "            loss = criterion(output, key)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # running_loss += loss.item()\n",
        "            # epoch_steps += 1\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                    Epoch, batch_idx * len(image),\n",
        "                    len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
        "                    loss.item()))\n",
        "                # running_loss = 0.0\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for image, key in val_loader:\n",
        "            with torch.no_grad():\n",
        "                image = image.to(DEVICE)\n",
        "                key = key.to(DEVICE)\n",
        "                output = model(image)\n",
        "                val_loss += criterion(output, key).item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_loss *= config['batch_size']\n",
        "        print('\\n[EPOCH: {}], \\tVal Loss: {:.4f}\\n'.\n",
        "        format(Epoch, val_loss))\n",
        "\n",
        "        with tune.checkpoint_dir(Epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=val_loss)\n",
        "    print(\"Finished Training\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtOmYnbmcgYj",
        "outputId": "15e942ea-53b2-428b-9962-9d37142a93a3"
      },
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    data_dir = os.path.abspath(\"./data\")  # 특정 경로에 대해 절대 경로 얻기\n",
        "    config = {\n",
        "        'l1': tune.sample_from(lambda _: 2**np.random.randint(3, 8)),\n",
        "        'l2': tune.sample_from(lambda _: 2**np.random.randint(3, 8)),\n",
        "        'lr': tune.loguniform(1e-3, 1e-1),\n",
        "        'batch_size': tune.choice([8, 16, 32])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric='loss',\n",
        "        mode='min',\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=3,\n",
        "        reduction_factor=2)\n",
        "    \n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"training_iteration\"])\n",
        "    \n",
        "    result = tune.run(\n",
        "        tune.with_parameters(partial(improved_cnn_train, data_dir=data_dir), data=(train_image, train_key)),\n",
        "        resources_per_trial={'cpu': 1, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last-5-avg\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "\n",
        "    best_trained_model = improved_CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = torch.device('cuda')\n",
        "    else:\n",
        "        DEVICE = torch.device('cpu')\n",
        "    best_trained_model.to(DEVICE)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    PATH = '/content/drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection/saved_model/'\n",
        "    torch.save(best_trained_model, PATH + str(best_trial.config) + 'val_loss: ' + str(best_trial.last_result['loss']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=40, gpus_per_trial=1)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2021-11-24 07:58:30 (running for 00:00:00.24)\n",
            "Memory usage on this node: 3.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+----------------------------------+----------+-------+--------------+------+------+------------+\n",
            "| Trial name                       | status   | loc   |   batch_size |   l1 |   l2 |         lr |\n",
            "|----------------------------------+----------+-------+--------------+------+------+------------|\n",
            "| tune_with_parameters_504dc_00000 | PENDING  |       |           16 |   64 |   64 | 0.0107154  |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |       |           16 |  128 |   64 | 0.0392584  |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |       |           16 |  128 |  128 | 0.00103798 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |       |           32 |   64 |    8 | 0.00138125 |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |       |            8 |    8 |   16 | 0.00404449 |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |       |           16 |   16 |   64 | 0.00553961 |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |       |           16 |   32 |   16 | 0.0136501  |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |       |            8 |    8 |   64 | 0.00144867 |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |       |           32 |    8 |   64 | 0.0123096  |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |       |            8 |    8 |   16 | 0.0351089  |\n",
            "+----------------------------------+----------+-------+--------------+------+------+------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:58:35 (running for 00:00:05.30)\n",
            "Memory usage on this node: 4.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: None\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=7189)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=7189)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 51.182941\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 21.791040\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-58-38\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 10.606613551344827\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.064067602157593\n",
            "  time_this_iter_s: 4.064067602157593\n",
            "  time_total_s: 4.064067602157593\n",
            "  timestamp: 1637740718\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 0], \tVal Loss: 10.6066\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 9.556102\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 5.109556\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 1], \tVal Loss: 3.6141\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 4.154134\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 3.064131\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 2], \tVal Loss: 3.2635\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 3 [0/1712(0%)]\tTrain Loss: 3.320019\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 3 [1600/1712(93%)]\tTrain Loss: 3.338257\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:58:41 (running for 00:00:11.22)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: None | Iter 6.000: None | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 3.26348 |                    3 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 3], \tVal Loss: 3.1264\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 4 [0/1712(0%)]\tTrain Loss: 3.277753\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 4 [1600/1712(93%)]\tTrain Loss: 2.463974\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-58-43\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 5\n",
            "  loss: 2.6733214209012894\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 9.226342916488647\n",
            "  time_this_iter_s: 1.2893216609954834\n",
            "  time_total_s: 9.226342916488647\n",
            "  timestamp: 1637740723\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 4], \tVal Loss: 2.6733\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 5 [0/1712(0%)]\tTrain Loss: 2.731818\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 5 [1600/1712(93%)]\tTrain Loss: 2.720591\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 5], \tVal Loss: 3.9588\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 6 [0/1712(0%)]\tTrain Loss: 3.682224\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 6 [1600/1712(93%)]\tTrain Loss: 3.083544\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 6], \tVal Loss: 2.4278\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 7 [0/1712(0%)]\tTrain Loss: 2.288465\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 7 [1600/1712(93%)]\tTrain Loss: 2.761505\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:58:46 (running for 00:00:16.33)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: None | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 2.42783 |                    7 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 7], \tVal Loss: 2.1534\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 8 [0/1712(0%)]\tTrain Loss: 2.084038\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 8 [1600/1712(93%)]\tTrain Loss: 2.418542\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-58-48\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 9\n",
            "  loss: 2.4612051972719\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 14.358543872833252\n",
            "  time_this_iter_s: 1.2709624767303467\n",
            "  time_total_s: 14.358543872833252\n",
            "  timestamp: 1637740728\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 8], \tVal Loss: 2.4612\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 9 [0/1712(0%)]\tTrain Loss: 2.444844\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 9 [1600/1712(93%)]\tTrain Loss: 1.913700\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 9], \tVal Loss: 2.1566\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 10 [0/1712(0%)]\tTrain Loss: 1.850574\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 10 [1600/1712(93%)]\tTrain Loss: 1.905584\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 10], \tVal Loss: 2.3570\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 11 [0/1712(0%)]\tTrain Loss: 2.136787\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 11 [1600/1712(93%)]\tTrain Loss: 2.047756\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:58:51 (running for 00:00:21.52)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: None | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 2.35696 |                   11 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 11], \tVal Loss: 2.0752\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 12 [0/1712(0%)]\tTrain Loss: 1.670490\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 12 [1600/1712(93%)]\tTrain Loss: 1.875842\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-58-53\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 13\n",
            "  loss: 2.0432500616412295\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 19.56094455718994\n",
            "  time_this_iter_s: 1.296144723892212\n",
            "  time_total_s: 19.56094455718994\n",
            "  timestamp: 1637740733\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 13\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 12], \tVal Loss: 2.0433\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 13 [0/1712(0%)]\tTrain Loss: 1.699437\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 13 [1600/1712(93%)]\tTrain Loss: 2.168883\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 13], \tVal Loss: 2.1450\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 14 [0/1712(0%)]\tTrain Loss: 1.991373\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 14 [1600/1712(93%)]\tTrain Loss: 1.713172\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 14], \tVal Loss: 1.8850\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 15 [0/1712(0%)]\tTrain Loss: 2.142733\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:58:57 (running for 00:00:26.78)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.88495 |                   15 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 15 [1600/1712(93%)]\tTrain Loss: 1.637802\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 15], \tVal Loss: 2.7748\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 16 [0/1712(0%)]\tTrain Loss: 2.533495\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 16 [1600/1712(93%)]\tTrain Loss: 2.772921\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-58-58\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 17\n",
            "  loss: 1.9602884176735567\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 24.958356857299805\n",
            "  time_this_iter_s: 1.3317103385925293\n",
            "  time_total_s: 24.958356857299805\n",
            "  timestamp: 1637740738\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 17\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 16], \tVal Loss: 1.9603\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 17 [0/1712(0%)]\tTrain Loss: 1.675786\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 17 [1600/1712(93%)]\tTrain Loss: 1.595591\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 17], \tVal Loss: 1.9355\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 18 [0/1712(0%)]\tTrain Loss: 1.722264\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 18 [1600/1712(93%)]\tTrain Loss: 2.181374\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 18], \tVal Loss: 2.1683\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 19 [0/1712(0%)]\tTrain Loss: 1.856103\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:02 (running for 00:00:32.18)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 2.16831 |                   19 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 19 [1600/1712(93%)]\tTrain Loss: 1.617838\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 19], \tVal Loss: 1.8270\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 20 [0/1712(0%)]\tTrain Loss: 1.518588\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 20 [1600/1712(93%)]\tTrain Loss: 1.527256\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-59-04\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 21\n",
            "  loss: 1.8517876562671127\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 30.354670524597168\n",
            "  time_this_iter_s: 1.3400986194610596\n",
            "  time_total_s: 30.354670524597168\n",
            "  timestamp: 1637740744\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 21\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 20], \tVal Loss: 1.8518\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 21 [0/1712(0%)]\tTrain Loss: 1.463790\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 21 [1600/1712(93%)]\tTrain Loss: 1.461639\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 21], \tVal Loss: 1.9283\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 22 [0/1712(0%)]\tTrain Loss: 1.476179\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 22 [1600/1712(93%)]\tTrain Loss: 1.852165\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 22], \tVal Loss: 2.0909\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 23 [0/1712(0%)]\tTrain Loss: 1.627421\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 23 [1600/1712(93%)]\tTrain Loss: 1.811617\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:08 (running for 00:00:37.56)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: None | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 2.09093 |                   23 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 23], \tVal Loss: 1.9038\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 24 [0/1712(0%)]\tTrain Loss: 1.590490\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 24 [1600/1712(93%)]\tTrain Loss: 1.507312\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-59-09\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 25\n",
            "  loss: 1.976464940008716\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 35.692527294158936\n",
            "  time_this_iter_s: 1.3345985412597656\n",
            "  time_total_s: 35.692527294158936\n",
            "  timestamp: 1637740749\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 25\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 24], \tVal Loss: 1.9765\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 25 [0/1712(0%)]\tTrain Loss: 1.670820\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 25 [1600/1712(93%)]\tTrain Loss: 1.665090\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 25], \tVal Loss: 1.9074\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 26 [0/1712(0%)]\tTrain Loss: 1.389732\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 26 [1600/1712(93%)]\tTrain Loss: 2.207817\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 26], \tVal Loss: 2.0788\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 27 [0/1712(0%)]\tTrain Loss: 1.635180\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 27 [1600/1712(93%)]\tTrain Loss: 1.711716\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:13 (running for 00:00:42.81)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 2.07879 |                   27 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 27], \tVal Loss: 2.0721\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 28 [0/1712(0%)]\tTrain Loss: 1.705443\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 28 [1600/1712(93%)]\tTrain Loss: 1.372715\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-59-14\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 29\n",
            "  loss: 1.8359222634930477\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 40.87856888771057\n",
            "  time_this_iter_s: 1.2893531322479248\n",
            "  time_total_s: 40.87856888771057\n",
            "  timestamp: 1637740754\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 29\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 28], \tVal Loss: 1.8359\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 29 [0/1712(0%)]\tTrain Loss: 1.204278\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 29 [1600/1712(93%)]\tTrain Loss: 1.565057\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 29], \tVal Loss: 2.0006\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 30 [0/1712(0%)]\tTrain Loss: 1.570236\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 30 [1600/1712(93%)]\tTrain Loss: 1.822073\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 30], \tVal Loss: 1.8653\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 31 [0/1712(0%)]\tTrain Loss: 1.453589\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 31 [1600/1712(93%)]\tTrain Loss: 1.544202\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:18 (running for 00:00:48.10)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.86529 |                   31 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 31], \tVal Loss: 2.0335\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 32 [0/1712(0%)]\tTrain Loss: 1.605000\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 32 [1600/1712(93%)]\tTrain Loss: 1.340410\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-59-20\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 33\n",
            "  loss: 1.7688174916205006\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 46.23642063140869\n",
            "  time_this_iter_s: 1.3224923610687256\n",
            "  time_total_s: 46.23642063140869\n",
            "  timestamp: 1637740760\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 33\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 32], \tVal Loss: 1.7688\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 33 [0/1712(0%)]\tTrain Loss: 1.331580\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 33 [1600/1712(93%)]\tTrain Loss: 1.453261\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 33], \tVal Loss: 1.9290\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 34 [0/1712(0%)]\tTrain Loss: 1.425543\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 34 [1600/1712(93%)]\tTrain Loss: 1.352585\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 34], \tVal Loss: 1.7906\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 35 [0/1712(0%)]\tTrain Loss: 1.271127\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:24 (running for 00:00:53.72)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |   loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.7906 |                   35 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |        |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |        |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |        |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |        |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |        |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |        |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |        |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |        |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |        |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 35 [1600/1712(93%)]\tTrain Loss: 1.624447\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 35], \tVal Loss: 2.1186\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 36 [0/1712(0%)]\tTrain Loss: 1.652687\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 36 [1600/1712(93%)]\tTrain Loss: 1.382470\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-59-25\n",
            "  done: false\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 37\n",
            "  loss: 1.8316029031700063\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 51.850849866867065\n",
            "  time_this_iter_s: 1.3290488719940186\n",
            "  time_total_s: 51.850849866867065\n",
            "  timestamp: 1637740765\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 37\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 36], \tVal Loss: 1.8316\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 37 [0/1712(0%)]\tTrain Loss: 1.246133\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 37 [1600/1712(93%)]\tTrain Loss: 1.268181\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 37], \tVal Loss: 1.8996\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 38 [0/1712(0%)]\tTrain Loss: 1.428842\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 38 [1600/1712(93%)]\tTrain Loss: 1.443396\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 38], \tVal Loss: 2.3673\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 39 [0/1712(0%)]\tTrain Loss: 2.193379\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:29 (running for 00:00:59.05)\n",
            "Memory usage on this node: 5.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | RUNNING  | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 2.36725 |                   39 |\n",
            "| tune_with_parameters_504dc_00001 | PENDING  |                 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING  |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING  |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING  |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING  |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING  |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING  |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING  |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING  |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m Train Epoch: 39 [1600/1712(93%)]\tTrain Loss: 1.724999\n",
            "Result for tune_with_parameters_504dc_00000:\n",
            "  date: 2021-11-24_07-59-29\n",
            "  done: true\n",
            "  experiment_id: 40cd9b8b749e40ea9af3091ad0cc6a02\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 40\n",
            "  loss: 1.8968099344556577\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7189\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 55.93886661529541\n",
            "  time_this_iter_s: 1.4121742248535156\n",
            "  time_total_s: 55.93886661529541\n",
            "  timestamp: 1637740769\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 40\n",
            "  trial_id: 504dc_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m [EPOCH: 39], \tVal Loss: 1.8968\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7189)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=7188)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=7188)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 51.281532\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:34 (running for 00:01:04.55)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00001 | RUNNING    | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  |         |                      |\n",
            "| tune_with_parameters_504dc_00002 | PENDING    |                 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 29.507652\n",
            "Result for tune_with_parameters_504dc_00001:\n",
            "  date: 2021-11-24_07-59-36\n",
            "  done: false\n",
            "  experiment_id: 13dc326b0dcd4d199010e9a032a73ce9\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 27.668206045560748\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7188\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.198816776275635\n",
            "  time_this_iter_s: 4.198816776275635\n",
            "  time_total_s: 4.198816776275635\n",
            "  timestamp: 1637740776\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m [EPOCH: 0], \tVal Loss: 27.6682\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 26.332428\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 3.545458\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m [EPOCH: 1], \tVal Loss: 3.4069\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 2.838318\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 3.171980\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m [EPOCH: 2], \tVal Loss: 3.7756\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=7188)\u001b[0m \n",
            "Result for tune_with_parameters_504dc_00001:\n",
            "  date: 2021-11-24_07-59-39\n",
            "  done: true\n",
            "  experiment_id: 13dc326b0dcd4d199010e9a032a73ce9\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.775649373776445\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 7188\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 6.976882457733154\n",
            "  time_this_iter_s: 1.3856592178344727\n",
            "  time_total_s: 6.976882457733154\n",
            "  timestamp: 1637740779\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 504dc_00001\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:40 (running for 00:01:09.62)\n",
            "Memory usage on this node: 3.8/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.519563149068957\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[1m\u001b[33m(scheduler +54m42s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 1.0, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (fc3): Linear(in_features=128, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=8112)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=8112)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 50.330708\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 13.861871\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:45 (running for 00:01:15.32)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.519563149068957\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 |         |                      |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_07-59-45\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 9.425826081605715\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.1214165687561035\n",
            "  time_this_iter_s: 4.1214165687561035\n",
            "  time_total_s: 4.1214165687561035\n",
            "  timestamp: 1637740785\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 0], \tVal Loss: 9.4258\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 9.622493\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 3.686879\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 1], \tVal Loss: 3.5172\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 3.418190\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 3.760936\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 2], \tVal Loss: 3.0729\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 3 [0/1712(0%)]\tTrain Loss: 2.497318\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 3 [1600/1712(93%)]\tTrain Loss: 2.580748\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 3], \tVal Loss: 2.9877\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 4 [0/1712(0%)]\tTrain Loss: 2.781883\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 4 [1600/1712(93%)]\tTrain Loss: 2.850064\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:50 (running for 00:01:20.38)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.9588077135175186 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 2.98773 |                    4 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_07-59-51\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 5\n",
            "  loss: 2.9490107509577386\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 9.46444320678711\n",
            "  time_this_iter_s: 1.3497169017791748\n",
            "  time_total_s: 9.46444320678711\n",
            "  timestamp: 1637740791\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 4], \tVal Loss: 2.9490\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 5 [0/1712(0%)]\tTrain Loss: 2.750591\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 5 [1600/1712(93%)]\tTrain Loss: 2.528930\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 5], \tVal Loss: 2.4584\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 6 [0/1712(0%)]\tTrain Loss: 2.508707\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 6 [1600/1712(93%)]\tTrain Loss: 1.992032\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 6], \tVal Loss: 2.2447\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 7 [0/1712(0%)]\tTrain Loss: 1.991932\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 7 [1600/1712(93%)]\tTrain Loss: 1.721858\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 7], \tVal Loss: 2.1833\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 8 [0/1712(0%)]\tTrain Loss: 1.669102\n",
            "== Status ==\n",
            "Current time: 2021-11-24 07:59:56 (running for 00:01:25.97)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 2.18334 |                    8 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 8 [1600/1712(93%)]\tTrain Loss: 2.045975\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_07-59-56\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 9\n",
            "  loss: 2.076146607087037\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 15.047614812850952\n",
            "  time_this_iter_s: 1.3630893230438232\n",
            "  time_total_s: 15.047614812850952\n",
            "  timestamp: 1637740796\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 8], \tVal Loss: 2.0761\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 9 [0/1712(0%)]\tTrain Loss: 1.611265\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 9 [1600/1712(93%)]\tTrain Loss: 1.519113\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 9], \tVal Loss: 2.1286\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 10 [0/1712(0%)]\tTrain Loss: 1.809920\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 10 [1600/1712(93%)]\tTrain Loss: 1.861551\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 10], \tVal Loss: 2.1129\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 11 [0/1712(0%)]\tTrain Loss: 1.579668\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 11 [1600/1712(93%)]\tTrain Loss: 1.631066\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 11], \tVal Loss: 1.9747\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 12 [0/1712(0%)]\tTrain Loss: 1.695360\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:01 (running for 00:01:31.50)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.97474 |                   12 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 12 [1600/1712(93%)]\tTrain Loss: 1.598593\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-02\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 13\n",
            "  loss: 1.9016964636116385\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 20.57571268081665\n",
            "  time_this_iter_s: 1.3438785076141357\n",
            "  time_total_s: 20.57571268081665\n",
            "  timestamp: 1637740802\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 13\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 12], \tVal Loss: 1.9017\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 13 [0/1712(0%)]\tTrain Loss: 1.850160\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 13 [1600/1712(93%)]\tTrain Loss: 1.461906\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 13], \tVal Loss: 2.0145\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 14 [0/1712(0%)]\tTrain Loss: 1.404728\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 14 [1600/1712(93%)]\tTrain Loss: 1.461643\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 14], \tVal Loss: 1.8650\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 15 [0/1712(0%)]\tTrain Loss: 1.541119\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 15 [1600/1712(93%)]\tTrain Loss: 1.277249\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 15], \tVal Loss: 1.8595\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 16 [0/1712(0%)]\tTrain Loss: 1.330809\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 16 [1600/1712(93%)]\tTrain Loss: 1.220993\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:07 (running for 00:01:36.97)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.85954 |                   16 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-07\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 17\n",
            "  loss: 1.9150830607547937\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 26.044369220733643\n",
            "  time_this_iter_s: 1.366053819656372\n",
            "  time_total_s: 26.044369220733643\n",
            "  timestamp: 1637740807\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 17\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 16], \tVal Loss: 1.9151\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 17 [0/1712(0%)]\tTrain Loss: 1.318410\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 17 [1600/1712(93%)]\tTrain Loss: 1.434795\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 17], \tVal Loss: 1.9303\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 18 [0/1712(0%)]\tTrain Loss: 1.294457\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 18 [1600/1712(93%)]\tTrain Loss: 1.326540\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 18], \tVal Loss: 1.8864\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 19 [0/1712(0%)]\tTrain Loss: 1.323445\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 19 [1600/1712(93%)]\tTrain Loss: 1.324439\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 19], \tVal Loss: 1.8390\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 20 [0/1712(0%)]\tTrain Loss: 1.125724\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 20 [1600/1712(93%)]\tTrain Loss: 1.518380\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:12 (running for 00:01:42.51)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.903761502738311 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.83899 |                   20 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-13\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 21\n",
            "  loss: 1.8143204573158906\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 31.61900043487549\n",
            "  time_this_iter_s: 1.3754351139068604\n",
            "  time_total_s: 31.61900043487549\n",
            "  timestamp: 1637740813\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 21\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 20], \tVal Loss: 1.8143\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 21 [0/1712(0%)]\tTrain Loss: 1.128956\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 21 [1600/1712(93%)]\tTrain Loss: 1.294050\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 21], \tVal Loss: 1.9782\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 22 [0/1712(0%)]\tTrain Loss: 1.078120\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 22 [1600/1712(93%)]\tTrain Loss: 1.157074\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 22], \tVal Loss: 1.7970\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 23 [0/1712(0%)]\tTrain Loss: 1.121958\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 23 [1600/1712(93%)]\tTrain Loss: 1.066354\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 23], \tVal Loss: 1.8194\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 24 [0/1712(0%)]\tTrain Loss: 1.015238\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 24 [1600/1712(93%)]\tTrain Loss: 1.112210\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:18 (running for 00:01:47.99)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.81944 |                   24 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 24], \tVal Loss: 1.7903\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-18\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 25\n",
            "  loss: 1.7902830649759167\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 37.02118158340454\n",
            "  time_this_iter_s: 1.3099193572998047\n",
            "  time_total_s: 37.02118158340454\n",
            "  timestamp: 1637740818\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 25\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 25 [0/1712(0%)]\tTrain Loss: 0.944542\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 25 [1600/1712(93%)]\tTrain Loss: 1.431909\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 25], \tVal Loss: 1.8524\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 26 [0/1712(0%)]\tTrain Loss: 1.096308\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 26 [1600/1712(93%)]\tTrain Loss: 0.907096\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 26], \tVal Loss: 1.8620\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 27 [0/1712(0%)]\tTrain Loss: 1.018686\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 27 [1600/1712(93%)]\tTrain Loss: 0.973851\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 27], \tVal Loss: 1.8835\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 28 [0/1712(0%)]\tTrain Loss: 1.057292\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 28 [1600/1712(93%)]\tTrain Loss: 1.197685\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:23 (running for 00:01:53.31)\n",
            "Memory usage on this node: 5.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.88349 |                   28 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-24\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 29\n",
            "  loss: 2.0604595736922504\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 42.32412838935852\n",
            "  time_this_iter_s: 1.280414342880249\n",
            "  time_total_s: 42.32412838935852\n",
            "  timestamp: 1637740824\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 29\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 28], \tVal Loss: 2.0605\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 29 [0/1712(0%)]\tTrain Loss: 1.296345\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 29 [1600/1712(93%)]\tTrain Loss: 0.996856\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 29], \tVal Loss: 1.8826\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 30 [0/1712(0%)]\tTrain Loss: 1.141313\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 30 [1600/1712(93%)]\tTrain Loss: 1.316263\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 30], \tVal Loss: 1.8465\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 31 [0/1712(0%)]\tTrain Loss: 1.035900\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 31 [1600/1712(93%)]\tTrain Loss: 1.215135\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 31], \tVal Loss: 1.9047\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 32 [0/1712(0%)]\tTrain Loss: 1.253273\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:28 (running for 00:01:58.51)\n",
            "Memory usage on this node: 5.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.90469 |                   32 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 32 [1600/1712(93%)]\tTrain Loss: 1.036911\n",
            "\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-29\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 33\n",
            "  loss: 1.90320912477012\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 47.49482464790344\n",
            "  time_this_iter_s: 1.2470154762268066\n",
            "  time_total_s: 47.49482464790344\n",
            "  timestamp: 1637740829\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 33\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 32], \tVal Loss: 1.9032\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 33 [0/1712(0%)]\tTrain Loss: 1.162520\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 33 [1600/1712(93%)]\tTrain Loss: 1.202501\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 33], \tVal Loss: 1.8819\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 34 [0/1712(0%)]\tTrain Loss: 1.133292\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 34 [1600/1712(93%)]\tTrain Loss: 0.984392\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 34], \tVal Loss: 1.7958\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 35 [0/1712(0%)]\tTrain Loss: 0.882777\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 35 [1600/1712(93%)]\tTrain Loss: 0.926006\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 35], \tVal Loss: 1.8410\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 36 [0/1712(0%)]\tTrain Loss: 0.994983\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:34 (running for 00:02:03.81)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00002 | RUNNING    | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.84099 |                   36 |\n",
            "| tune_with_parameters_504dc_00003 | PENDING    |                 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 36 [1600/1712(93%)]\tTrain Loss: 0.809757\n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-34\n",
            "  done: false\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 37\n",
            "  loss: 1.7580870155976198\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 52.91054081916809\n",
            "  time_this_iter_s: 1.3711605072021484\n",
            "  time_total_s: 52.91054081916809\n",
            "  timestamp: 1637740834\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 37\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 36], \tVal Loss: 1.7581\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 37 [0/1712(0%)]\tTrain Loss: 0.892766\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 37 [1600/1712(93%)]\tTrain Loss: 0.881710\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 37], \tVal Loss: 1.8696\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 38 [0/1712(0%)]\tTrain Loss: 0.949671\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 38 [1600/1712(93%)]\tTrain Loss: 0.872843\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 38], \tVal Loss: 1.7533\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 39 [0/1712(0%)]\tTrain Loss: 0.846281\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m Train Epoch: 39 [1600/1712(93%)]\tTrain Loss: 0.867084\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m [EPOCH: 39], \tVal Loss: 1.8289\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8112)\u001b[0m \n",
            "Result for tune_with_parameters_504dc_00002:\n",
            "  date: 2021-11-24_08-00-38\n",
            "  done: true\n",
            "  experiment_id: f2daeca7ecf84b1eb69ab758cad2f433\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 40\n",
            "  loss: 1.8288547613910426\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8112\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 56.88304042816162\n",
            "  time_this_iter_s: 1.3608407974243164\n",
            "  time_total_s: 56.88304042816162\n",
            "  timestamp: 1637740838\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 40\n",
            "  trial_id: 504dc_00002\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:39 (running for 00:02:09.19)\n",
            "Memory usage on this node: 4.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00003 | RUNNING    | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 |         |                      |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (fc2): Linear(in_features=64, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (fc3): Linear(in_features=8, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=8945)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=8945)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 50.867588\n",
            "Result for tune_with_parameters_504dc_00003:\n",
            "  date: 2021-11-24_08-00-44\n",
            "  done: false\n",
            "  experiment_id: 663aca90b97244ee99f19c792713a67c\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 44.661568614924064\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8945\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 3.6352696418762207\n",
            "  time_this_iter_s: 3.6352696418762207\n",
            "  time_total_s: 3.6352696418762207\n",
            "  timestamp: 1637740844\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00003\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:44 (running for 00:02:14.47)\n",
            "Memory usage on this node: 5.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |     loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------|\n",
            "| tune_with_parameters_504dc_00003 | RUNNING    | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 44.6616  |                    1 |\n",
            "| tune_with_parameters_504dc_00004 | PENDING    |                 |            8 |    8 |   16 | 0.00404449 |          |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |          |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |          |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |          |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |          |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |          |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  |  1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  |  3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 |  1.82885 |                   40 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m [EPOCH: 0], \tVal Loss: 44.6616\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 42.516392\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m [EPOCH: 1], \tVal Loss: 4.3246\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 4.167256\n",
            "Result for tune_with_parameters_504dc_00003:\n",
            "  date: 2021-11-24_08-00-46\n",
            "  done: true\n",
            "  experiment_id: 663aca90b97244ee99f19c792713a67c\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.46155897479191\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8945\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 5.291407346725464\n",
            "  time_this_iter_s: 0.8196849822998047\n",
            "  time_total_s: 5.291407346725464\n",
            "  timestamp: 1637740846\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 504dc_00003\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m [EPOCH: 2], \tVal Loss: 3.4616\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8945)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:50 (running for 00:02:20.20)\n",
            "Memory usage on this node: 4.8/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.3625179495766897\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00004 | RUNNING    | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 |         |                      |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (fc1): Linear(in_features=4608, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (fc2): Linear(in_features=8, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (fc3): Linear(in_features=16, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9036)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9036)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 51.366745\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 0 [800/1712(47%)]\tTrain Loss: 35.327564\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 31.019598\n",
            "Result for tune_with_parameters_504dc_00004:\n",
            "  date: 2021-11-24_08-00-54\n",
            "  done: false\n",
            "  experiment_id: 2b469a955a0d4acf842b312fa6db0bb4\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 24.46262637699876\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9036\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.926730155944824\n",
            "  time_this_iter_s: 4.926730155944824\n",
            "  time_total_s: 4.926730155944824\n",
            "  timestamp: 1637740854\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00004\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m [EPOCH: 0], \tVal Loss: 24.4626\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 22.622297\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 1 [800/1712(47%)]\tTrain Loss: 3.844552\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 3.803928\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:00:56 (running for 00:02:25.72)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.3625179495766897\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |     loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------|\n",
            "| tune_with_parameters_504dc_00004 | RUNNING    | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 24.4626  |                    1 |\n",
            "| tune_with_parameters_504dc_00005 | PENDING    |                 |           16 |   16 |   64 | 0.00553961 |          |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |          |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |          |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |          |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |          |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  |  1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  |  3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 |  1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 |  3.46156 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m [EPOCH: 1], \tVal Loss: 3.4026\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 2.879955\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 2 [800/1712(47%)]\tTrain Loss: 2.666326\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 3.714727\n",
            "Result for tune_with_parameters_504dc_00004:\n",
            "  date: 2021-11-24_08-00-58\n",
            "  done: true\n",
            "  experiment_id: 2b469a955a0d4acf842b312fa6db0bb4\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.4304770532055437\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9036\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 9.383198976516724\n",
            "  time_this_iter_s: 2.266619920730591\n",
            "  time_total_s: 9.383198976516724\n",
            "  timestamp: 1637740858\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 504dc_00004\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m [EPOCH: 2], \tVal Loss: 3.4305\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9036)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:01 (running for 00:02:31.24)\n",
            "Memory usage on this node: 4.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.4304770532055437\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00005 | RUNNING    | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 |         |                      |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (fc1): Linear(in_features=4608, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (fc2): Linear(in_features=16, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9126)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9126)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 50.950569\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 12.199120\n",
            "Result for tune_with_parameters_504dc_00005:\n",
            "  date: 2021-11-24_08-01-05\n",
            "  done: false\n",
            "  experiment_id: 7519ee6e35f047da818aa2272ac860e2\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 29.730996319066698\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9126\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.060785293579102\n",
            "  time_this_iter_s: 4.060785293579102\n",
            "  time_total_s: 4.060785293579102\n",
            "  timestamp: 1637740865\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m [EPOCH: 0], \tVal Loss: 29.7310\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 29.095310\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 3.749107\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:06 (running for 00:02:36.25)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.4304770532055437\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00005 | RUNNING    | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.46111 |                    2 |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m [EPOCH: 1], \tVal Loss: 3.4611\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 3.567913\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 3.481297\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m [EPOCH: 2], \tVal Loss: 3.2591\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 3 [0/1712(0%)]\tTrain Loss: 2.824063\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 3 [1600/1712(93%)]\tTrain Loss: 3.568175\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m [EPOCH: 3], \tVal Loss: 3.2273\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 4 [0/1712(0%)]\tTrain Loss: 4.020510\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 4 [1600/1712(93%)]\tTrain Loss: 3.505915\n",
            "Result for tune_with_parameters_504dc_00005:\n",
            "  date: 2021-11-24_08-01-10\n",
            "  done: false\n",
            "  experiment_id: 7519ee6e35f047da818aa2272ac860e2\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 5\n",
            "  loss: 3.309630688105788\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9126\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 9.434425592422485\n",
            "  time_this_iter_s: 1.373784065246582\n",
            "  time_total_s: 9.434425592422485\n",
            "  timestamp: 1637740870\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: 504dc_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m [EPOCH: 4], \tVal Loss: 3.3096\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 5 [0/1712(0%)]\tTrain Loss: 3.395811\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:11 (running for 00:02:41.26)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2085934999947234 | Iter 3.000: -3.3469769887835064\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00005 | RUNNING    | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.30963 |                    5 |\n",
            "| tune_with_parameters_504dc_00006 | PENDING    |                 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m Train Epoch: 5 [1600/1712(93%)]\tTrain Loss: 3.716477\n",
            "Result for tune_with_parameters_504dc_00005:\n",
            "  date: 2021-11-24_08-01-12\n",
            "  done: true\n",
            "  experiment_id: 7519ee6e35f047da818aa2272ac860e2\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 6\n",
            "  loss: 3.4927744286082616\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9126\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 10.85139775276184\n",
            "  time_this_iter_s: 1.4169721603393555\n",
            "  time_total_s: 10.85139775276184\n",
            "  timestamp: 1637740872\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 6\n",
            "  trial_id: 504dc_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m [EPOCH: 5], \tVal Loss: 3.4928\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:17 (running for 00:02:46.71)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.4927744286082616 | Iter 3.000: -3.3469769887835064\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00006 | RUNNING    | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  |         |                      |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (fc1): Linear(in_features=4608, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (fc3): Linear(in_features=16, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9278)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9278)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 51.326027\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 23.721272\n",
            "Result for tune_with_parameters_504dc_00006:\n",
            "  date: 2021-11-24_08-01-18\n",
            "  done: false\n",
            "  experiment_id: 1abfe9d759d74ae4a52cd4b162154b7f\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 33.279791787406\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9278\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.087630033493042\n",
            "  time_this_iter_s: 4.087630033493042\n",
            "  time_total_s: 4.087630033493042\n",
            "  timestamp: 1637740878\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00006\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 0], \tVal Loss: 33.2798\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 32.835705\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 5.047678\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 1], \tVal Loss: 3.4098\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 2.996041\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 4.027865\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 2], \tVal Loss: 3.1943\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 3 [0/1712(0%)]\tTrain Loss: 2.857746\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:22 (running for 00:02:52.06)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.4927744286082616 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00006 | RUNNING    | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 3.19431 |                    3 |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 3 [1600/1712(93%)]\tTrain Loss: 3.705602\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 3], \tVal Loss: 3.2257\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 4 [0/1712(0%)]\tTrain Loss: 3.947583\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 4 [1600/1712(93%)]\tTrain Loss: 3.319024\n",
            "Result for tune_with_parameters_504dc_00006:\n",
            "  date: 2021-11-24_08-01-24\n",
            "  done: false\n",
            "  experiment_id: 1abfe9d759d74ae4a52cd4b162154b7f\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 5\n",
            "  loss: 3.21961740689857\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9278\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 9.404934167861938\n",
            "  time_this_iter_s: 1.274958848953247\n",
            "  time_total_s: 9.404934167861938\n",
            "  timestamp: 1637740884\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: 504dc_00006\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 4], \tVal Loss: 3.2196\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 5 [0/1712(0%)]\tTrain Loss: 3.405746\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 5 [1600/1712(93%)]\tTrain Loss: 4.661064\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 5], \tVal Loss: 3.0467\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 6 [0/1712(0%)]\tTrain Loss: 2.751192\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 6 [1600/1712(93%)]\tTrain Loss: 3.383569\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 6], \tVal Loss: 3.8049\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 7 [0/1712(0%)]\tTrain Loss: 3.392789\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 7 [1600/1712(93%)]\tTrain Loss: 2.396078\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:27 (running for 00:02:57.33)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2697404790147444 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00006 | RUNNING    | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 3.80485 |                    7 |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 7], \tVal Loss: 2.7448\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 8 [0/1712(0%)]\tTrain Loss: 2.607177\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 8 [1600/1712(93%)]\tTrain Loss: 2.213486\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 8], \tVal Loss: 2.9675\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "Result for tune_with_parameters_504dc_00006:\n",
            "  date: 2021-11-24_08-01-29\n",
            "  done: false\n",
            "  experiment_id: 1abfe9d759d74ae4a52cd4b162154b7f\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 9\n",
            "  loss: 2.96750772779233\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9278\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 14.6345853805542\n",
            "  time_this_iter_s: 1.2956328392028809\n",
            "  time_total_s: 14.6345853805542\n",
            "  timestamp: 1637740889\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: 504dc_00006\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 9 [0/1712(0%)]\tTrain Loss: 2.555404\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 9 [1600/1712(93%)]\tTrain Loss: 3.020599\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 9], \tVal Loss: 2.4675\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 10 [0/1712(0%)]\tTrain Loss: 2.350443\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 10 [1600/1712(93%)]\tTrain Loss: 2.413914\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 10], \tVal Loss: 2.5853\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 11 [0/1712(0%)]\tTrain Loss: 2.277586\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:32 (running for 00:03:02.55)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.0249568769864945 | Iter 6.000: -3.2697404790147444 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00006 | RUNNING    | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.5853  |                   11 |\n",
            "| tune_with_parameters_504dc_00007 | PENDING    |                 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m Train Epoch: 11 [1600/1712(93%)]\tTrain Loss: 2.501068\n",
            "Result for tune_with_parameters_504dc_00006:\n",
            "  date: 2021-11-24_08-01-33\n",
            "  done: true\n",
            "  experiment_id: 1abfe9d759d74ae4a52cd4b162154b7f\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 12\n",
            "  loss: 2.5355649484652223\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9278\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 18.5487163066864\n",
            "  time_this_iter_s: 1.2899625301361084\n",
            "  time_total_s: 18.5487163066864\n",
            "  timestamp: 1637740893\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 12\n",
            "  trial_id: 504dc_00006\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m [EPOCH: 11], \tVal Loss: 2.5356\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9278)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:38 (running for 00:03:07.88)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.2697404790147444 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00007 | RUNNING    | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 |         |                      |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (fc1): Linear(in_features=4608, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (fc2): Linear(in_features=8, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9547)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9547)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 51.939289\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 0 [800/1712(47%)]\tTrain Loss: 19.974001\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 19.724520\n",
            "Result for tune_with_parameters_504dc_00007:\n",
            "  date: 2021-11-24_08-01-40\n",
            "  done: false\n",
            "  experiment_id: 6a2fc64cc5cc445cb77d3901bd50ebce\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 22.969548840389074\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9547\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.936206817626953\n",
            "  time_this_iter_s: 4.936206817626953\n",
            "  time_total_s: 4.936206817626953\n",
            "  timestamp: 1637740900\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00007\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 0], \tVal Loss: 22.9695\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 22.312086\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 1 [800/1712(47%)]\tTrain Loss: 3.868646\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 3.482887\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 1], \tVal Loss: 3.3457\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 3.808271\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 2 [800/1712(47%)]\tTrain Loss: 2.708780\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:44 (running for 00:03:13.70)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.2697404790147444 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00007 | RUNNING    | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 | 3.34567 |                    2 |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 3.013112\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 2], \tVal Loss: 3.1566\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 3 [0/1712(0%)]\tTrain Loss: 3.362580\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 3 [800/1712(47%)]\tTrain Loss: 3.359378\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 3 [1600/1712(93%)]\tTrain Loss: 2.920123\n",
            "Result for tune_with_parameters_504dc_00007:\n",
            "  date: 2021-11-24_08-01-47\n",
            "  done: false\n",
            "  experiment_id: 6a2fc64cc5cc445cb77d3901bd50ebce\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 4\n",
            "  loss: 3.4812632409211632\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9547\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 11.668578386306763\n",
            "  time_this_iter_s: 2.228003978729248\n",
            "  time_total_s: 11.668578386306763\n",
            "  timestamp: 1637740907\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: 504dc_00007\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 3], \tVal Loss: 3.4813\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 4 [0/1712(0%)]\tTrain Loss: 3.210999\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 4 [800/1712(47%)]\tTrain Loss: 2.721790\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 4 [1600/1712(93%)]\tTrain Loss: 2.589794\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:49 (running for 00:03:19.19)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.2697404790147444 | Iter 3.000: -3.261265759156129\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00007 | RUNNING    | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 | 3.48126 |                    4 |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 4], \tVal Loss: 2.7071\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 5 [0/1712(0%)]\tTrain Loss: 2.581501\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 5 [800/1712(47%)]\tTrain Loss: 2.138535\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 5 [1600/1712(93%)]\tTrain Loss: 2.792082\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 5], \tVal Loss: 2.7812\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 6 [0/1712(0%)]\tTrain Loss: 1.946613\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 6 [800/1712(47%)]\tTrain Loss: 2.311105\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 6 [1600/1712(93%)]\tTrain Loss: 2.688433\n",
            "Result for tune_with_parameters_504dc_00007:\n",
            "  date: 2021-11-24_08-01-54\n",
            "  done: false\n",
            "  experiment_id: 6a2fc64cc5cc445cb77d3901bd50ebce\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 7\n",
            "  loss: 2.495213125353662\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9547\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 18.423653602600098\n",
            "  time_this_iter_s: 2.243833541870117\n",
            "  time_total_s: 18.423653602600098\n",
            "  timestamp: 1637740914\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: 504dc_00007\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 6], \tVal Loss: 2.4952\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 7 [0/1712(0%)]\tTrain Loss: 3.108282\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 7 [800/1712(47%)]\tTrain Loss: 1.747641\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:01:55 (running for 00:03:24.93)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.046706529421227 | Iter 3.000: -3.261265759156129\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00007 | RUNNING    | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 | 2.49521 |                    7 |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 7 [1600/1712(93%)]\tTrain Loss: 2.591947\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 7], \tVal Loss: 2.4407\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 8 [0/1712(0%)]\tTrain Loss: 2.766926\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 8 [800/1712(47%)]\tTrain Loss: 2.257392\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 8 [1600/1712(93%)]\tTrain Loss: 2.216165\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 8], \tVal Loss: 2.3453\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 9 [0/1712(0%)]\tTrain Loss: 2.100590\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 9 [800/1712(47%)]\tTrain Loss: 1.746567\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 9 [1600/1712(93%)]\tTrain Loss: 2.208266\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:02:00 (running for 00:03:30.43)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.075169549924191 | Iter 6.000: -3.046706529421227 | Iter 3.000: -3.261265759156129\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00007 | RUNNING    | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 | 2.34528 |                    9 |\n",
            "| tune_with_parameters_504dc_00008 | PENDING    |                 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_504dc_00007:\n",
            "  date: 2021-11-24_08-02-01\n",
            "  done: false\n",
            "  experiment_id: 6a2fc64cc5cc445cb77d3901bd50ebce\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 10\n",
            "  loss: 2.3887266020908533\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9547\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 25.193849086761475\n",
            "  time_this_iter_s: 2.27581787109375\n",
            "  time_total_s: 25.193849086761475\n",
            "  timestamp: 1637740921\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 10\n",
            "  trial_id: 504dc_00007\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 9], \tVal Loss: 2.3887\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 10 [0/1712(0%)]\tTrain Loss: 2.930166\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 10 [800/1712(47%)]\tTrain Loss: 1.762869\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 10 [1600/1712(93%)]\tTrain Loss: 2.116349\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 10], \tVal Loss: 2.3858\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 11 [0/1712(0%)]\tTrain Loss: 2.155803\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 11 [800/1712(47%)]\tTrain Loss: 1.962161\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m Train Epoch: 11 [1600/1712(93%)]\tTrain Loss: 1.710648\n",
            "Result for tune_with_parameters_504dc_00007:\n",
            "  date: 2021-11-24_08-02-05\n",
            "  done: true\n",
            "  experiment_id: 6a2fc64cc5cc445cb77d3901bd50ebce\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 12\n",
            "  loss: 2.518541839635261\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9547\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 29.648306846618652\n",
            "  time_this_iter_s: 2.216947078704834\n",
            "  time_total_s: 29.648306846618652\n",
            "  timestamp: 1637740925\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 12\n",
            "  trial_id: 504dc_00007\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m [EPOCH: 11], \tVal Loss: 2.5185\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9547)\u001b[0m \n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:02:06 (running for 00:03:36.18)\n",
            "Memory usage on this node: 4.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.296855694779726 | Iter 6.000: -3.046706529421227 | Iter 3.000: -3.261265759156129\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00008 | RUNNING    | 172.28.0.2:9817 |           32 |    8 |   64 | 0.0123096  |         |                      |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "| tune_with_parameters_504dc_00007 | TERMINATED | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 | 2.51854 |                   12 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (fc1): Linear(in_features=4608, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (fc2): Linear(in_features=8, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9817)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9817)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 51.452568\n",
            "Result for tune_with_parameters_504dc_00008:\n",
            "  date: 2021-11-24_08-02-11\n",
            "  done: false\n",
            "  experiment_id: c206d51582484415b6f7cd46e8aa4ced\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 42.692078207140774\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9817\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 3.585425853729248\n",
            "  time_this_iter_s: 3.585425853729248\n",
            "  time_total_s: 3.585425853729248\n",
            "  timestamp: 1637740931\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00008\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:02:11 (running for 00:03:41.35)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.296855694779726 | Iter 6.000: -3.046706529421227 | Iter 3.000: -3.261265759156129\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |     loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------|\n",
            "| tune_with_parameters_504dc_00008 | RUNNING    | 172.28.0.2:9817 |           32 |    8 |   64 | 0.0123096  | 42.6921  |                    1 |\n",
            "| tune_with_parameters_504dc_00009 | PENDING    |                 |            8 |    8 |   16 | 0.0351089  |          |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  |  1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  |  3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 |  1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 |  3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 |  3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 |  3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  |  2.53556 |                   12 |\n",
            "| tune_with_parameters_504dc_00007 | TERMINATED | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 |  2.51854 |                   12 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m [EPOCH: 0], \tVal Loss: 42.6921\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 40.668404\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m [EPOCH: 1], \tVal Loss: 3.6763\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 3.687704\n",
            "Result for tune_with_parameters_504dc_00008:\n",
            "  date: 2021-11-24_08-02-13\n",
            "  done: true\n",
            "  experiment_id: c206d51582484415b6f7cd46e8aa4ced\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.961163636679961\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9817\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 5.269490003585815\n",
            "  time_this_iter_s: 0.8464560508728027\n",
            "  time_total_s: 5.269490003585815\n",
            "  timestamp: 1637740933\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 504dc_00008\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m [EPOCH: 2], \tVal Loss: 3.9612\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9817)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:02:17 (running for 00:03:47.08)\n",
            "Memory usage on this node: 4.8/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.296855694779726 | Iter 6.000: -3.046706529421227 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00009 | RUNNING    | 172.28.0.2:9910 |            8 |    8 |   16 | 0.0351089  |         |                      |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "| tune_with_parameters_504dc_00007 | TERMINATED | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 | 2.51854 |                   12 |\n",
            "| tune_with_parameters_504dc_00008 | TERMINATED | 172.28.0.2:9817 |           32 |    8 |   64 | 0.0123096  | 3.96116 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m improved_CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (fc1): Linear(in_features=4608, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (fc2): Linear(in_features=8, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (fc3): Linear(in_features=16, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m   (dropout): Dropout(p=0.5, inplace=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9910)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9910)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 0 [0/1712(0%)]\tTrain Loss: 51.603420\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 0 [800/1712(47%)]\tTrain Loss: 41.303268\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 0 [1600/1712(93%)]\tTrain Loss: 18.653753\n",
            "Result for tune_with_parameters_504dc_00009:\n",
            "  date: 2021-11-24_08-02-21\n",
            "  done: false\n",
            "  experiment_id: 186d4056c0e4404782bc1c7a8300641d\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 1\n",
            "  loss: 25.432847174528604\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9910\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 5.033922433853149\n",
            "  time_this_iter_s: 5.033922433853149\n",
            "  time_total_s: 5.033922433853149\n",
            "  timestamp: 1637740941\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 504dc_00009\n",
            "  \u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m [EPOCH: 0], \tVal Loss: 25.4328\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m \n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 1 [0/1712(0%)]\tTrain Loss: 24.471090\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 1 [800/1712(47%)]\tTrain Loss: 3.613387\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 1 [1600/1712(93%)]\tTrain Loss: 3.229449\n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:02:23 (running for 00:03:52.71)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.296855694779726 | Iter 6.000: -3.046706529421227 | Iter 3.000: -3.2634769243614694\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |     loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------|\n",
            "| tune_with_parameters_504dc_00009 | RUNNING    | 172.28.0.2:9910 |            8 |    8 |   16 | 0.0351089  | 25.4328  |                    1 |\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  |  1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  |  3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 |  1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 |  3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 |  3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 |  3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  |  2.53556 |                   12 |\n",
            "| tune_with_parameters_504dc_00007 | TERMINATED | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 |  2.51854 |                   12 |\n",
            "| tune_with_parameters_504dc_00008 | TERMINATED | 172.28.0.2:9817 |           32 |    8 |   64 | 0.0123096  |  3.96116 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+----------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m [EPOCH: 1], \tVal Loss: 3.7340\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 2 [0/1712(0%)]\tTrain Loss: 4.073952\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 2 [800/1712(47%)]\tTrain Loss: 3.236449\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m Train Epoch: 2 [1600/1712(93%)]\tTrain Loss: 3.011681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-24 08:02:25,874\tINFO tune.py:630 -- Total run time: 235.45 seconds (235.30 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for tune_with_parameters_504dc_00009:\n",
            "  date: 2021-11-24_08-02-25\n",
            "  done: true\n",
            "  experiment_id: 186d4056c0e4404782bc1c7a8300641d\n",
            "  hostname: 7d4b23a5ec39\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.8162507788043154\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9910\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 9.639986276626587\n",
            "  time_this_iter_s: 2.297236442565918\n",
            "  time_total_s: 9.639986276626587\n",
            "  timestamp: 1637740945\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 504dc_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m [EPOCH: 2], \tVal Loss: 3.8163\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9910)\u001b[0m \n",
            "== Status ==\n",
            "Current time: 2021-11-24 08:02:25 (running for 00:03:55.32)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=10\n",
            "Bracket: Iter 24.000: -1.8615990977420984 | Iter 12.000: -2.296855694779726 | Iter 6.000: -3.046706529421227 | Iter 3.000: -3.3469769887835064\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-24_07-58-30\n",
            "Number of trials: 10/10 (10 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_504dc_00000 | TERMINATED | 172.28.0.2:7189 |           16 |   64 |   64 | 0.0107154  | 1.89681 |                   40 |\n",
            "| tune_with_parameters_504dc_00001 | TERMINATED | 172.28.0.2:7188 |           16 |  128 |   64 | 0.0392584  | 3.77565 |                    3 |\n",
            "| tune_with_parameters_504dc_00002 | TERMINATED | 172.28.0.2:8112 |           16 |  128 |  128 | 0.00103798 | 1.82885 |                   40 |\n",
            "| tune_with_parameters_504dc_00003 | TERMINATED | 172.28.0.2:8945 |           32 |   64 |    8 | 0.00138125 | 3.46156 |                    3 |\n",
            "| tune_with_parameters_504dc_00004 | TERMINATED | 172.28.0.2:9036 |            8 |    8 |   16 | 0.00404449 | 3.43048 |                    3 |\n",
            "| tune_with_parameters_504dc_00005 | TERMINATED | 172.28.0.2:9126 |           16 |   16 |   64 | 0.00553961 | 3.49277 |                    6 |\n",
            "| tune_with_parameters_504dc_00006 | TERMINATED | 172.28.0.2:9278 |           16 |   32 |   16 | 0.0136501  | 2.53556 |                   12 |\n",
            "| tune_with_parameters_504dc_00007 | TERMINATED | 172.28.0.2:9547 |            8 |    8 |   64 | 0.00144867 | 2.51854 |                   12 |\n",
            "| tune_with_parameters_504dc_00008 | TERMINATED | 172.28.0.2:9817 |           32 |    8 |   64 | 0.0123096  | 3.96116 |                    3 |\n",
            "| tune_with_parameters_504dc_00009 | TERMINATED | 172.28.0.2:9910 |            8 |    8 |   16 | 0.0351089  | 3.81625 |                    3 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Best trial config: {'l1': 128, 'l2': 128, 'lr': 0.0010379776478907162, 'batch_size': 16}\n",
            "Best trial final validation loss: 1.8288547613910426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ5clIPygWTg"
      },
      "source": [
        "dataset = FaceDataset(train_image, train_key)\n",
        "\n",
        "lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\n",
        "train_data, val_data = torch.utils.data.random_split(dataset, lengths)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_data,\n",
        "                          batch_size=16,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "val_loader = DataLoader(dataset=val_data,\n",
        "                        batch_size=16,\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dJqz4TIg3tC"
      },
      "source": [
        "def train(model, train_loader, optimizer, log_interval):\n",
        "    model.train()\n",
        "    for batch_idx, (image, key) in enumerate(train_loader):\n",
        "        image = image.to(DEVICE)\n",
        "        key = key.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(image)\n",
        "        loss = criterion(output, key)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                Epoch, batch_idx * len(image),\n",
        "                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
        "                loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNWNLAsQg6aq"
      },
      "source": [
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for image, key in val_loader:\n",
        "            image = image.to(DEVICE)\n",
        "            key = key.to(DEVICE)\n",
        "            output = model(image)\n",
        "            val_loss += criterion(output, key).item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_loss *= 16\n",
        "    return val_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpmB8G-qmV1B"
      },
      "source": [
        "test_model = improved_CNN(l1=16, l2=16)\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "test_model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(test_model.parameters(), lr = 0.005)\n",
        "criterion = RMSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD-OfY7Rg7kR",
        "outputId": "6028a9dd-5bb8-41b7-e59a-719348ef8915"
      },
      "source": [
        "for Epoch in range(1, 50 + 1):\n",
        "    train(test_model, train_loader, optimizer, log_interval = 100)\n",
        "    val_loss = evaluate(test_model, val_loader)\n",
        "    print('\\n[EPOCH: {}], \\tVal Loss: {:.4f}\\n'.\n",
        "          format(Epoch, val_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 51.847679\n",
            "Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 20.160702\n",
            "Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 20.185585\n",
            "Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 25.803709\n",
            "\n",
            "[EPOCH: 1], \tVal Loss: 27.4746\n",
            "\n",
            "Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 21.505964\n",
            "Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 19.210417\n",
            "Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 21.712437\n",
            "Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 21.798086\n",
            "\n",
            "[EPOCH: 2], \tVal Loss: 19.8322\n",
            "\n",
            "Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 21.945637\n",
            "Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 19.015829\n",
            "Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 19.810150\n",
            "Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 15.587580\n",
            "\n",
            "[EPOCH: 3], \tVal Loss: 23.4501\n",
            "\n",
            "Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 19.098373\n",
            "Train Epoch: 4 [1600/5639(28%)]\tTrain Loss: 23.404871\n",
            "Train Epoch: 4 [3200/5639(57%)]\tTrain Loss: 18.212337\n",
            "Train Epoch: 4 [4800/5639(85%)]\tTrain Loss: 23.424971\n",
            "\n",
            "[EPOCH: 4], \tVal Loss: 22.6138\n",
            "\n",
            "Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 11.126288\n",
            "Train Epoch: 5 [1600/5639(28%)]\tTrain Loss: 17.923790\n",
            "Train Epoch: 5 [3200/5639(57%)]\tTrain Loss: 12.819024\n",
            "Train Epoch: 5 [4800/5639(85%)]\tTrain Loss: 14.223560\n",
            "\n",
            "[EPOCH: 5], \tVal Loss: 21.0437\n",
            "\n",
            "Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 15.320071\n",
            "Train Epoch: 6 [1600/5639(28%)]\tTrain Loss: 16.790955\n",
            "Train Epoch: 6 [3200/5639(57%)]\tTrain Loss: 13.723590\n",
            "Train Epoch: 6 [4800/5639(85%)]\tTrain Loss: 13.686770\n",
            "\n",
            "[EPOCH: 6], \tVal Loss: 16.7104\n",
            "\n",
            "Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 12.442713\n",
            "Train Epoch: 7 [1600/5639(28%)]\tTrain Loss: 14.752013\n",
            "Train Epoch: 7 [3200/5639(57%)]\tTrain Loss: 16.730141\n",
            "Train Epoch: 7 [4800/5639(85%)]\tTrain Loss: 12.204539\n",
            "\n",
            "[EPOCH: 7], \tVal Loss: 13.3519\n",
            "\n",
            "Train Epoch: 8 [0/5639(0%)]\tTrain Loss: 14.880552\n",
            "Train Epoch: 8 [1600/5639(28%)]\tTrain Loss: 15.253014\n",
            "Train Epoch: 8 [3200/5639(57%)]\tTrain Loss: 11.723580\n",
            "Train Epoch: 8 [4800/5639(85%)]\tTrain Loss: 13.066930\n",
            "\n",
            "[EPOCH: 8], \tVal Loss: 7.5568\n",
            "\n",
            "Train Epoch: 9 [0/5639(0%)]\tTrain Loss: 10.244949\n",
            "Train Epoch: 9 [1600/5639(28%)]\tTrain Loss: 8.841381\n",
            "Train Epoch: 9 [3200/5639(57%)]\tTrain Loss: 9.584957\n",
            "Train Epoch: 9 [4800/5639(85%)]\tTrain Loss: 12.622294\n",
            "\n",
            "[EPOCH: 9], \tVal Loss: 3.6993\n",
            "\n",
            "Train Epoch: 10 [0/5639(0%)]\tTrain Loss: 9.733482\n",
            "Train Epoch: 10 [1600/5639(28%)]\tTrain Loss: 11.385902\n",
            "Train Epoch: 10 [3200/5639(57%)]\tTrain Loss: 9.487130\n",
            "Train Epoch: 10 [4800/5639(85%)]\tTrain Loss: 12.246396\n",
            "\n",
            "[EPOCH: 10], \tVal Loss: 3.7380\n",
            "\n",
            "Train Epoch: 11 [0/5639(0%)]\tTrain Loss: 8.674600\n",
            "Train Epoch: 11 [1600/5639(28%)]\tTrain Loss: 9.128211\n",
            "Train Epoch: 11 [3200/5639(57%)]\tTrain Loss: 11.000778\n",
            "Train Epoch: 11 [4800/5639(85%)]\tTrain Loss: 10.460979\n",
            "\n",
            "[EPOCH: 11], \tVal Loss: 3.7663\n",
            "\n",
            "Train Epoch: 12 [0/5639(0%)]\tTrain Loss: 8.632144\n",
            "Train Epoch: 12 [1600/5639(28%)]\tTrain Loss: 10.832394\n",
            "Train Epoch: 12 [3200/5639(57%)]\tTrain Loss: 9.807860\n",
            "Train Epoch: 12 [4800/5639(85%)]\tTrain Loss: 9.171597\n",
            "\n",
            "[EPOCH: 12], \tVal Loss: 4.1291\n",
            "\n",
            "Train Epoch: 13 [0/5639(0%)]\tTrain Loss: 11.972808\n",
            "Train Epoch: 13 [1600/5639(28%)]\tTrain Loss: 8.217729\n",
            "Train Epoch: 13 [3200/5639(57%)]\tTrain Loss: 8.573534\n",
            "Train Epoch: 13 [4800/5639(85%)]\tTrain Loss: 8.395200\n",
            "\n",
            "[EPOCH: 13], \tVal Loss: 3.7874\n",
            "\n",
            "Train Epoch: 14 [0/5639(0%)]\tTrain Loss: 12.222426\n",
            "Train Epoch: 14 [1600/5639(28%)]\tTrain Loss: 10.395771\n",
            "Train Epoch: 14 [3200/5639(57%)]\tTrain Loss: 6.720899\n",
            "Train Epoch: 14 [4800/5639(85%)]\tTrain Loss: 9.323552\n",
            "\n",
            "[EPOCH: 14], \tVal Loss: 3.5936\n",
            "\n",
            "Train Epoch: 15 [0/5639(0%)]\tTrain Loss: 8.551378\n",
            "Train Epoch: 15 [1600/5639(28%)]\tTrain Loss: 8.999591\n",
            "Train Epoch: 15 [3200/5639(57%)]\tTrain Loss: 12.947161\n",
            "Train Epoch: 15 [4800/5639(85%)]\tTrain Loss: 9.886893\n",
            "\n",
            "[EPOCH: 15], \tVal Loss: 3.5388\n",
            "\n",
            "Train Epoch: 16 [0/5639(0%)]\tTrain Loss: 9.440693\n",
            "Train Epoch: 16 [1600/5639(28%)]\tTrain Loss: 8.525499\n",
            "Train Epoch: 16 [3200/5639(57%)]\tTrain Loss: 6.658828\n",
            "Train Epoch: 16 [4800/5639(85%)]\tTrain Loss: 7.655040\n",
            "\n",
            "[EPOCH: 16], \tVal Loss: 3.4288\n",
            "\n",
            "Train Epoch: 17 [0/5639(0%)]\tTrain Loss: 6.951675\n",
            "Train Epoch: 17 [1600/5639(28%)]\tTrain Loss: 7.796909\n",
            "Train Epoch: 17 [3200/5639(57%)]\tTrain Loss: 7.213093\n",
            "Train Epoch: 17 [4800/5639(85%)]\tTrain Loss: 8.272079\n",
            "\n",
            "[EPOCH: 17], \tVal Loss: 3.5133\n",
            "\n",
            "Train Epoch: 18 [0/5639(0%)]\tTrain Loss: 6.127557\n",
            "Train Epoch: 18 [1600/5639(28%)]\tTrain Loss: 11.996055\n",
            "Train Epoch: 18 [3200/5639(57%)]\tTrain Loss: 6.895971\n",
            "Train Epoch: 18 [4800/5639(85%)]\tTrain Loss: 9.319344\n",
            "\n",
            "[EPOCH: 18], \tVal Loss: 3.5562\n",
            "\n",
            "Train Epoch: 19 [0/5639(0%)]\tTrain Loss: 10.117800\n",
            "Train Epoch: 19 [1600/5639(28%)]\tTrain Loss: 5.526810\n",
            "Train Epoch: 19 [3200/5639(57%)]\tTrain Loss: 8.660289\n",
            "Train Epoch: 19 [4800/5639(85%)]\tTrain Loss: 10.151128\n",
            "\n",
            "[EPOCH: 19], \tVal Loss: 3.6038\n",
            "\n",
            "Train Epoch: 20 [0/5639(0%)]\tTrain Loss: 8.330812\n",
            "Train Epoch: 20 [1600/5639(28%)]\tTrain Loss: 9.363922\n",
            "Train Epoch: 20 [3200/5639(57%)]\tTrain Loss: 7.471871\n",
            "Train Epoch: 20 [4800/5639(85%)]\tTrain Loss: 7.474701\n",
            "\n",
            "[EPOCH: 20], \tVal Loss: 3.8675\n",
            "\n",
            "Train Epoch: 21 [0/5639(0%)]\tTrain Loss: 9.630465\n",
            "Train Epoch: 21 [1600/5639(28%)]\tTrain Loss: 9.946881\n",
            "Train Epoch: 21 [3200/5639(57%)]\tTrain Loss: 8.455581\n",
            "Train Epoch: 21 [4800/5639(85%)]\tTrain Loss: 5.742221\n",
            "\n",
            "[EPOCH: 21], \tVal Loss: 3.3359\n",
            "\n",
            "Train Epoch: 22 [0/5639(0%)]\tTrain Loss: 6.753999\n",
            "Train Epoch: 22 [1600/5639(28%)]\tTrain Loss: 7.071431\n",
            "Train Epoch: 22 [3200/5639(57%)]\tTrain Loss: 8.092751\n",
            "Train Epoch: 22 [4800/5639(85%)]\tTrain Loss: 7.453084\n",
            "\n",
            "[EPOCH: 22], \tVal Loss: 3.2344\n",
            "\n",
            "Train Epoch: 23 [0/5639(0%)]\tTrain Loss: 10.136086\n",
            "Train Epoch: 23 [1600/5639(28%)]\tTrain Loss: 7.646594\n",
            "Train Epoch: 23 [3200/5639(57%)]\tTrain Loss: 6.469287\n",
            "Train Epoch: 23 [4800/5639(85%)]\tTrain Loss: 6.942750\n",
            "\n",
            "[EPOCH: 23], \tVal Loss: 3.2342\n",
            "\n",
            "Train Epoch: 24 [0/5639(0%)]\tTrain Loss: 5.264232\n",
            "Train Epoch: 24 [1600/5639(28%)]\tTrain Loss: 6.631558\n",
            "Train Epoch: 24 [3200/5639(57%)]\tTrain Loss: 5.486377\n",
            "Train Epoch: 24 [4800/5639(85%)]\tTrain Loss: 5.229579\n",
            "\n",
            "[EPOCH: 24], \tVal Loss: 3.2596\n",
            "\n",
            "Train Epoch: 25 [0/5639(0%)]\tTrain Loss: 7.555954\n",
            "Train Epoch: 25 [1600/5639(28%)]\tTrain Loss: 7.004234\n",
            "Train Epoch: 25 [3200/5639(57%)]\tTrain Loss: 6.157148\n",
            "Train Epoch: 25 [4800/5639(85%)]\tTrain Loss: 7.299322\n",
            "\n",
            "[EPOCH: 25], \tVal Loss: 3.6323\n",
            "\n",
            "Train Epoch: 26 [0/5639(0%)]\tTrain Loss: 8.230108\n",
            "Train Epoch: 26 [1600/5639(28%)]\tTrain Loss: 5.858377\n",
            "Train Epoch: 26 [3200/5639(57%)]\tTrain Loss: 7.276484\n",
            "Train Epoch: 26 [4800/5639(85%)]\tTrain Loss: 6.008001\n",
            "\n",
            "[EPOCH: 26], \tVal Loss: 3.3348\n",
            "\n",
            "Train Epoch: 27 [0/5639(0%)]\tTrain Loss: 7.308729\n",
            "Train Epoch: 27 [1600/5639(28%)]\tTrain Loss: 4.246066\n",
            "Train Epoch: 27 [3200/5639(57%)]\tTrain Loss: 4.726641\n",
            "Train Epoch: 27 [4800/5639(85%)]\tTrain Loss: 6.435086\n",
            "\n",
            "[EPOCH: 27], \tVal Loss: 3.3771\n",
            "\n",
            "Train Epoch: 28 [0/5639(0%)]\tTrain Loss: 6.590071\n",
            "Train Epoch: 28 [1600/5639(28%)]\tTrain Loss: 5.937527\n",
            "Train Epoch: 28 [3200/5639(57%)]\tTrain Loss: 7.616859\n",
            "Train Epoch: 28 [4800/5639(85%)]\tTrain Loss: 5.410349\n",
            "\n",
            "[EPOCH: 28], \tVal Loss: 3.2331\n",
            "\n",
            "Train Epoch: 29 [0/5639(0%)]\tTrain Loss: 6.237221\n",
            "Train Epoch: 29 [1600/5639(28%)]\tTrain Loss: 6.822154\n",
            "Train Epoch: 29 [3200/5639(57%)]\tTrain Loss: 4.909613\n",
            "Train Epoch: 29 [4800/5639(85%)]\tTrain Loss: 6.406476\n",
            "\n",
            "[EPOCH: 29], \tVal Loss: 3.4548\n",
            "\n",
            "Train Epoch: 30 [0/5639(0%)]\tTrain Loss: 6.627069\n",
            "Train Epoch: 30 [1600/5639(28%)]\tTrain Loss: 5.792988\n",
            "Train Epoch: 30 [3200/5639(57%)]\tTrain Loss: 6.953719\n",
            "Train Epoch: 30 [4800/5639(85%)]\tTrain Loss: 6.190244\n",
            "\n",
            "[EPOCH: 30], \tVal Loss: 3.3814\n",
            "\n",
            "Train Epoch: 31 [0/5639(0%)]\tTrain Loss: 6.542247\n",
            "Train Epoch: 31 [1600/5639(28%)]\tTrain Loss: 6.098964\n",
            "Train Epoch: 31 [3200/5639(57%)]\tTrain Loss: 5.375916\n",
            "Train Epoch: 31 [4800/5639(85%)]\tTrain Loss: 6.146450\n",
            "\n",
            "[EPOCH: 31], \tVal Loss: 3.2418\n",
            "\n",
            "Train Epoch: 32 [0/5639(0%)]\tTrain Loss: 5.122211\n",
            "Train Epoch: 32 [1600/5639(28%)]\tTrain Loss: 4.893905\n",
            "Train Epoch: 32 [3200/5639(57%)]\tTrain Loss: 6.020857\n",
            "Train Epoch: 32 [4800/5639(85%)]\tTrain Loss: 5.098991\n",
            "\n",
            "[EPOCH: 32], \tVal Loss: 3.2569\n",
            "\n",
            "Train Epoch: 33 [0/5639(0%)]\tTrain Loss: 6.407697\n",
            "Train Epoch: 33 [1600/5639(28%)]\tTrain Loss: 4.864749\n",
            "Train Epoch: 33 [3200/5639(57%)]\tTrain Loss: 5.359983\n",
            "Train Epoch: 33 [4800/5639(85%)]\tTrain Loss: 4.995404\n",
            "\n",
            "[EPOCH: 33], \tVal Loss: 3.2032\n",
            "\n",
            "Train Epoch: 34 [0/5639(0%)]\tTrain Loss: 5.497859\n",
            "Train Epoch: 34 [1600/5639(28%)]\tTrain Loss: 8.622902\n",
            "Train Epoch: 34 [3200/5639(57%)]\tTrain Loss: 4.797528\n",
            "Train Epoch: 34 [4800/5639(85%)]\tTrain Loss: 4.273094\n",
            "\n",
            "[EPOCH: 34], \tVal Loss: 3.2630\n",
            "\n",
            "Train Epoch: 35 [0/5639(0%)]\tTrain Loss: 5.695134\n",
            "Train Epoch: 35 [1600/5639(28%)]\tTrain Loss: 6.590923\n",
            "Train Epoch: 35 [3200/5639(57%)]\tTrain Loss: 4.568226\n",
            "Train Epoch: 35 [4800/5639(85%)]\tTrain Loss: 4.570922\n",
            "\n",
            "[EPOCH: 35], \tVal Loss: 3.0804\n",
            "\n",
            "Train Epoch: 36 [0/5639(0%)]\tTrain Loss: 5.665848\n",
            "Train Epoch: 36 [1600/5639(28%)]\tTrain Loss: 4.581738\n",
            "Train Epoch: 36 [3200/5639(57%)]\tTrain Loss: 5.613245\n",
            "Train Epoch: 36 [4800/5639(85%)]\tTrain Loss: 4.156966\n",
            "\n",
            "[EPOCH: 36], \tVal Loss: 3.0815\n",
            "\n",
            "Train Epoch: 37 [0/5639(0%)]\tTrain Loss: 4.398730\n",
            "Train Epoch: 37 [1600/5639(28%)]\tTrain Loss: 5.958748\n",
            "Train Epoch: 37 [3200/5639(57%)]\tTrain Loss: 5.260807\n",
            "Train Epoch: 37 [4800/5639(85%)]\tTrain Loss: 4.738435\n",
            "\n",
            "[EPOCH: 37], \tVal Loss: 3.2934\n",
            "\n",
            "Train Epoch: 38 [0/5639(0%)]\tTrain Loss: 6.084547\n",
            "Train Epoch: 38 [1600/5639(28%)]\tTrain Loss: 4.414151\n",
            "Train Epoch: 38 [3200/5639(57%)]\tTrain Loss: 4.598698\n",
            "Train Epoch: 38 [4800/5639(85%)]\tTrain Loss: 6.171483\n",
            "\n",
            "[EPOCH: 38], \tVal Loss: 3.1224\n",
            "\n",
            "Train Epoch: 39 [0/5639(0%)]\tTrain Loss: 5.026506\n",
            "Train Epoch: 39 [1600/5639(28%)]\tTrain Loss: 4.107264\n",
            "Train Epoch: 39 [3200/5639(57%)]\tTrain Loss: 5.565334\n",
            "Train Epoch: 39 [4800/5639(85%)]\tTrain Loss: 3.699817\n",
            "\n",
            "[EPOCH: 39], \tVal Loss: 3.4399\n",
            "\n",
            "Train Epoch: 40 [0/5639(0%)]\tTrain Loss: 4.352367\n",
            "Train Epoch: 40 [1600/5639(28%)]\tTrain Loss: 4.038260\n",
            "Train Epoch: 40 [3200/5639(57%)]\tTrain Loss: 4.613454\n",
            "Train Epoch: 40 [4800/5639(85%)]\tTrain Loss: 4.440111\n",
            "\n",
            "[EPOCH: 40], \tVal Loss: 3.2054\n",
            "\n",
            "Train Epoch: 41 [0/5639(0%)]\tTrain Loss: 4.930134\n",
            "Train Epoch: 41 [1600/5639(28%)]\tTrain Loss: 4.494182\n",
            "Train Epoch: 41 [3200/5639(57%)]\tTrain Loss: 5.781406\n",
            "Train Epoch: 41 [4800/5639(85%)]\tTrain Loss: 3.961711\n",
            "\n",
            "[EPOCH: 41], \tVal Loss: 3.0847\n",
            "\n",
            "Train Epoch: 42 [0/5639(0%)]\tTrain Loss: 3.994271\n",
            "Train Epoch: 42 [1600/5639(28%)]\tTrain Loss: 3.903566\n",
            "Train Epoch: 42 [3200/5639(57%)]\tTrain Loss: 4.602127\n",
            "Train Epoch: 42 [4800/5639(85%)]\tTrain Loss: 3.735210\n",
            "\n",
            "[EPOCH: 42], \tVal Loss: 3.1218\n",
            "\n",
            "Train Epoch: 43 [0/5639(0%)]\tTrain Loss: 3.848373\n",
            "Train Epoch: 43 [1600/5639(28%)]\tTrain Loss: 2.925149\n",
            "Train Epoch: 43 [3200/5639(57%)]\tTrain Loss: 4.606752\n",
            "Train Epoch: 43 [4800/5639(85%)]\tTrain Loss: 5.728879\n",
            "\n",
            "[EPOCH: 43], \tVal Loss: 3.1296\n",
            "\n",
            "Train Epoch: 44 [0/5639(0%)]\tTrain Loss: 3.685476\n",
            "Train Epoch: 44 [1600/5639(28%)]\tTrain Loss: 3.753284\n",
            "Train Epoch: 44 [3200/5639(57%)]\tTrain Loss: 3.518133\n",
            "Train Epoch: 44 [4800/5639(85%)]\tTrain Loss: 4.733437\n",
            "\n",
            "[EPOCH: 44], \tVal Loss: 3.1305\n",
            "\n",
            "Train Epoch: 45 [0/5639(0%)]\tTrain Loss: 3.816947\n",
            "Train Epoch: 45 [1600/5639(28%)]\tTrain Loss: 4.880400\n",
            "Train Epoch: 45 [3200/5639(57%)]\tTrain Loss: 3.969789\n",
            "Train Epoch: 45 [4800/5639(85%)]\tTrain Loss: 4.291212\n",
            "\n",
            "[EPOCH: 45], \tVal Loss: 3.0592\n",
            "\n",
            "Train Epoch: 46 [0/5639(0%)]\tTrain Loss: 4.725047\n",
            "Train Epoch: 46 [1600/5639(28%)]\tTrain Loss: 4.559823\n",
            "Train Epoch: 46 [3200/5639(57%)]\tTrain Loss: 6.143219\n",
            "Train Epoch: 46 [4800/5639(85%)]\tTrain Loss: 4.177722\n",
            "\n",
            "[EPOCH: 46], \tVal Loss: 3.0924\n",
            "\n",
            "Train Epoch: 47 [0/5639(0%)]\tTrain Loss: 2.968869\n",
            "Train Epoch: 47 [1600/5639(28%)]\tTrain Loss: 4.040763\n",
            "Train Epoch: 47 [3200/5639(57%)]\tTrain Loss: 3.619991\n",
            "Train Epoch: 47 [4800/5639(85%)]\tTrain Loss: 3.875939\n",
            "\n",
            "[EPOCH: 47], \tVal Loss: 3.1060\n",
            "\n",
            "Train Epoch: 48 [0/5639(0%)]\tTrain Loss: 3.346550\n",
            "Train Epoch: 48 [1600/5639(28%)]\tTrain Loss: 4.229281\n",
            "Train Epoch: 48 [3200/5639(57%)]\tTrain Loss: 4.288584\n",
            "Train Epoch: 48 [4800/5639(85%)]\tTrain Loss: 4.208843\n",
            "\n",
            "[EPOCH: 48], \tVal Loss: 3.0642\n",
            "\n",
            "Train Epoch: 49 [0/5639(0%)]\tTrain Loss: 2.821195\n",
            "Train Epoch: 49 [1600/5639(28%)]\tTrain Loss: 3.445683\n",
            "Train Epoch: 49 [3200/5639(57%)]\tTrain Loss: 3.059009\n",
            "Train Epoch: 49 [4800/5639(85%)]\tTrain Loss: 3.804323\n",
            "\n",
            "[EPOCH: 49], \tVal Loss: 3.0802\n",
            "\n",
            "Train Epoch: 50 [0/5639(0%)]\tTrain Loss: 3.403077\n",
            "Train Epoch: 50 [1600/5639(28%)]\tTrain Loss: 3.973871\n",
            "Train Epoch: 50 [3200/5639(57%)]\tTrain Loss: 2.706615\n",
            "Train Epoch: 50 [4800/5639(85%)]\tTrain Loss: 4.390205\n",
            "\n",
            "[EPOCH: 50], \tVal Loss: 3.0549\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vIPjhaK4JA3"
      },
      "source": [
        "best_model = torch.load(\"/content/drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection/saved_model/{'l1': 128, 'l2': 128, 'lr': 0.0010379776478907162, 'batch_size': 16}val_loss: 1.8288547613910426\")"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po8-4smr4CRF"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b91-FVjB5H6a"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93XvOVSO5J_v"
      },
      "source": [
        "def predict(model, test_image, idx, plot=False):\n",
        "    image_pred = torch.from_numpy(test_image[idx]).type(torch.FloatTensor).to(DEVICE)\n",
        "    image_pred /= 255.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_pred)\n",
        "        print(prediction)\n",
        "    image_pred = image_pred.cpu().numpy()*255.\n",
        "    prediction = prediction.cpu().numpy().reshape(30)\n",
        "    # answer = val_data[idx][1]\n",
        "\n",
        "    if plot:\n",
        "        fig, axis = plt.subplots()\n",
        "        prediction_plot(image_pred, prediction, axis,\\\n",
        "                        'prediction plot for {}th image in val_data'.\n",
        "                        format(idx))\n",
        "\n",
        "\n",
        "def prediction_plot(image, keypoint, axis, title, answer=None):\n",
        "    image = image.reshape(96, 96)\n",
        "    axis.imshow(image, cmap='gray')\n",
        "    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n",
        "    # axis.scatter(answer[0::2], answer[1::2], marker='x', color='r', s=20)\n",
        "    plt.title(title)\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "0FagIGNC5MdQ",
        "outputId": "ac564fc4-dc06-43eb-caf8-4919a35f7ff3"
      },
      "source": [
        "predict(model=best_model, test_image=test_image, idx=600, plot=True)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[64.6764, 36.0455, 30.2624, 34.6959, 59.3086, 37.2831, 71.4462, 37.6534,\n",
            "         37.5096, 35.4970, 23.7864, 35.7586, 56.1714, 27.4597, 76.2876, 31.1136,\n",
            "         39.9739, 25.3867, 19.0632, 27.7206, 53.0837, 54.4316, 59.4383, 77.9416,\n",
            "         38.5160, 76.7086, 50.5363, 71.9189, 49.3605, 80.2081]],\n",
            "       device='cuda:0')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARAAAAEICAYAAACXj6vjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9abhkaVUm+n4xx4kTmacyq4AqKKoKoZhBwQEBtRSUUQpt5TbaNtgo4kBrtzSKeNXrSPv0bcFrX2laUUEQcaQdsFGwtEUFRECEEqSYap4y80wxR3z9I+Ld8e511rcjTlZmREqd9TzxxLT3N693vWt9ww4xRhzJkRzJkZyNlNZdgCM5kiP5lytHAHIkR3IkZy1HAHIkR3IkZy1HAHIkR3IkZy1HAHIkR3IkZy1HAHIkR3IkZy1rB5AQwqdDCE+dff6hEMIvnWU6HwkhXHNOC7c4zytDCDGEUDlH6T00hPDBEMJuCOHfn4s0VyWHbYsQwmtDCP/3+S7XPZEQwgNDCHshhPKK8/3VEMJPnsV9MYTw4PNRppSsHUBUYow/HWP8tkXXeQ0cY3xkjPG681a4eyghhB8LIfz6gsteDuDPY4ztGOPPn6N8HxRC+MMZKN0VQvhZ+e9ECOH3Qgj7IYTPhBC+ydz7TbPf90MIvx9COCH/ZcB/NhJjfEmM8SfO9v5VSIzxszHGzRjjeN1lOZdyLg3fOQWQc2WJ78VyBYCPnM2NXtuHEGoA/hTAuwDcD8ADACiI/TcAAwD3BfDNAH4xhPDI2b2PBPDfAXzL7P8OgP//bMp2JJ/DEmMsfAH4NIBXAPgogNMAfgVAY/bfNQBuAvADAG4D8EZMQekHAdwA4G4AbwVwQtL7FgCfmf33yln6T53992MAfl2ufTKAvwZwBsCNAF4I4MUAhpgO/D0AfyDlZDp1AK8GcMvs9WoAdVPm7wdwB4BbAXxrQf2vA/AzAN4LYAfA21gfAFcCiAAqs++XAfifAE4B+ASAb5/9/vRZeYezMn/IyeddAMYAerNrrgZwHMAbANw5a7MfBlCaXf9CAO8G8HOztvxJJ80XA/jfiXq1ZmW6Wn57I4BXzT7/NIA3y3+fN7u+PbtuAqA7K+vLpS1eAOCzAO4C8MqCdv1Vlln65OXSJ88F8EwAH5+15w/JvV8M4G9m4+JWAL8AoCb/fw2AjwHYxhT0/gLAt8n//w7A9ZiO5/8F4IpEGW3/XgfgJ2btvgvgHQAuTtx7PYBny/fKrB8fN/v+W5jqzDaAvwTwSK9tFujmf5rV/5ZZnSKAB8/+exaAD2A6Zm8E8GNy32dn1+7NXl866993zcbSXQDeBGBrYRmWBJB/BHA5gBOzxtOOHwH4z5gqbRPA9wL4W0ytXR1TK/Ybs+sfMSvwl8/++6+z+w8ACKbWeBfA8wFUAZwE8PmpBkYeQH58Vob7ALgEUxD6CVPmH5+l+0xMretFBQByM4BHYap0vyNltAPsLzEdsA0Anz8bMF/lgWNBXjrQ34ApYLVneX0cwIsEQEYAXorp4Gw66b0eU2V/+2xQXAfg0bP/vgBAx1z/MswB+W0AfsD8vwfg8ba9TVv8j9k4eCyAPoCHLwkgIwA/MuuTb5+13ZtndX8kpmB11ez6xwN4wqzeV2KqrN83++9iTJXm62f/fy+mwP1ts/+vxRTcHz77/4cB/PUhAOQGTMG9Ofv+qsS9PwLgTfL9WQCuNyDWxtzYffAwAIKpUbod83H5ZuQB5BoAj8bUoD9mdu1zvXrNfnswgK+elecSTMfyq88VgLxEvj8TwA1SyAFmjGT22/UAniLfL511YGXWqG+R/2gFPQB5BYDfWzT4EgByA4Bnyn9PA/BpKXPXNN4dAJ5QoNSvku+PmJW5rB2BKcCOAbTl2p8B8KtnAyCz9AcAHiH/fweA6wRAPrsgvXfM2v4ZAGqYWqxPzj5/GYDbzPXfLum/U/t99tvNAK5ZACAPkN/eC+BfLwkgXQDl2ff2LK0vkevfj5kCOGl9H8cKgH8L4G/kv4CpBWa7vh0zEJ59L2FqQK5w0s36V/rnh+X/7wLwJ4kyPRhTA7gx+/4mAD+SuHZrls/x1Ph27nm9GZdXQwDEuf7VAH7Oq1fi+ucC+EBRGWKMS8dAbpTPn8GUqlPujDH25PsVAH4vhHAmhHAGU0AZY+pHX6ZpxRj3MaVMnlyOKRCcjVw2K2eqzHfHGEfyvQNgsyA9W/8qppbO5nkqxrhrrr3/soU2cvEsH1sPTe9GFEsXwF/FGN8eYxwA+C+YMrmHY8omjpnrj2E66LHE/ym5TT4valeVu+M8WNmdvd8u/3eZVgjh6llg+LYQwg6m7hb7w46xiKl7RLkCwGtkfJ7CFGSW7ael6hdj/ASmY/9rQwgbAJ6DKUtACKEcQnhVCOGGWfk/PbvNjqkiydUT+XGCEMKXhBD+PIRwZwhhG8BLitIPIdw3hPCWEMLNszL9+jLlWRZALpfPD8TU56JEc+2NAJ4RY9ySVyPGeDOm/lqW1qxhTybyvBFTv8wTm6eVWzAdKKkyH1Zs/YeYugQ2zxMhhLa59ubZ50VltnLXLB9bj5vl+6I0/6Hgmo8DqIQQHiK/PRbzIO5HZt8BTGdzMKW3H18y7/MpvwjgnwA8JMZ4DMAPYQoCwHSMPYAXhhCCfsd0XH2HGZ/NGONfn4dy/gamLvi1AD46AxUA+KbZb0/FNM51JYt7iLRzuoTp2FB5M6bxuMtjjMcBvFbS9/rup2e/P3rWpv9mmfIsCyDfHUJ4wGwa75UAfrPg2tcC+KkQwhUAEEK4JIRw7ey/3wbw7BDCk2czBD9eUIY3AXhqCOF5IYRKCOFkCOHzZ//dDuBBBWX4DQA/PMv7Ykxdp0VTqEXyb0IIj5gB3o8D+O1opvZijDdiGmv5mRBCI4TwGAAvknxvB3BlCGGpNp+l/1ZM27I9a8//eMh6/DqAJ4QQnjpby/B9mALT9TP297sAfjyE0AohPAnTQf3G2b1vwtR6flkIoTWr9+8Kw1rUB+dT2pjGOfZCCA8D8J3y3x8BeHQI4bmzmanvxnQGivJaAK+Q2abjIYRvPE/lfAumAd3vxIx9SPn7mLLvDUyV97DyVgAvlHH5o+b/NqaMuBdC+GJMQYtyJ6ZB8AeZ6/cAbIcQ7o+pu7tQlgWQN2PqT38SU7eiaJHLazBFvneEEHYxDWZ+CQDEGD+CaYe+GVMEPY08vcwkxvhZTOMt348pzfwg5hbxlwE8YkZDf9+5/ScB/B2mFvjDAP5+QZkXyRsx9UtvwzRAmlrk9XxMrcktAH4PwI/GGP9s9t9vzd7vDiH8/ZL5vhTAPqbt/leYttvrly10jPFjmFqS12La1tcCeM7MnQGmPnwT0xjQbwD4zlkfsa9egimQ3IHpAPsuSf5nMAXpMyGEly1bpnMkL8NUIXYxDdpmBi3GeBeAbwTws5gq6CMwHQv92f+/h2nQ/y0zqv6PmMaIzrnEGG/FdLboicgb3Tdg6nLcjOns5t+eRdpvxzSu8S5Mg8LvMpd8F6bGYRdTA/pWubcD4KcAvHvWf08A8P8AeByms0J/hKlxWShhFjBJXxDCpzENQP1Z4YWfoxJCuA7T4OdZrZA9kvXKjPHdBOCbY4x/vu7yfK7JBbUS9UiO5FxICOFpIYStEEId8/jIoa38kSyWIwA5ks9F+VJMXe27AHwtptO/3eJbLjwJ071he87r7esuG2WhC7MwgRCejmncowzgl2KMrzoXBTuSIzmSC1/uEYDMIvsfx3QF200A3gfg+THGj56b4h3JkRzJhSz3dPPbFwP4RIzxkwAQQngLZnPe3sWlUimWy/7O6BACQggol8solUrZO1+UGCPG4zFijJhMJphMJtnnlDDtUqmU5cHvTHu6XCBX1uyd9/OlaXovAFm5xuMxJpMJxuMxRqMRxuMx+v0+YoyF9/O9YKXggbKwPqyjtpl9t/W1deL9tjz6WdsuVQ/tu1RZUn3m3ZNqB/ubvU7HDH/TsaPX2nHltXmqrN533u/1b0pYp1S72bz0fTAY3BVjvKQwg3Mk9xRA7o/8aribMJuypYQQXozppi6USiWcPHnyQIcQMKrVKra2ttBoNHD8+HFsbGyg0Wig1WoBmIPH9vY2er0eer0eut0uRqMRer1edo0qCIGoVquh2WxmeVSrVWxsbKBWq6FSqaBSmTbFZDJBCAGNRgPlchn1eh21Wg3VahW1Wg3lchmVSgWlUgnVahWlUgn1eh3VahWVSgW1Wg0xRvR6PYxGI2xvb6PT6eD06dO4++67sb29jU996lMYjUao1WpZ2arVapYf2wUAhsMhxuMxhsNhBkCDwXQWtlKpIISAer2OSqWCRqOBRqOBer2O48ePo1QqYTwe515UiBBCrs6TySQrR71eR6vVytVVQapSqaBcLmflZztoW2p9VBlUkUejUQ78LSCyPwFgNBphMplk7coXx1WMEf1+/0B9h8Mh+v0+zpw5g+FwiF6vl70PBgMMh0N0Oh2Mx+NsLO3t7WX/d7tdTCYTDAYDxBgz4+MZE37mmFPwGo1G2VixhtHWlekpoLHvWGeKGlz254033phblXo+5bxvv48xvg7A6wCgVqvFcrmcDeLZ/9k7G6jX62UDz17LQUTFAqaN2Gg0ss8c6AByg50Asrm5iXq9nimedjgBhP8RQGq1Wtb5CiAEPipRtVrFZDLJlM5asRBCpnj1ej0rG8vBQaD3AsgGkQ481pX5s4x8lUolDIdDVCqVDHyYJssaQsgGp1cfgpQyHGWHqkyWBWj/KqCzHuVyOWsTa1k9tsG+YXrKlqjczvjL6gZMAUzzCyFgOBzmAJWGYDKZHGBiNm2v7FYWsVXveu+ztot+LpVKmRFYxGzOtdxTALkZ+eW0D0B+qfUB4eC3A42/EamHw2E2QO09tD5quajUajGpJLSIGxsbqFQq2TsVF0A2CDnAeI++qPhUKiocrS6Vwg4QW9ei+1l23qdpEeTUcqnLwhfTU6Xm9ayjZRdUQJbJAoQCs76nxOtfCyIWDDya7oGSR9s1Hdtm7JfJZJL1tzIyBSC+K0tQRmsVX0HEA5NlwCN1n20Lr22pA0Uu1vmUewog7wPwkBDCVZgCx79GfslsTnQgqmXWxiILGQwGOeDge4wxo5qqDBwsdDOUQpNNtFqt3LtlK1qejY2NjM5bBsIBpqChn8mMtE5qLWq1GsbjcQ6QqPC0eAqOnlKoO6EuFt0pltHWSxWELE8Vh22n5VFabum3tfoea1BQpdBqphTFUyittzIDflZQ0nRsXE2BVkFF30ejUXadjtFU+bw2sG5NCkAsSNrPXrtqHmSUi5jQ+ZB7BCAxxlEI4XswPZSlDOD1cbYU2hNadlJpayGBOVUfDocAkFFv/Z9+sIr649VqNXNXms0mms0marUaNjc3US6XsbGxkRuA6ipQyZvNZubCUCkbjUZSqSwYWRdGlZj0mGCnys4Bm7K4mpe6TQQ5ffE+dYOYrsYnaIkti0vVzwajrXJpPloHa3XVuqsV1r6w6aiipFwYy2DG4zEqlQpijDmQYL/QleOYI6Czbxa5BkXgp8Cn7cf/LNjqfYsYmd5Dt3DVco9jIDHGPwbwx8tezw4nc9DBYim/MhJgPjAZ//AamHnQBWk0Glkw9tixYyiXy2g2m7kgFwcS7w0hZADSaDQyV4cKpwMCQA4ATNvkwIN15OBNzTRpG9h6ab7Wmuqgt9em7mVdtDyahjf4l3FhbL94VlzLZZUuVXdv5ofgwHFiLbQFLpuefVmmp2l47MCW09bXY4+2naxbUsRQUr+poV2VrPQMU+1sG0y1CEo6Sl+VlmQ8HmcMhPdSuUejEdrt6W56AsfW1ha2trawsbGBEydOZJSfcRaN1qtSaRBV4yisB5D2T6mQZFPqbzNtWkMdXApo1vXRNuRAZ7k4+9JoNNBsNnOAppbe9gPbge1rXQTNy3P5NP1lLLTWhffZGRhtQwt4vM4CBBmIN0vBcWXTsu1IRsfxpMF1vmxA39ZHy8/fLeAqS7L9rf1gx5l92bwsg1mVrPwQZOv7stLqS1PxAGTKyN/1BeBA4ymtpdKnpmFZDn3p1KzOSKhV1nLazrR10OlKinVJFEAIMgRQrQ/zUCVPuRme9fMsoWfVlcF4FlljU16ay7ATFY9xpK6z6Xq/Laq37Xftk2XS8OITmkZK2Sns31Qdl6m7ZULLtN/5kLUAiFoCRc0Yp3P5k8kE/X4/+90qERVSBwWtBgOJrVYLx48fx7Fjx9But7P1EeouVKvVXMPTmtMqqeXVwB2BgzNBHpXs9XrY399Ht9vN1huoO8P8VVmVsfA6GwhVsCTIWQaiZeX1OvAVJFgO9ocFI9bdllXrbGMmBGlvwFt3Qmdk7DX2WmVTWi8rer+6eCyHZXv8XceoBspjjKhWq9l6jhQTsWXXqXf2pZ0WVkDW+JcyQn63wfl1AgdlLY9hsNaRousUNMaRQl/b+LqWwk6/etOk2rFK61UB9D5Loz3w4HeChi4A8yivpdfqylnabRVH66vxD5bVgqzSakvjvc8KNDY+oOWwwGPbme3C+7T/LUjq73bM6P0WjFLjzJbPxn6K0rDjdJl7tI8VuD2w81iQx6aL0vDKu0pZ23NcdHBTLHW2Pt1oNMoNCI0BtFotNJtNtFotbGxsoNlsZqxDV3p69F7dAcYFdLCpNeaAn0wmWdSelmk0GmE4HGJ/fx9nzpzB/v4+tre3sbu7i9FohH6/n2M1jUYjFwcheJCB2AVpbDe2HYFDp3KVDeg9OgvEuunCOAXjVCDRiqeglubTEmu6i6ym5wKoMrI+qd+su6hxDqal6Wu/WnDR/1gH/W4NiG0nL9Zh3W+PdVlwYb529tBrKxsHOp+yFgDRge25Mx6FBeaDiAqt06ycrrXAocur7eyEDnydwgTmi728mRK1vBrnGA6H6Ha7mfuyu7uL3d3dbN0KpxMZoLXrNVS5bLzBE105qlRdBz6przISXe+hgOYNYr5bENHf7Ev7OQUkXv8WjRWbpioix46d8eJ/nkvH5QRcuKig4TEu+x9BxHMjvHZQsLPKr/fp/7adLbNMAcgqZa0MhGJZCH+zNJXfaTm5l6XZbGJzczOLASiI0I2xUXUFIv0dOLgBzKOeHBRc9NbtdtHv97G3t4dOp4Pt7W3s7+9ncRA7qO3SdQAZ47BU1bImvtuVrB5b4v36mfXVtTOWLnv9ZN0HDzTsfXb5OX+z7WwV33MN+W5dXDtrZZmrKr0aH7tYjDODHuuyzEPrkWIhy7o69j0lalA88LhXzMJ4ldUZGDvTwd94T6lUyhaJ0VVpNps4duwYGo1GFjBVN4YshIur7MpE/sbyeaIDitfQgmnA9PTp09jf38epU6ewt7eXbcwiaKi7QWCzg4DlYn1tXIJtQgCx6Wl9LCBYF0rbgaKKoYv++FuKoWgf2z4vAhw78FN9YMFDZ+rsf3ZRlrYHr2NglH3guWKpGJGCEsu2rGumn21AViXFerQvNJ1lGN25lrUAiDdgPHpr3Qy+uMir2WyiXq8fiHXoS/e8aHxD0/cGdYqOq689HA4xGAyyXcGdTidjHb1eD/1+H4PBAKPRKBukmr4dRHYnqaWxFI1z2Jetjw5y1teb/lUACSHkppFT4lF83q/tZoGX13gW01Mim2ZKlrXkTM9zD/ifNw7VbfHckCIwSLGp1LUpQLJupMa2lq37uZSVA4hdxwEgN5h1yXEIIYtjVKvVbA8LXRhd5EV3ptVqZUFVTuW22+3sGs9SKoB5VNQG6xgwJcM4deoUTp8+jd3dXdx1113odru4++67c7EPYD5dykFJF4jT0tx6zjNDeE+MMXMzdKaIIMk4ENspJQQLMhDdi6NAY+vrgb4CjwUSlSLL7F2r/ymo2SBm0X4jz/3xlJV5sI3ZvgqsrJO6hl6auu7HW6fktactq8dmLHuzS+t1jK4aPIA1xUAWVZaNRcpPJeEGN3vuBJesK9jYBWAaKPQ6apHV1M4eDAaZkvf7/YyB9Pv9LIhql9tbCsx2YHnIOuzaEs8i28CvF+i196amYz3F99iEjX0APgMpAoWzEVsuW76UaB8um48X+7J9Zt8tA9EYVhHrsL/rd2/MFbGrdbgulJUCiNJ/66PaczQIEFwU1mq1cNlll+XO9dDpWR4ApGd+NBqNA9N3nrIAB48Z4MseTjMej9HpdDAYDHDLLbdgZ2cHp0+fxunTp9HtdrODaziVRrbAKWZO3XIGh24Q14z0er2cxdfgr65rIYMgI9EzRfR+Wkzrx1tXx7N6vI9xGU1HA7aqYFbsjMthXBLvehsQttd6QGbHnQ3YFpVF3T7vOo4RlktXEvN/zxil6qfXWtanMR5lscvU43zJ2hmIUjvPJyVVp3tCQNEdqFQmpeW6eIxiG1qtql6j1kOnabkHZ39/H4PBAHt7e9jd3cXe3l4W9+A1qsCq+Jw6Zed7IGItPctqY0GWeXiKrHW1lnSZAae03StHinl46Xifz0Y0JqG/pRQ89TqM2LZV1qGrae3Y4rVnW2cCiXWBPLnXAIhKqpHtwFT/XWczLHCoxS4a2CnwUOYxmUyPsmOgdG9vD8PhEDs7OxgMBlncg1O3ZB1kRAAyt6vZbGafGafo9XqZ26KrVsnAWK9arZYtVac7p6DpbT+3qxlt0NS2kVo+VVDPQi7junjtbYXgXhRM9VhjKn3LJrVPvfKqsfLiObbcqc8EEZ0IsEagyCXlu7aHvdayJxv0LmKB51PWAiBK8YDFU6fW0jL2Yd81LuDNtDBNTd/LX4NidCs6nQ52d3fR7/exvb2Nfr+PnZ2dHPtQhsOt/5wpIghQ2TnYFEDoKtGlIzjogUY2iJqqM8uhroudvl7EIFKAbhV1kQvg9T//K2ICqnSLyqfAZ6c0rXJ6dbGAouW0rpJXF5uuna1Jld2Cm1c32xbW/VqWTZ4PWfl2fg5cGwPxxMYher0eQgjZwcUU3SrPd9JuKqvtMJsHkV1nRBgo5RTt3t4e+v1+5sJwmjbG+QIlrjUhAGi8hoqv8RYbMCVg0t3RjXIEIDt7kmIUqlTKPvS+lGLa/lmWbaiyeTM3ts9V6b2+VyVUGs9rNC2bLt/VYrPOer4sXRBtv1TbWPHcKC+QnQIRLR9BR9vDMphUGusCkZUzEF0PYeklf9OBomsueEI2FZWWW5WK9F4BRA+asaxD2QbzmUymu4F1qnZnZwd33303BoMBdnZ2sj0vg8EgWxrNfTkEDg10apk04Obt32A6jPfQ/WGaXL2qAWe7TB+Yxy8UOCxzAQ6eq+m5k541VqX32IgX5LTfPaXQ3ywD0XFh01JmZMumrI9tpov2dJpWwdjGWlJsVcthwYcxtKKYjWVJi9rDy09jVauStRwoZJccL+NiUMkJGhSebDYcDjNLosuTlY3ovgemyzT4IiAQQBgD0RgFy09lZDk5Q8TZI1Va1kcZVepRC8pA7NS0Kr7HDDwLbIFEGYmWa9FS6CIqn+rv1AyBZQj6uYjKq3hs0iuzuhdaZwUJIH9ivLaVslubj/1N8/FWh3r9o+VJBdDt76n+XrWsFEDK5TI2NzfR7XYzF4FBQ+AgkPAzEZz3ESx0APAzFfiiiy5Cs9lEu93GYDDIziHVPDglRlCaTCZZYJMrSDXOsbu7mwOPzc3NjA1xMRePTeRCOOZBl0ddIE4HE5QIHDx+UZ+Pw/QtRbbtpqJtY4PParE0YKlgbSUFMMsCizfYtQ72ECWrXLYM1sWx7MSOJwUPMmGyxxhjbtEewZ0L+shivfNAbLm8eBTvV+DSMrJMFO/sDwt2Np+ieNL5krUwECI9EVobAji46En/0wFiV+TpLAS3ztOCA8h+U4urrgvjLLrmw7IP5gXMl5RrjKLVauVYjwIlRR9LoYNRXQ27HJ8vC6wsj7XkKQvnWV4OULW2tq3Pp3iDP8VGrHWmLFIejw0pW0i9dJ/MIveFYmNRReXWMpA1W8D0jOqidFclKweQWq2WreSk9VfKyOCj3Wlbq9Vw7Nix3IyLNp6NZpOxkFWou6EApIFaTtuqVSFrYDlDmAdIGRQleNRqtWy5PNMlUHAdSafTwalTp7LzQfQZLbyfu4qZpp7zYa2XFRtEti6LZW6qWDbuxHZJKUqRz20ts7pxFhSoJB7lt0pPRaay6X/WfbDp2BkMLYPGSfh0QGUoHB8ADhiElJumZbIvdSE1hqGL9rS+zENje4d1Qc+HrDyIamcLNDhHhkJl0u35Gl/wAERBAZi7J5xR0YFlB5IqDoNd7FQyENJrsppyuZx7pKTuRaHrov4xWQ2XvutSd1VoXXBmTxmzQVhP2azPbxlekWVMBSw9mk7x2MOyVNqyDC27tfiWSaXYiRXbx7bf9V6CCN/ZB8B8No/PK6LLZdOx9Uu1tR33fC+Xy9kxAxyTXt2t27ouFrLWhWQ6QNhpXCy1ubmJjY0NtFottNvtXHyBCqzUnVaTAKAvZQFkFfzOcqj/H8L82bg6U8IO1iXkuqSbYAXMXZdOp5Nt9+90Ouh2u9kmOx00CpgKSNZtoXi/qXJw4CsD4X18T82SaFzB9hMwZznWAtp7iyyiKpS911sj5FF3tdQaMPfAVdtH+1vLw3vp8o7HYzQajcx9VXeUY0LZjjIpyxy8cmsbWheebWBB1RoG7TOvr863rO1EMu1I9f2pPO12G+12G61WC1tbW9mDoVRxdQCqC8LYBa0+X/rwZMYhLEVnxzAvXbfBctpZEfVXuRSdbg8XmXGbP88NIVUmADEQaw9DUsbG8mk5PRfEWjT97FnClCJTbNDQDuRFgGPFsk9bfgAHrLveZ+/ndQQAvYfl1e/KTvVaKrbOrLGNyAoY9Ldn3Gr5aXisgdR206leyzBpXDQYrKxS09P/1yErBxA7fUmxFJ5rHzY3N7MYCJ8s550Qzg7lpjzdFs/GJqjQivT7/Wz2RRGfeXBA6Jmh+q4uis4m2XUkjH3whHYOLpZLzy2xC86sO8G24rtnae3gtwqv/6VcGbarB1I6m6XpLWP9isqrimLTTLkd/KztuSyQWRBh3xKIaNQ4rkIIGTvh+iIPfFkHj31Y1qff9Z5lRfvXi4mdb1n5blxdV0FF1pWRm5ubaLVaOHHiBC666CK0222cPHkyO+JwoEkAACAASURBVIFMp0i1waiUuhBsPB5nLkOn08ncCzKVvb29bGqYQKMLtcgGCCIayORgZx10HQnz1Y13utGOFoYBWAaKW60WNjc3c8/zVRCxsy4pFuCBnRc8tVQ4xUQ0TVJ3wD8k2XMPrKSUif8xbfsAdQUWvV/jArryWMti62YX8WkdOb6YDwGF/ayuLTCforXrm9geLKMGsm1dvM9WvDZledclKwcQDgo9KwPIo7/dLGfPNVXFUqupPiiAnK86mUzQbDYRQshmSvRZqExHqbMOMH3XB3sTtAgMBC6ChZ5KRtalbov3XFtvQ6Dncljw0La0AJECD++l4gU2ixiCd2/ReGB59d3WYVnR8qWsuWVvtp1sAFr/0+MSuFaEszQKKExf22oRW9RxnBLLuorYzKpkLQxE11RQiPS0xnwgVLvdxubmJkqlUnaGpz1Ih2lbpSbTYAxka2sLg8EAJ0+ezBgI97qQjdx999254BjjKOxYglIIIRdvIUh0Op3cdLB9OHipVMqOI+Bit62trSxozEAxT11T4LAWTBmAB8YhzJ+0p8FUPTdVB6I3mFUJOBPlWW+9XtPQVbM6DvRazV/L5dXNG1P24CYVZSAWRLU9S6X5QjYdm2xDDWySNXNJAt0Z9r0VBWCWWeuoMTRlXCocj3Sh1rl8XWXlAEKq7wkHty6k0uXgVokUQDQNDhidOeG1HBxcndrr9XK7ZPf397OAmRflJqAwDWUZdFUIIMB8AOug1d21fNnAqUbq+a6vIgttLZUqi1Uee61NxwaX2RaLWEiKVdjxoOl5FtuyCQ9MPAC1/9v20nbhZ43t2HagwhIQLQMB5kpuXWuPudk20ra2wvpboLUGZh2ycgBhcFHPzuA7O0hnHyxSWwZirRWDXnznEnbGKkajUfYgbsY/lIE0Go3czkxPWTlgdc2JTuUpreX9Coztdhu1Wg0nTpzITpLncvWNjY1chJ1iLaa1XCwXr9X2VPqtFteblbFCpaDfz7ax6248FsLvXtr2Gpab/ajWWBlL0St1vbafF3+wrqpVVNsHdjyybdhe2nca1LVtrvVXELXlVgCkWBaa6r/zLWuLgSgyA/ml6NZa8n9rhT0rStEpPQ4O5k3KSSCh9efzW+zOSTsg6R5ptJ6DyBtAVGCuLD1+/Diq1SqOHz+e23HLw4K0PpYB2MFrlY552vZiGRV0LcvxXBj+r6CqddPrUqCgymH/t9daJbIKVeTOaF/Za62lVqVne3iGQ+/32JF+t2BBUZaSYnsAkszca3ObN+uxalnLmai6D4aig9s7itDzhbUjiygc7+WgCSFkfizfeWoYfV361XrStp1p4bqAarWaLU7jrA3rxrro6elbW1vZYjmu/dDFaSwzcDA+wDQ9VqRUWS2UBRBlcExb25TvmqYXYLSzHQoS1kBo+su6I148IJWml453jXVLOSMG5BdwpViCsiPLuqwLoopNcNL0rJHQ9IrcGW0L/bwOFrIWBqInb1labc+2sHQ2JVahFI2t68G1IgzmNpvNbFr52LFj2X92QRo/c8MdZ1oYpOUMDXDwrA6uNCWAVCqVbGm+ffg3cHBQaR01LmJBxF6nbaplUQbiDTw7uBVQqXTK7jymoX3GshaxEe8+Wx5+tjMeRWLHhQUXDQ5rWXmvgoc1KHbq1ou3MF8VDyiLxDIPrZuyplXLWhaSKXrrYLd7P6ziK5iww5VG01IyXSs6KJTC6uBiutxAValUsuMDaElId8lACCIESE2LsyAaOE2dGaI02Bso2l72N3utMg/ryqRous2Ln62lpRIqkKUUp6j8Xh5sOxuPoOi4sbNAGpvxxHNJvLJ47WpdIy/2AhxkXgpAKhxHi1wyWz47NrSdlgGicy0rZyC07JPJJFvdx2lL+4AkDnANglJpteF1ANtIunUByEB4P4Od6jponEO38uvzYPifuja8X10Itf58NZvNwhkQb1DZAW8tpL1WAYwrazWQqy6Ml5eWRa0r258DVgG9yDXR8hcxBk1D2YJlWWQD2u/qGtu87AwJ+zkFKvY67WfvQCh9QJiyMq3XsmDhjV1bdsuYPLBdhazl0ZYqXqDPWyhm77cWQBvfNvYi8Sy/9XWpIErdrRLxPs99UKahD7P22scOar6nWIn9TZXAujNWuRbRf8A/1UvdLb7bM16WcTuXFe9aLy7j/XbYvD0Gtgjg9T+rzEUxDa9OzFcD9dY4Wia2TL3Oh6xtDSytJIOLrVYLGxsb2dmfuifEUnKKBx66p4XvFtGtX2+psP4HzN0Ry2DsVKaKLtZKzRyxDtomiz7rPXy30Xt1Uezsi5ZH6+T1j372BqauwKSF5vUKtppGinXpd2sk7DXqsgD5hVrqWvE/TUP7jIDnbYyzTAQ4eBSFVwc1avrZLk6zhoF50TXmf3Y8e6JMfdWyFICEEC4H8AYA9wUQAbwuxviaEMIJAL8J4EoAnwbwvBjj6SXSA5B/ypk+slLXgdiAVooW2gGgVoP3eesEvCCYikd9OYgtvdb62bUaqTUXHoAUDVAAB+ph66tp2JcFD15bBGRaZ69N+K7HBHoAmZKiulpZVE/Wz/a/l45nPKxrbNmpBRVNX8cVMAcvPYAo1Seat12Lwt+8uigzWYcsy0BGAL4/xvj3IYQ2gPeHEP4UwAsBvDPG+KoQwg8C+EEAP5BKRP1O0ntdiannYOieECqktwR7EQ336KwdLItYgN6XAhq9j5ZfB3QqPc9CMw3bdlofT1E1fWVsNjjtzd4UtaEtN62krsZUN07TtVsWtC7L5mv7xzMO2ubqRujUPYEvZThSTEzBQ8ul/er1l7rBGq9QlpSqv3VfbNvZ8bZID86XLAUgMcZbAdw6+7wbQrgewP0BXAvgmtllvwbgOhQAyOz+HJUn87ABVN39aoHE+vJFeVnryt/5nrKQniJHE9jTa+3LglyqfJ7ie/WiJfNA0LO4dlo8BR68fhmmoGUE8rNW6gqwXGrdPbDW8mv6tkxWgVSpWSeND3AWTdd4aHqq3LZMFtBtmSjKTorKaPPSsixSeFtXADkgsvf/iwiihhCuBPAFAN4D4L4zcAGA2zB1cez1LwbwYiAfl+B6BH2ljvGzQVarrIlyup8XXZv63RtkHntQt8wCiFUeZRA6ID22pEFbj8ZqOWy7eQHUw4rXngokwHzjHMutsYUUmFDhF/VByl1kOVTBbJxM2yYlRenba7RsFkQ84NHPnrFJjVUbNLVjxY6XC5aBUEIImwB+B8D3xRh3jILFEMIBMxZjfB2A1wFAtVqNAHLb9XnmBgOodgu/WlJvduYw4OBZiaL79FUEIHq9Tt9696sC2UCvDkYNEHrA49VNLTKnavXUNLsPxJOUO2fbRpfBWxbE/HRqW3cxW6vstYPWaZHYmQrb7iHMj27wHpewiH1513lt75XZAxVtoyIwAfIgom3lpXdBA0gIoYopeLwpxvi7s59vDyFcGmO8NYRwKYA7lkgn88t1cZV38rh9P0RZ3e+qyDq4UhSW7xYEUkDkDRQv7yKf3soybpYOLmVAyzIQjyqn8lBRRsR+4pJtAoouvksBQsq9Kcrf9p/2jcYPLMDZgGhRu3uzdPaVuteWtWhcpO7lfxokXWfA1JNlZ2ECgF8GcH2M8b/KX/8TwAsAvGr2/rYF6WQLqfjwpK2tLRw/fjw7kYsncdndo9Z6pgZ9yp/W31IWXeMM+tI0LZh5AzvFPLzpTlU2loVKwO96r/rS1g9X149rTfT0tpR7tGx7avsVAbFlFmQrnM7U+qgo+CgI2vy9z6roCmQshy6/DyHk/mfZddaEL92iQCal2zHswVi2XZmHbvlPsQ5tD2uY+F13eWteXnuuQpZlIE8C8C0APhxC+ODstx/CFDjeGkJ4EYDPAHjeooR09oXsw3v2idfQi8QbXN59dtCrUvDFQW8HQ5H18KyJdjBnJFJz+xasNA0vEKfMgwrnBUtTgVkPTIvqs4x4lnYR61CXjMphA7WLymbbw4tPsS2Ugdj21sWDlml4cZxz3S7aNsvK2ZTnXMmyszB/BSA1mp6ybGal0vS0862treys05MnT+L48eNot9vZdndlHQScFAVPoTnzs2JRXpVTDwvi/Wo5bJ787KVnH4WoZ6fyP7vJTfNTALOWRa0arZIXhPVYh2Ugi0DEqxtftL762Ay7xNsu++YhTDyIaTKZbwlguiw3DY0Hht6eHtZDz2dRILHAVOQa25jVskqq/eXd67E+SsrNAQ6OZTJVex7rqmUtT6bjQcJ87kuz2cyYiLWi3uo/TU8/63dvnwcHp1VIqxR6v/r4Nt+UZVWKrodHU8EGg0HWFowRMGDMdFPK7SmRthfg79wsYmJF4imSWml1rVIvBWjdfMi2sc/pUQBRRuLVOYRw4DEM3spibTuPnaTqrq4N01ZJgVAqXuLFOKxYNpW6VsFKXeNVysofrr2xsYF2u41jx45lr83NzdyZGDqVq3tjvAi0pu1ZWk8B9P+U2BkDdpYCkFpyBaDJZP6Qbp7ApgpDAOFuXLpwXJzlzZjYeEeKER0mwLYseLA9lHVoXID10peyrclk/pgLPf6x0+nkTobTYx7YBtx4SLZF0FX2xuMJtM8VtKzSeqDrsTb9n6KMlCtvrSukoOOtdLVpW3fK63sVjXeo0VsHC1kpgJRKpexZLwQRPn1+Y2PDPc6Qv2kDWwDx/HxPub1VnJZGqqT8YCvWyo7H88c6UDn46Ah1k3iAkT6Fjsqg08AAcqBqQWwRGHr18j6nfrOsgyCpAUaPbek1PDul1+tlj/fc39/HcDjE7u5uBi76rJVqtZo9IZCurW534Ge6Oap4Fvwpy7h3+r8NVio7YrsoGNm1L96KVwU6L8+i6XYaMTtDdK9gICGE3JPsuXRdp3E9Wk7qahvWAgjgLwFPWQBdwMRO1PUNSk/tINB3petUIioMHyY1GAxyABJC/twRnozG2QE7pa0DkuW0TIhpbneH2NqoZfdtd0e4qFXLDVotf5F4s0HKMhQkFED0VHwFkG63m4GIBRA+M5h14TGQpVIpY2ucpaMrrG3m7Z/yxoKOH1Vejb/Y/ygan2NbeAbMuj82AK5jT8ewAoitg2UzdjyuQ1buwrTbbRw/fjybvm2329kDtO0KVKWouhfGc2F00KjrovTOujR20REtHdOwA0IDVvyNFtkGCXd2djAcDrMHS9HqqgvD5wDrTBQtLl0ZPRJRH/asyqLu1d5gguf/ygfxlVefxA8+7SH42T+9Ae/62F34ne/4ElzUqh3okyIfne2nikB2QYVXkNBnyCpwTCaTjInxCX29Xg/b29sYDAY4depU1lY8mGkymWRtUyqVsvHBs2O5+LBaraLVamVt5q1iZt96bMO6KTHOn0jHMhAoGK/SjXHAPFDO/ym8zwKNdZP4rm6ZBS47ja8gwv47DAs9V7JyF4YzLco4UqzCNohnDey19h5lGvrZKoxnnVQs8Ni4h/X/qUze4TMcBHowkvrrtG5qFRXQFBRt+xxvVvGVV5/EG997M9743psBAC94wuU43jzY1cvEQDxLqq/UbIt90Y0jwJKhKIuiYmnd7SFSZG8KUgQNZW8WRFQpyQC8cWWZgddeh3UZ7f2L2l2ZMMeqMmyNxWk51sFEVgog1WoV97vf/bIHKdGvBZBZHrseBDg4lWkVyjIFFXUBeJ0Gm1Tp9V69zyoQry1iIHqSmT6iUe/b29vL5UXfvlKpZDEhWlm6e/o0PV7LAccA9Cuf+bAMPADgh5/18FybLKK+Gixlv+h3sg0qMN/5rGF11/je6XRyj/hkuuVyOTuHlmfTKiCx39j2zIttXCqV0Ov1AOQftsQXmZx9KJn2nTVEFlC0LNaNUAX2gpiLgEbzs/unLANhXEhnCq1R+pwOoiptt+zDdsoiZPX8U/6ug46/2fts+jZGouLFOqx1Tr28AWVjGvydA5VAQAoNIDcbxUGv6evA+5k/+Xguz596+8fwymc89ECdvPbR/2wQ0KPNqTZQZuINcKsotLLVavVAEFbbTcunoMb2Uyaj59qqu2ddFts3mofHwvQeLx6xiM3afK14jNwyaWVci/I6n7LyGMjm5mb2GEcGUhldp8XwtvKr72j9RyAfdbfBQs1fYx8cTLRWynQ6o4Bjjfliru3uCBuVcs56eYrE8nHmyOZByq3+tVUqDdDRUqsCMA5Cv1wH8f4QeOc/3YkXfukD8UPPeCh++k8+jndefwe+55oHYWujdmDgWvCkstuZFYrnc7Nctl4AUKvN8qxtoIYxRqMhhsMR9ocTVCaDXBkYaOXjSMk4eA3zUuPhuSXWnWV9rEuoedu6Wyal08y8Rqer9Tfrrip7tm3NPqSrpvEa695bgNY0LMivStbCQPQAZQ182VWZGvjyaKZlEjooLBPR65iuHXC0aju9Mb7r9z+JJ115DN/zpEvxC+++Fe/+9A7+v+c8EO2aH223AMLy6wOFbPxCWYRVSIIcrapSdrU81ipubVTxuy95Ao43p67NK5/x0Aw8bF2tpfSYg67q5DWexdOyE0D5+/4w4uf/qYLHnASee8UEv/Mp4B9PBXzvw3vYmI3AGCM6nU6muPo4DVVwimWewDzga5mVKhm/e26vKqfGbeiaxjh/6JgFDxsjWqTQFrjY12QWyjC07TVNrasFqlXJygFEF4qpK+NtnrNTc17sw4o2rr4zfyC/FFzTpsW9qFXGk686jrd+6C781j/cDQB43mMvxsnNRi5dq1wEAZYbQOaDTyaTA5F9j8HY8qqVoZXiAPMGaIwxC5gS0C5q1Q+0j7VsAHLWU2MdCm5WSbQ9dRGcMsg2gC+6fwdvv6GHd908vfbpV9VwxaXtnJITQLjYTAGEZVTl1PKoS+O1iwc4RaKKqYzCCxrbDXae68Y2t64zrwXy56poH9sy2Xp7LtWqZC0Awuk5XTimC6VUCe10XBF4UDwQUUupoETFZofzupc95Sq89UN3ZWl+/1ddmXU22Qvv13x1ELAe1iJpcBDIK4XuK7GDWGckFECsG6Lt7fWBN8g0Dw9AbFDZ5sN6851tQRf1P3zFSbz9hn/O7vlPT31QLu8YY27KW/fJaJtZBVZl1R2z1n1IKaIn1p3RNS+6t4nt5O3QtSBrmbJlIMB8Dw/BV9uT5fLaX/9btazl0ZbLVNZGxBeBhubBd6tUXjCVCk7g4grDV//Fjbl0f/5/34zv+4rLcwrCdDhogIM0U90vjYHY8lhazLTsQLMDaFGbeFR9UZtZ/11dLL3GAqF1M7muJoSA177nzlye//29d+GlT74s+06ltIyQJ+B7cQaN0xDkCOyqyNaAFLnAzJPsmK6YZTAKMur6pMa3dfU0nWV0wX7WOA7H1Oe8CwMgNwDswLWKoe6MJzowvHQU3S2D4W/6Tqu53R3iL284gxc84XK84ulX41X/65/xzn+6E9/zVQ9GuzYN+HKK0jIG6xdbRsW6hTB/ah3LqgFErs6k722trrWo+lqGndnfrBLYlaSWgdD6AnPXRXcH60O0dvsT/M1n9/DNX3gpXvaUq/Bf3vkp/MUnTuO7r2nkVsxymlqniC2r0L1FKZaiDESfGKjgbYOx2g5cTg8g50ZpLIQAxngN+40xE/aDzUP73/aBgrSWEcifusY0VE/WAR7Amp6Na18abaYbYZHca3BLxz3l0MZdhsGUy2Wc2Czjt1/8xdjamM6kvOLpV+M7v/wqHGvMO9HGZJi/Ul87EGglNchqp+FYb7uYbJmyqywCEQuwtvzWwjJNz1Wy9bEu6cl2DW9+4efjeHPani//6s/DS75shOPNuXXXOivwaqzDtru2DdtW3Uj2J9vRGhHtC21/GojhcJgxKI7VWq2WrSROBdIXib3Oa1O2A/Ox6SrbWpaNng9ZKYCMx2OcOXMme96sAggpI/eEsJHtM2IAHzxUtNFJpbXDrYKocPr1WCP/HN9mOaLfHx3Ix4IfF5J1Op2MktMqc/NXs9nMldNS/1KplMWI6CLp7IZtC2/wFg1w6554gUGNgSgD0XTpq3P5vQbC2W9UwIsq88AuABxrVFwGquWnqMukVp9TrHoMAN8JPhwHtOAKPlpWvb9araLf72dLDrgEfzQa4fTp0+j1etjb20On00GM0+lnzz225S/qGx2LGutSEFGxbcW2X7Ws5dm4g8Eg8y850NjI9HnZiGwUtUpMyxMFFm34VKfpwLXrA7w4ANPy0iTz0CXbun7DK7sqpTIUDmrdPq6DX9vUm3WweXkAYoFFZzEUUGx7Wvqvq2B1Ot5ad8/SFhkCvU/L6AUrbVuqwmn7sOw602f3njDw2x0HNCcRtdoIpVIZe4MxmjOwIIBpwNO2vS2//c1j0B4zBObBVNuvet3nPAOhaMRfT6Oym5hUmawPWCSen0ghOBHMVGk0D7umAMhbOA7m4XCIbreL3d1d7O7u4s4778RgMMD+/n5GeTkbwYOTuNqSdVaQYX4c1ErJaWkodmGTdSV0gCrr0lhBjDGbMu31epl173a7Of9fN/UxH9bNntFhp9+Zv/rzVrScrA/HB8vGGAjjDYyTeAbAuioKgqklATRQ1WoV3XHA629q4+pWG1998S7+unspPj5s4Fkn/hEbvfkWBABZW9k2t+Bt8/NebAPVlWXcWK37KmXlAMIGVhDRwU06OhwOc0u6U43jNaylk/Z6VX5dZ6DTyDZ6b62uptPv99HpdLC3t4czZ85kW9XH43EGGJz+jTFm29B1BkhBggqoNJxio+9K7TUSTzdQ65yaNaA7qQBChdVYjp1m10OQ1KLrS9uvqB9ZTtbLHpNoTzJjmVU5LVPSvmK7qDtrGSrLW61WcTyU8KgTEX952wb+9vQGAOBLT/RwcaWFXq2U24G8u7uLwWBwgMF69UvFYayxU1algKLu7qKxvgpZy25ce3yhRV8FFw4gDhAdJEr5tEEtLeQ702UknwrDPPWEKZbXW4PC9DgTsL+/j+3tbZw+fTpjIJ1OB5PJJFuyD8zdEfruzINAqTTaAgnracErhJCxD3X7+J/W3bomat3JOvh5f3//gOVkWhZAlHl4LoT2vycKLOqmDIfD7AgEHtDEjXgEE02X9dR2TLkPCqoEUo2dABFfd8UEf3nbnC19/VURp0+3UKmUc0czNJtNjEajzFDYzZaUZRkI29jOILKt7G/rlJUDCPfB6PNvgTl6qlKQhXCgaIfYzvHiE3qd+s1c8djpdHJ7HRg8Ux+eDML68wAy8Dhz5gzuvPNO3HXXXfjsZz+b+ccxRmxtbWFjYwPj8TjnnumCMFVEe/oakHc/dCDR/VO3hhaUCq33020hKFMJ9vb2srqwTRggVJAHkDsxTY9kXDSgi9xKuozsLwWPvb297CAiO3XOsaKbDDnOUu6njhcqowYq5/cCb/ts3tX6k9s38DWXTNBs9jOXJcbpruoYI2q1Wm4dT2pcqnHw3Cm9D0DOGGhM8EIAkbUcKMQt6pyZAOaoqwOfv9vZEssqikStLi0XAWR/fz8HILqEmAxJLYl2NK1Pt9tFt9tFp9PB/v5+lqYeW2hnndRNU8UB8j66fueA4bXKQGj1lLEwGK3tYF1HugI8IYx1IAMBkIvhME2r/NYVuCdCtqXxJb5rH7IdFGwtk9F3mwev5zVkcASF/RHw4VM1fPn9xviGBwW87cYKPnTnBNdcUs6C//pg+MFgkI1p26/aJvpdy5i6JtVGKTa3alkpgNRqNVx55ZXZFCUVRX1tpWg64Hkd4D9sSf/Ta/RwH9LfnZ0dDAYD7O3tZTtAuVGNrGhzcxPVajV74FWj0cDm5mbWeaPRCKdOncKZM2dwxx134JZbbsGdd96Jm266KbcFnSBUq9XQ7XazdQblcjnzm/WcT8ZHNBjJ+tgXBypBRGMaVAob09Fp0L29PQwGA9x1113odDrY3t7O/Pm9vT2Uy+WsHchq7IyHijfoUwps3U51gcg0dnd3ccstt+SmbDUeo7u22U5sDzs1bmcxtB25cIzHAhCgv+OqMlrVgH6/iq+/cgPPfGAJ9VDHZFLF1tZWxhY5q7i5uQlgfkKZBng1XTsDxnaybqrXrpbJsC3WJWs5VJkKBMwPRSFAcJCotU/5sNpJHqgwTsEpVT1Wj5ZXt5CTBnMwU7F1f4sOcl2FqGdY0Poog9G6qRIqk9CFVLbOXmwhFRz1pjat32+ZiAZP+b1cLufOb7XxpmXEY4jat7aOrBfL1u12s70xdl0N77GunQZsCSbqnmh91BXiNeynahhjNCqhVJpeXw9zMKDrqUdS6lMFaJC0r7y+syCi7zpG1u2qpGTlLszW1lbuMCF2pt2sRHai6wsA/zAbnW6kVdYTsehScGp1d3c3R4/VBWDHU4F2d3ezwcFzOXnGxfb2NrrdbhbbGY1GuOyyy3IU9tixY9kjK7TO03UGJTTBQTzB3mCCev3gCd4qXtCN11ogBfIBRmv9vClTZSgap/Hyp5Ja99JTDorO0CgI8DoqP1nQHXfckQt2cyzwUGXGZHifthmZSblczvpOp1o1oKz1UzeJefZ6vQw0CDKMkR07dgwxRpw8eTIbG+VyGd1uN2sTNR40QArkFui8ftY2VyPFsqxD1jILYyP2Stusv69WP+U/8rNO+5EVkGVwZkSBQw+u0f0P7CACCS0NBxFnVTgzEML8gVntdjtTxhhj5q5pIDKEgN6kjDfcdl88ZKOLp9+3g7ffvoGP79Xx/Y8Z43h5OepPSVmyVDvZeywr0dWdeo2Nc7CfUmlZtsUxoAFE/uaxMsZi2IcAsoDtZDLf16IuiB1vNEBcmKd5eO5vCOEAm9SVrgQiXku3s9/vZ8Fy7unRdU421qVMyevbVJxDdUPv0fZYpax8O7/OaCitJN1XP1stlZ0rVzqqCqWdrqehc8pVV1eq761ui3acTqPqhize2xkFNJsbs7JX0I9llEa9bNMVZ3F4+loW7wgBV7d6eO92G+/baQMAnnzJAK2Kb0lUASmlUgn7Q6AhsY69wQQnyeWc+wAAIABJREFUagfXYihLYPvpkX+6+3Q8HmftYFeY6iAl8DJN3XTGmJZaSvY/Lbl1Ua1L1ev1siA1Z7W4WpnuFYDs1DK6owQydTNYF7uBURkAy6rGhYy00+lkrol9BEmj0cBwOMSxY8emY6LTybU9WZx1TWw8w8ZtrAumnwEcAGhlUquStZwHouBhKbaiO5CnvBptZ3rasGr1dMemHvarwUamr4G2RqOR85G1s3SPwpRFlPDaT27ioZt1fNVFZby7cz/8c7OB5zQ/BvT3D1BushGu+Xjaffbx3u3NrH2ec/kIpVI91ya2/XTg7Q8jfvTd+/iiy8Z40eNqeP0HtvHem3v4xa+7Cq2WPz2oba/LznUamYFYDVSq0mjAUpkNlU53EXNQU3EZLwDmq21ZV/aNxpYUQNgnA1QQSv3prBCA3f4E9TDKHgtBVsF2Z9uzbt4sn4oeZ0jmw/ERY8zcWbpPjUYD4/EYx48fR6VSwf7+fq4PNY6k+Xr64Y1rCzZWbJ1WKWtZym7FsogiH5ANryyEro+1ul4+pVLpwBQnlYkb3TRdW0beH0JAYxLxmJPAdbc28TenphvkHt/ewSWbLQwGlWwQq/WbK2DAO+7czJXvD2+u4RsfvHhun/8fb1bwhZfW8Eef6OCPPtEBAPyrR12ErY05y1PGwHYiSHNdij6vWA8oVgXUGQ+mpa5KjDFzFRmgJuiGEDLw1Da1bEAD3nRFrXs2KtXxiQc8Gxf3b8VjJp/AhytX447xffA1+FBu4RmZFVnExsZGxiBt7EWZLOtut1qwPbjhU9uXfdtsNhFjzFwZ1kM3h6aCpymQTwGJFS9YvQpZC4B46Kubxviun9WaKdoqDbYMRl0U66aoFeZ6lGq1ina7faCTbBxAlehbtya47ta97NpvfngDk8llmQKxjCwD69EZBXxsr44nXzLAtQ8c4w9uruEjp8t4Tiyh5lgT68Kxbi963Bb++Ibbsute+uTLsrrooFNfn2lw01ir1cr547pJrNVqZRacQML62zNDOC3e6/Wylbh0D1qtVta+o9EoW1ei7JGL2PTpdTZuU570sbV/I27eeiRuxvSk+YfEmxDG3eyhVQQxgrWufuZ6H7t9QPtc3V99fi+Dp/V6PesT1gMAtra20Gg00Ov1ciuL+V0DpzaozXKo2O0NKhYwvID1KmQte2E8aq3/ewE7oLiRPL9R/fYYYw6k1J2in6wPuVaxwUmWaTwe4w0f3s9d+wc3VfHcK/InvGv5mMaxSsC/f1gXrWoJlUoZX39lxLOuANr18kJro+X61Q/t5X77b39zO/7DV1zutq+2jYKntpPGRdg2yj7U5dCAoFpt3QRHBkIl0yliTp3vDSZoVUPmcm53R7nYCZnSZDJBvVbDg/f+AbdvPTKr1xeWPoMY5utBWEa2P9OlgfHGjO1fbRetB10aumrqTjN+ZGcO1RDyWjWGdEFsOVQWMYxFY+V8ydqDqLazKQQRO0hTMwkUDqJ6vX4gPsLZEw4yKoU9QYv/eW4U8wWA0/t9fOCOHXztQzfxbx+1gdd/cAd/d0sfX//QDRxr1A/MRGh8JoSAjcr8oVDlchktUQAdUDYmwrbZH0a875Yern1YG9/5hPvgl95/Gu/+1A5e/ERg07hxFlx14Nbr9dzsgbI2LiTTxX+qaBo0tcFPzoIByIKfjHEwprE3iHj5n92FL7y0jm/4POCNH+nig3dEPP/iaYB0c3Mz67cQApobG/hYcw4eAPDR+sPwhPotWd47OzuZS0RGwj7d2LoE7UYZzUYD9XoD40odxyr52JoyuFKphN3d3dxCtm63m2N4XGDYarVygVrg4Al8dkzqWCN71mUAFBpBltECOttn1bJyALFxCg0uHSYY5LERyz5oTRk8sz6mrmK0j5dI7VNQ0DqxGfAL1z4QdUwt0Yu+4Diu/bzOzJoefBAVp4bVMqlr5eWnoiBbKpWwWSvjVU85gYvbU9D77ifeD9/2xAq2NqqwoszOAoltN7I1BddUXMmCo85s6BQolVinN0ejERol4HH3q87iONM0n3hyiDrG6AMH9vVUmm3cVr4Ej6zcjq84djf+uns/fGpwDF+yeSoDQgV/lm8wGGAYqvhA60m4dHInvmhyI94zeABu7G3h/6p+Gs1yfiUogByTYL0488PyKzBwvFm2qa6LZbe2v1PtrO3rsXamtWpZuQvjKQmtmkcv9bP66Ur39DoeRwcgt1JQpxzVyqsCM2iov6n/aQGkVCrh5GxWhsHHk+15rIaKxMGjVofls8BhZwioBDboxv9alXybbTWqubYtSovlsqCrA1LjHqy/7QN+JvCy3dnW2ubWNQ0h4IWP2cTbb+hlaf2rBwXs7x9HvV7LBS0BoF6v41vKt2GzWkKttoVnXdRHbzJANTYw6kyDmLoGiKBdrVZRLk1wn+Ht+GTjQfgkrgTGwKNqd6JR8qdXNUDKcaH9o23H6wk2dJs4g8RFZVbJrVFlX3mi41/bXPNfNQtZSwwk5Z/ba/R3BREABwaxdUt0Zoadao++00AXX/psE7UIWj5VQlUqtTAcXNblsgPUDh47ANSFsyCSWlvgsRctizcTwGtYfp2xsQNTp8Bt33ivorLFGPGGD3dyZf3Dm2t4xv2aB/oaQI4p8v9GjBgOQ7YblgCm2xLK5TIq5TIeMbgen2nMHynxxMatCKGWM0i2TgRFT7w207VIen4J+0zrn+p3FQVv+5s1hKuWtU7jpuhYispZ/5tp6KyLnumhVFpnIawro52g/ugiS65gYVczKoBY90cBLBUwtcDhzaLYVYle+9lgLtP20ky5NKrIumKXyqIzCapwdFlYVwUj5rU3jPi7W/t49kNaeP7DqnjDh/fx97cP8fTLm2jXa9nScPahbTM1DAQQxnK0/RuNBsqVCv659ehcW7xncH9c0zwF7nfxxiJncrgGyLaV7QNurSAL4ZoW/qd9pUbgMFOxnru/avYBHAJAQghlAH8H4OYY47NDCFcBeAuAkwDeD+BbYoyDJdIBcDBI6il1EYDoO3/XNNVv5X90LVLshgDEvPQaFXaeVXJgzmoIXgBymwXVinjKwDrp3glVIFVUD1QsCOi1TFvjFExTFV+puTI6ewIYf1ffXKfFQwgZAChz0FWcW80S/t+n3RebtSnYvPCxJXxdd4hGKb9JUmc82OZ8Z13pvjabzYx9AMiC6uNyHbeVL8EjyrfhSRu3432jy/GpQRuj8h6apcmB+rBNFUA4Pa8Mi2ON7U4A0VXQXFhnx7eujVH3Vse8pwuqA9rvq5bDMJDvBXA9gGOz7/8ZwM/FGN8SQngtgBcB+MWiBDwrm0LNFA33rrNgREtlqXqK4qmiWeZjO0U72dL2lD9qGYkqmxdfsW3kzeakmJsHvl67p1wYO8B5r+ZNYGQbewu+FEzY9inG1a7nzz451qjkdmVb46CugtZPYxb1+nxFb6lUyg4/urb5MbSq0+9fWTmNJ4VdtGslAHlAVmCyIOEtGdf2UhdG99XYdHmt1+8sN9tcx4L2g7blBctAQggPAPAsAD8F4D+GaUm/CsA3zS75NQA/hgUAQlEFOIwPr5Y6RflTIGH9f/uZeRRFwYG85ef6BL1XXSpaQV6vikbrY4NyOoiVKdhdo/q/DnILSraN9V5N09JpbRNgvmjMll9FFYUAymXrVGA7q2Lz8oLJqnzWHWVdQpgfokT3cjweZ4vkCCg8b0VnlyzD4pYHjas1Gg2MRqNs+tm2qxoEZR+6MlbHrec+q1ti24f304XSa/j7hbwX5tUAXg6gPft+EsCZGCNrcxOA+3s3hhBeDODFAHD55ZcfsLQpdyJlZS3bcPJzP+v1ls7b+1NMyZbJAp4yGMtUVJkV7FJ5LWJgmqcty7JiXbzUNUD+BHgFSBX9X10j2zaqHB6IH6aPgfyDqQhSrBtBggCiu6PVXVWWx+8KaLrC2ZbFMhUPHFN10XQWxcO0vaw7rIxslbIQQEIIzwZwR4zx/SGEaw6bQYzxdQBeBwCPf/zj4yzNnDLaRlOGoO9FFtWmk1I+Dmrg4EyJdqLN26Zll0GzAznQyE6oVPquAKIU3bIBXgMc3PLOKedKpZJtFFMXwZtqVdEByAFpByXLNJlMsqMQ6M8v0w+l0nxWS3f76toKliVVPo+9sT0UqMvlcnZUJmMg2qcEEO98W+ZDtsD7yEK4toRjR6f4+WJ/ADiw85rl1bFnjY8FWa0r28ebECiXy9mO4Qt1FuZJAJ4TQngmgAamMZDXANgKIVRmLOQBAG6+p4VJIa9nkYoQPaU0FqSKgGyRBbSgo+nzO31UnZlRH5oMRK0KB5oOqJSrlRrMwMGp1pSk4i5858yLrjTVVZmeC6rBUk5/WgutgF4EcsqQbF6sH4FbA9jK5BRAvMA1lVJnWCwj8JTcvtQ1sv2iZbZ9ZdPWOInXNxr/0H4qasvzJQsBJMb4CgCvAIAZA3lZjPGbQwi/BeAbMJ2JeQGAty2ToecKJPLN3i0DORuxwOABhXU1FqWlgVpVHNJpHZzKQuyCNnY+rYlOD5dKpdzhPhRlILrTd1HdVRS4GPTTg29ijLkNc7rFnfEAZU5UGC68spv6tKxqMT1gp0LpIUFe7IFWXV0tTYd5c+OggpcCCJfYsx90TxD7gW3OOuqKV7KbyWSS23xoD6L2QMMCiMaWeK8aBAvw6iquWu7JOpAfAPCWEMJPAvgAgF9e9sYi666ScmHuqSiILPK3i9LQdw1o6aC2QVS7dkODsiyHxhEITNYCc6BZa6fumS2fdRXUmtnYhQKL3d7O81W445ZKznLUarWcC8J20HiCN+3oWWg7de1ZZGVxdjWzVXztK7aTsjmmodsOtLxM31vXQuAhSGrA2DNUKearC9P4G1fjWmZiweSCYyAqMcbrAFw3+/xJAF982AxTDES/ew2Uun9Z2pYCCpuvV95UPtZyenmpItlAnPrxCizerIMtt8YVdICn6uC5fwoaChIWQGy8hO8KKroHBZgv61ZqTdDT1aFWFFDtlKU3JtQ9ZJk1LVV8r20ULJQJsPwWQPQZPrrj1tbBsgrtCw8obf9ZkNPyAnOmZfv0sIbwnspaVqLaAe0hMeCDh3e//T/FWIoaeZHr4gGfR0s1L32pUtiFR6rEOp1nGYGKHcw6yJm2tuky7os+LtIyCFsXLpaye05YHyoFAchaabo0tj2VTTDNFIBYkAMObjjUqVhv3LBPedqc9o+mzd/q9TpijDn3zLaTurMpAFHGyP8V8Pg9hHkwl31ujZPWZdWytmfjLmIgeu2i9Oz9lGXvXXRPEXgUlVmv0UFkfX9VcKu8HvvgPXZZuPXvbdq2jh642UVhqhRUSPr6HMxc3m6Zlc1bf6OC2f9sGRUQFEhZbgXcVH42yK3pea6FBnute8iYBsFbDUDRiWq2/rZ9vEA8f1OW5DFNr61WJWtlIEWDvQhgvPRSklK+1D1F7OZswUxpvTegVHn14GaPeaho0NLOLqTiCxQFB7vxS10BHcRMm0vVGeRk8Jfp6fN01Npa94AzNJYtsc60vDHGHDMCDh6VoC4g66f9YQGG/1vGotO1KjwqgOlxmpZBbn0CwPb2dnY6G1maBVfLQLSsypoIkAys8p1xJ42Vef18vuWCOBPVEwWZIreiSFJ01X4uuqco7WU6y5bfA0kOJB24ywBIypotA3apOulRBFQwHexaLp2RUCC0AUZbNg9UKBrsVVdC40eeO8f8LaOwmyMp3lSvshkAOdfBrvvR+rE9ODPFIxFpDJTFpfqryKCxXNqe7C++L9KV8yUXBIB4VNLzd5dV2sPkl0rPA59UOvp9mQ60+StD8RTB5m+ta0pRU/dbJqAWUH1/KqCCCC30ZDLJDg9utVq5k7d0DYiyI+atSqgW2GNJzM8+LtKbLeLsj3UDbOyAQKnTt0D+qAB1GUII2bmvLCdjIExzMBjg9OnT2N/fx6lTp7Czs5OxELtGw4KIupE622Kvta6issd1ycoBxBvUVpk98LD/LUrbE6vsKRCx+XvpF5XnsCCiaVpFS5VrGeuVKo9VMG8gK4ipwuu1jIUAyD1Mi/WwrpUtg07rarlUlJFo2fR3Ag1BTIOQyiAUQOyMR4wxdw3f7al21m2kjMfj7CHr+shUDfB6LERF65VqL+taqSu6DrkgGAjFA5fU4PfksIqkv+u9i9hJUV5F5bWdnRpM3hSnBsq8wZUCEh28ei1ZAqmxZSJkILqk3lJ3q2RUGFXM1FSnMpDUFKvWnczMTu/qu26NV9dE62HXx6jbyPZgXpo/Z18sOGp5ut0u9vf3s3Uy3kO27RhQFmWZlbYV29OODY0LrQNELggASbku57pBUrTe8x1T/uSywOFdq4MFOPh4Cn2nVbTl0T0kReWxedpyLePGWEXSe7gOAkB2+pfuOqWyKeDYMnhrNGzdtB88ZSyqqwIr29oqr8Y89DAqOxtFAGGZWX8GUAFkh0kz/qFraBYBCN0RlsPOvnnrVbz0Vi1rmcYFlosppO5NUXvvv1T6Fkhs2l5ZFilrUZk1P2sFU+kum98yboyd4qNykzF4U4Q6OL1BSsWjNbYrWrVsKffEs6i2/ta181w5La+tp5bVAqICDO/XlaBUZH1odgghx6wItt5mNy2buomav20zj2Vq/6Sm6+/1DMT+5lkTT5ZxOXhdUX56zbLKuwi8rGukVpQDIQVWy9RpmbJ6FpsAwkAnd3Rq3sD8HAprSdUFoQLyGlUkncLUMlg2w7ZRtmFnfVTJtd6WqaQYipaBszp24Rbb3Lo4ygC17jpzw9kXOzWs7onGaJgPQZwMxO59UtCwcSsFv3XIWp9Mt6jSy1rmZRXNppGyVsvm6f2+LIjpID0sA0kBlR343vXLuDQ6yPU+G39Qi23B0AKCByKahjUQNvDqWdpU+3ngq8qoroiCgHUfdRpZy6V5KmCm9hRpeW09LcPTvIoWhlkwWXT9+ZK1AUgqMk3x4gP2c+q3ZYEple7ZovkiALKxhtSA8n7z0tMBX9QuusKUn22sQtkAlUIXuOmJZFRQ0njdXUtl5H26QU1nOXT60ZbDxiiA/OpdC/6eQbILy+zaEhVdPMdykKHwjA+bD58DzAdZ8eFT+oBxnYGxgKVp2pkvFf3ftgdnhtjW65jOXRuA2E5fZG0PwzAOy0bO5v6zzUP9eS8dtZ7LANlhyroo4MayKbCpZSTN1j07Fgzt/Xp0gQYytTxW2b020DRT7oq2hZ35UTahZQVwoC7Mj/d4jIFlmEwmGWBw1sXbAOmN48O43RQtc2r9zCpl5QDiNey6G2HVssygWQZEUuBqAboIMFJ0morAMzK4wEmZDBeLaSCWeVGBuXeEAGKPIFCL7DFCbwGa10apOI/WTcGHDEOX3Ot11rCxbsoqyMz4MHEyEss+VHSa2/ahHkjE7QJ6gpo+LTA1g7ZqWRuAeOh7bwGRZdjF2TIQj9qnvhcJ+0jPMLEsQ62tjXeoj67BR89yFhmQInfvMOKNNe9l/+dnDeDqGNZdybr2wwK3ApRtZ80jBbK6U1jf122AVwogHoVedwN8rkkR8HgKk6LClolwsKrlU/9bYx8WbDS+YV0cyyA862zLrqL3LeMSaHreOhRvulvroXuFdBOiF/vw+sEGjQkY1Wo1OzFeT7DncnmyP2/q/V4DIED+IBRvheKR3HNJgYgFjiIrbAOuNg2drbF+vyqyXp+y9ioWBBZdn3I9lm0nBZAUK2b57epUxj24cIy7mYsARIOgTJfuSrPZzLku3NXMMtkHnus0tILxKmVt60Csb8vPQJpq39tYivXzF9XfKpP+rouj7DRlam3BPbFsllHYeIUGNG0w1nu3n71YRVEbpQBV6+7Fi5ieN32rIKubCVP5eXmTcdTr9QxA7EFRzJMMxFukd69hICqpwXEkPnikAoTevfYaVS6dDgTmT3TTbfieZbPKb3+zCuj5+1bpYoy5B2EXxSFS7aP1KzI++p/O+ujBx3alqpZDy2NdGO8pdJ4wH3Vb2u02Wq0WLrroosxl8dig9oueW3KvCqJ61vHeKIvYlr32MP693mPzUymaoVkm/ZQUpWlBhC6Qrjg97JjwZmAOUyYFCp1qtgBi80oBZ5EooymXyzl3hTEQWx8vKKvlXScLWXkQlYNFxSI7r9X/z2Zg/UsWj5p7wWfvHu83XaxEK8lHOFrruYhdqBKkBq8ti11haV0pTqlSvAOVtA2sIqfqb+9RANNrbJ7ePd5KU7aDxibsgUoqbLNGo4Fms4l2u40TJ06g3W7j4osvRrlczs6Y1UOJ9DR8XTCmcRQyKZ5vsgpZ23kgdvGQd431ce8tcthAYMpv1/Ss26ErG1NLr5fNfxmxCkkGojEQ/q4zN9ZN8d5Tv3lt4wHOMjE4vSe1TB3IH1mYKoNVej5Dh8+u0QdcAfmVxF6+Grc6W0Z5trIWBqLBIM+3tZ/XGSS6ECRVd20fq0zLtJcdfDqtaTeb6aAl4HAJuF0spv1FVqHWWsvOMUEg8Va4egbFYyBFbWUBQO/19vdYcGWAlEvYdc0H209PNLML5LQOtp3tPcyb08P64sO9venilEE+n7KWhWQcJMDifRypQOC9XYoUpSiY6DE6GwPQAa6bzTQNDvAQ5kftpRaTATjQ3x6AqDJ7sYgiBrKMWJAtcn/U2Km7x6laPXVd2QenWlNTq15g1q7ItftbOFWsoGUX8Glbr1LWvpTdEwseR6AxlXPRDjpw7UY3XWfAE9N18xmVmxZQGQgA9FHBRWG+tb8zAk5sVl2wUqU5TL3OlqJ7bpyChF6jRxHwxcOSedK6BRFt28OUJ8aYW4wWQsgxD7tIjQDiAck6ZG0rUdXX9fzWI/AolsPOgqiPrEobYzyw/wJABgrWBQHmANLr9bL79gYTvOb6gMfdd4Rv/fwKfunvz+B9N/fwi193FbY2DoKI9/JiIF6dzgZELNvQuIIFFborDFyORiN0Op3snftflBFQLDCm4ipaBn1kKDA92YzPIOYOX+6x4UI1vusxAuuQC+JAIZWiwXFvBJN7Wmfrnug+CoquA7GnoOuKUxsHIa2v1+uoVIBHXVTHOz4zwjs+cwoAcO3D2jjerCSZBmfjUrM5XvCxCFhSM0Ep18UyELYJmQUtvj4wSh8cpXEIO3voKbSWT1ncYDBApVJBt9sFgNyxAPZsVS2XZSDrAJGVAog3QKwvqv/dG6dvVYpiQ5Rl3UGNZzDKz0HXbDZzAT/uwC2VSrnzMABkg5eWWnfdPu0+Adfd2s6ufdHjtjK3qFar5aY1i+rG8aDjJXUfYy/2PxsLUpDQIKR9glyMMWMdZAGDwQC7u7sYjUbodrsZE+n3+7kzUG1Q1pZT68P09/f3EUJAv9/P/tvf3z/wlDv7GFG7ZN7ToVXIyqMuy4DBPQ2WfS6Kp0xFUjSoNHhng3ipl02bg5dKOBgM8Qc3VXPX/Y/3nz6w7FqDs3bWxmMhy9S3KKBsXRd1WfiurgBfVnGVDeiydW8aPNXe2gZaFg2UatzDMg2bn63HOkDkgnVh9N0Lwt1bJTV1Cxxc9MRrdH2AjQNounaKcZkAZ4wRw+EQ3XHAR7creOLJHr7xwWW8/bYm3nNjB3fv9XBys5GdawGkZ94o1uUpYqHeTIpltqp4PFlNg6B0xaiIdCEYe2DMg9O4lp3Ybfw6RUuXkHVgO+hjK8ks9vb2ACDLQ5meBS47C7MuBrK2hWRFlfWCqvx8JMVilUcpfup6oHjfx6J2n0wmqIYhXnDp7djaqGE83sQ3fF7A113dQquCnIug6Sqz8VaGerGOZQKrFjyYvlp9teo66+Kt0NWApbo+3nSqltOuobFsj3VRt4qAXHS+atFr1bKWhWQaaffEo7BH4HF4sUCQotflchmj0Sh7zolaT567yVkZCoOrTCPGiGocYDicWtDpykpkMQPdsKZrFjRGY8t8Nn3uKZQquhcgVVai7IQsJAUa1u1hfnaRGMe9xoL0hDaCCWNTutGQbawzRssshViVrIWBaMTarka11PwIOM5OitpNGYCupNQYhb6q1Wo2oAFkNN32Fd2BSqWCXq+XBQdpVQlW3q5b+342AGLBQ2MEGq/RqVMFBgsgNuahyuu5EboITuM86troXhm2h5Zfy2NZTVGcZVGfny9ZOQPhlCC/639FEfd7Ewu5JzEfq4SeotsBaIObng+vu0SB+cOPPIPANQ0hhOzh0s1mMwcgQP5AKesyHabeRdbYgkjKLfDAJhUgVYX2GIHdXEf2YDfc2bUiXrlTQdILZYZyLStRdfBZRC2arru3ybJ19gBBzyAtSlutJNkGgIxx1Ov1LI5i98nY2IFdDzEej9FoNLIDczTACMy3NVhFKqo3lWZR4NQqIFfN2liG547oOg+75ySVr8ZwuCBPwYggXK/Xsz0z2j+eW2KBzf6vWwyK3NTzKWuZhfEGSypQeiECx4Uc1LXKZZmdfrfA7fWLN8W6yGpqmlxdORwOc4+/tJbUlj81PrRe9vdU/b020gAnmYG6ILaOy/Q18yIQW4D1Ns959UqxpRSI2fxXKUsDSAhhC8AvAXgUgAjg3wH4GIDfBHAlgE8DeF6M8XRROqn1BZae6fs6pKhzPAA5H2X2KKq1wDZP/s5pXA+QdeBqDET7xAsIenRf20KnHieTCer1OgCgXq+jVqtlAVp7XoYHdCmg8lirbQfbHta9AJCdNaqbBQkiOiti2y1l4LRtGo1GVm8GY3u9HkqlEprNZu58WGVtfCymzgJZACla+7EOfTnMQrLXAPiTGOPDADwWwPUAfhDAO2OMDwHwztn3s5J1oGdKPFpsv6cGsleP1H3L1Nm7PnWfVbhl87GK57k9iwanKpAu0dY1FXpAzjJTk0VtsajuVjw3SQHEsgPvMKCievMz09YANM/7oOuiAWubRyoW47lRtv7rkKUYSAjhOIAvB/BCAIgxDgAMQgjXArhmdtmvAbgOwA8UpWUtpz2t27vOlGWZIt8j8YKOy3Yqko3xAAAgAElEQVTcheDWsC2LfPY4iwd41sxbN2L/T6Wnm70mkwn29vYwGAzQbDYxHA4zBaKlVtZTNBY8xkXW4n3XDXkaa7DTx3wnE9O4jLchTmdwGCy2AMl8NTBdq9VyAK1tHOP8LBWWW2eNNG7IMq9j674ny7owVwG4E8CvhBAeC+D9AL4XwH1jjLfOrrkNwH3tjSGEFwN4MQDc//73X8qCLKLpq5SU5bdl1d+K0rqn9TgMU0vFD2yAcdl8UwxB09JAJIDsAJxut4sQAlqtFvr9fjY1rKCxqL7LMCnbRx7rsKDC+ARnCL3Nf1pHrScZl74UFAki1tUJIeTcTFtWzUsBZNnVvKuSZQGkAuBxAF4aY3xPCOE1MO5KjDGGEA70cIzxdQBeBwCPfvSjo65KBBbP+SuYnO8GSzGglNIU+cReOquURW1lQcQG7HRLu7okuu3cLm5S+s3/uBScm9H40CQA2NzczD04ydsXs6heVulULJgoI7Gvon6yAGn3yehCMl6nDESnw7lRkeNH2QqPTrDL+JmmthGngbVu6xhrywLITQBuijG+Z/b9tzEFkNtDCJfGGG8NIVwK4I5FCdkovCepjl+1FPnmFwL6p0QpPJCO6dhYhFpXtah6nB6pO+/30uH9pOrlchm7u7sYDAa5lZjdbhfj8fjAg5JYB0qRO5vqB49ZWSZyGGHdLNtQd4ZAQrCgolcqFTQajVxw1mNCutbGAogyF/vMGOrIsmzyXMpSABJjvC2EcGMI4aExxo8BeAqAj85eLwDwqtn72xakk6uoNzDWZbGteEp3GDq9LkkBrucW6n+Wms932c43lXGbOwHEG+QqykwIFvv7+9msTLfbRYwxd5yAVzab7jKuone/13+2f1PTp3Zhmbowmi4Zgq5E1SnblDHitTwiIcVubdBXr1mHUTvMOpCXAnhTCKEG4JMAvhXTWZy3hhBeBOAzAJ63KBEvcOdV3HNdVt1A2tHck0BR+nmhibaXtqO1UHYwq9vS7XbR7/ext7eHfr+P3d3d3KlYIcx3lurUr2UOug6EbcbAYLvdxsbGBprNJgDkFEdZEuti4xmpuni/eYFfu9DL2zpvjy1QV86eSKZno3AfET/ryW72ECIb1+AaErtWpVSaH9ysj71k8NeO0VXI0gASY/wggC90/nrKYTL0LILnu94Tunm2ospWxDDOZppz3aL1sbMuChx6Hme/389evV4vt81cTxPT57daa8q8CSCa7mAwyG3ks8HClIGxAOmNH++Vmj5OMQvv/I3U7luWx551YseKshordqyri0MQ4fdUgHjVstYzUTl4tUGU1i0KsJ6P8ql7pQOE/ryW9TBLsFcpnhX21hUwANjpdDAYDLCzs4O9vT3s7e3h1KlT6PV6uOOOO3IMhOnQr69Wq5kLontA2FYaR5lMJrmzVk+dOoV2u42trS3EGDNQ8uIAntLYOtoNbqlt97pWZTwe5x6VMJlM0Ov1MgZGt00P+iH46WlmwPwcWbYLWQIw32Gri+2A/NknBBsuuuPuaNZZYx9MU12ddUztru1U9pTlSMn5Vk6PFVnrogG+swENa13ORxwlVQ+tj/XtlXGQaXS73QPv3FmrIAIgW+PAgCktsLIHVVr7vJPhcJhb+m3d22X9fK2zF8dIvdtt+amXdXm8+AcA9zkv1njaMitzU+Ok7qE3vWzTWbWs/EzU1Ly/F/PgPatmIBY4bP7WZz1s+SwFP9diY0y0zvTz6bt3u10Mh0Ps7Oyg1+vhzJkzGQu5++670e/3sbOzg8FgkJ39qXnQ/bAnuQPzx0HoYyFUYRVM9KUgnVqPYb8rGFjg8FwUXbuhU7FkJPZhThb47OlgyiaUKZAtsP2VQaXArmh62Pan5rku9rtyBlJU2XWhqOZvfVPLNLzpN73//7T3rTGyZeV1a3fX+9GPO3PDMDMED/aMJ8ixMSYG28Ti4Vg8THAi5GcCQib+YSQc4oj4JTlRgpwoyI8fFhEG20SKjO0JChZ5WGFMrESxJ5mBJGAzCAuSecD1zL23u7q6qrq7unvnR/U6verr75w63X3rnL5965NK9TqPffY5e+31re/be3vHtJrOPM26iF4D4kPKyX5GoxGGwyG2t7fR7/cxGAwS8ZQgQyqv5ddwLsOXwDTAegBgy2LfddZ4TwvJq3mkMRHPnbNlSJuJzH62DER1EDIK1XZs2FUB1tNfPKaudaD1dEe4MFq5esFl6gfeg+cJVJ4PnvZQl2Fer8bvKpTSh6db0u/3MRwOsbW1hX6/n+ggmvthtak8D6vScp1XxIYpvYiHHfKv12cBKeul21mGwh7fAoMyADukXxu2RohUVFatwrq99vlRoTnGOOXW8Vz2Huu1lf38Fe7C2AfooqTkAscNDTg5xZ5HFWeVu6yb6vn6GoJkTgdZRq/XQ7/fx+bmJnq9HgaDAXq93pT4SbP+udeYgWPmoA0qhDC1fqz2xtqIuQ+N1+A1Fo9xeSzEMhKbik7XhQDirQZnAUXLwmebIrKOOGZ96IvsRbUVHtuKtBqJsXXCc98RLox9+Dxl3aOnZQOM58akmQcaRZbf63UJINo4CCQqlA6Hw+QzIxN8wHXNV4IAcxwIEMpKbI4Dc0YYndB99d6nsUFeSx630Vpa1MYDlDS2YY+nkQ99LmwINy1yZPUYO2zARnjsdVgdReu8SCvchdEHxwthKbKXDRxAOeHks5rnwrD3VLel3+9jNBrhxo0b2NnZwfXr17G9vZ0wELo1ZBAKHNVq9WglukkYd3l5OXkHTgrDdF3Yg7bbbbRaLbTbbTQaDdRqteQZsJEO4DhUSQ3BPhdWME3TDLSOWD+24bLXZ/iWYKv1qc8rNR9eN59p1o+66Uy+03LznNSXmK3LZTR5HcrWbHKZuqoXOpHsVthpG6L2PkU3Xk8DOc+x5m22N9LfrXBoxTovcYrlVgAh69BMS6XrPJ/2iBZAdH+l+FreNDdkVrr7aVxGT2S14VqvR/dEdH63+p59brRePOZjp0/0zpkVgCjDZS6cgaSFndJCm9ow5q0ya69imdFZrQzW4kUY2OOp38zrrdVqGI/HaDabJ4bYs6E3m01Uq1U0Go0kUardbmNpaWkqD8QyCBo7gVarhUajkTCQer2e6tbyWrI6EuoI+p4GKJ7bYgcLatKYah3qnrB+bGRF61SXc7ChYS5Upa4kmZAH3gRudfesC6NMrEgrPYybxS4sCs+LiagP7iWLZZXV84+tQj5Pm0XX7Wctm86+RSbBXA4+yHxwORlwvV5PAISuC+m6io1aj7QQQjLBss7QlaUxecxqlmaSp07Y2Dw2ZhPaeE5lGl55bXBAhWyrr9jfsthf1uA6K6wWbaVMqqw32d7wMnUGyzjSQCTPccoy7SkVdHVGLABJ5iejLMPhEM1mE51OJ8kR0chCo9FI2IpqH9RGQggnohXAyRnAqHswzdsCiMdA2NtrOFeBwGaGWlHRukJWvLRD8zUHRCcI8joLvUY9l6bRM3Rus3A1z4TXqbqJzSdR1qPXrAB46RkI4Pccs7YH5tsw01woW4asfcs2BQw2RG2A2jA5XoW0udlsotFoJFMPjkajZDu6KWQonNODwKEAoj49gBO9NZkMj+EBtpbT0nSPfXgiqoKHbp+WC2Jfmiim51VXRdkCTbdXIFFw8kb98r7pvVPmoRqTsiOrpRRtpa2Naz+rpekj87ZZIDLPc9LyuElp+6sLppPT8IFWwZM9K7NL9/b2Em2CPjqPR5BQV0cfdHVhNJtUt9GIQlbWpHULrAtjAcECiNJ4Dzy0105jH/qfMoO089iGS8C2eSVWW/FcFk+M5f/2GmyouQwhtRQAybpQ23g9v3heZv31tIfX7qON/rxs6TzXaX1x4BhItEflQ9fpdLC/v49ut5uwDoYSCSC8HruSmva0PJ9N/dYy8VhZUQrbA6e5up5L4t0jD2S0fDoGRxu4dS808SutA7TnICjv7+8n+TY6updAos8L64WhbZtXo9fKfWdFjeZtpTEQj66mWVEMxAOCs27n0Vtr3n8WgM7ao1hgU5eG/5MlcEwLH06mrmvPpnRaG7fqRtQJ9Hf+xgfe0yoUNNLqyAKEdUPs8bx9+FmBxxsop2nkWl+287OAxmvk+W3Cnk5C5Am13v3WnA89n95ffV1qBkLEJMJqirOKZh6NLxJEZpl3o/KwFe6bh1XldVuy9vfORTFStRE+/AxB7u/vo1qtTjUKmpdIRR3Fdg78HGNMel4FCp7Dm55PQcij7jZk7JVRz68gQ7eC2be9Xg97e3vY3t4+kQFqWY7t8bUM3rwiHJDY6/WmtrFApKBn9SrvswK31tmlBpC85gHIPM5xWlPgyAKRPOe1DOE0lrafVx7bkCxj4LuGcwFM9b4eWyCAsFHxP8sqtee2vbmWcVY9eOzFE09pNjLiHUcZiGohVtS0YGKBTCdNIugSLNR1IXDZ++S5wdzGujFZIFE0eAAlJ5JluTD2pnuIW4ZZn13N61Hsf7pfHvDI6wIprfUatbev6iT6EOskwOp68Bq29w7RrRwzhu1xRLuSXhcqWiod9+oEODlQLOv6PdC0x9bPqlOw0XPKArowNA2fako6BVJNQFN2wReTxejCaB1YJuG9WF4tu3V9bBi3aLuQDAQoB02zzn2Wxm4biGUEpz1env9sb8/GmgUi6merK2EZyOHhIfq7B/ipP3gOr7y/iR//a3fh1x+/iT95eohffN06ajgJaro/xUXr9+e53rPUg1cOez1kBVbU5PWzPuzESJrBqsKzHt/qKtrIvUznWR2qlpv35o4SUfP6/2pFuDN5ynAWTWKWxpFH55nntSvQZJ1fr71TW8Ir72/iE0/28Ykn+wCAtzzURre2hPF4aWowpG2k2vtWKhUcHBzP+6HnSxNUdRtPNLRAYX+3101GQADJ0nGYN8P9mRy2s7ODwWAwxfoYKtcQMEO6nFeW9aOs0YKHDiRU10hDt/r50gMILS+IaAWXASLeA5nVc6a9px13lp3nms+7rwf27C3f9W3rCXgAwDtftpKaVGUTqegeaUPIKoetKxWAvR47Sx+wvbiWiwBnr4EAYKcdoAjL2dxYNh0eQOABJhm4OrqX5aSorfVu/1e9xgMQz20ryi6MCzNLoCzaLN3VG+SJYHzXxub1Krr/WZhLnjLzofdcJtuItBfTd03Dtvt95DObU+f9zf+1hbf/1faJsljwYC/Phra/vz/Vs+cFBQsi3jaeKOm9vCiONetqqHjKBh1CSNL92+02qtVqcl27u7uoVCrJshhkI9ZFtO6nns8bM2NdZJaxSC2klMF0Z/mvaLONTD97N8+KYlZ80+Pq5/Necxojsksk2Guyo3QJFjqxsPZwvObtccRjz4zwlofa+HuvuIIPP7GBx54Z4W8/3EJDelA+5DrXxe7u7pRA2Gg0Tmgi1j1R4E5z+RTYtUf3rlt7dPtdj6mulLpcPJ7O8MYlOzkc4O6770ar1UqulWFczvPBhD3OEUIwVPDQcymAeK4WMD0n6qUGELW8QlqZwKK+rX3YvIfejpHwWEjadZ/WZbOMyGNKdhsNU/Jh5LsdC2IBBABay8AvvnYN3fpk4ah3fHMH3/9QA63liL29k2nhOnx9Z2cn0Qeq1SrG43HCQPSh9+rK1oeyEG7HMhJEZukjlvZ7rpvnytlOhIDNsUQrKyvodDpJWHw0GqFSqaBer+P69esApteI1nf7HHBbjeDo82FD8CGERMwtwkphIJaueeHZNDo777LRbE9tab3XO+pYBqZ+2wxOO75Br1V/ywpZpwGElpkRBfuQ6nXou71GCxx63kqMGI0EsA4OsCVsY3d3F8PhMFkyYm9vL1moipMJraysJONn1tbWAGAqgU3rQ0FYG3yaC8BeXc0mftmcCt4364bqvdXnQht8rVZDq9XCvffei9XVVTz44IO4cuVKMnfKcDjEzZs3sbm5iRgjbty4geeeey6Z/Y0DF+kuan1bN4vXxeeKrKfRaKDb7SKEgI2NjdRn51ZbaQzEugezrEgWYn1nmzRkaT334UPI3/ShI3Bor6nns+xjVp14bMPqGB5Ft0CoQKL7eONLbENVlkK3R9mGriczHA6xs7OTjMbV4e1pfr3WT5re4ekcs/5T4PHOo/fOjtnxzgMgmeKx2WxidXUV6+vraDabaDabU+5Mp9PBzs5OMrpZNSYL1vb5s53V0tLxHLONRgOdTqdwtl4qgADHN5NClKVoRTIQa2nip95Y74G1LozO6aADyfS4p3FdrFkQoZ9tmYc29lnAYQePKWjQFx+NRjg8PJyaU4S/c7nMfr8/xUTYYy4vLyeMxBsbwnrQWc21HPxP74GyAmV8niujLILXxIiJ5n7Ye6Xb27R3NuR2u41ut4tOp4NOp4PRaJQcs9PpYDgcTs2FonWr5bRApcy21WqhUqmg2+1iZWUFrVYL6+vrJ8Li87bSXZi010USVD3zGIA+uHyQOf5D6fHh4fSCQucBDy2PgoAyEe+/GKPrrlggsUBPbYPLQjAD8+DgAIPBIGEdg8EA4/EY29vbGI/HyXcek9MnsgxpkQW+WwBhg1PQ9piB/c3r2a22oOdTl1MBx6a9c1udtY2TR3Oh8dFolEyopLPY2zqm6XcVc8l26vU6Op0Out0u2u021tbWCs/WLjyRTE1pm/dfWSBi3RLN1AQw1cspHbbKvT7cmr5tNRA9l9VC0spnQWdzOEarcvx9a2cfndr0w6TXotqTNijL+LSnptvDBCoOPuMKd4PBYGp5iPF4jOFwiMPDwySVm700cHKtlKxhDnaIP+vRMjkPQPQ/C5Qe8LCedGZ1vZcWaHVMDXNDRqNRMvOarvPisS3LuFleC2KVSgWtVgvVahWrq6toNBpYW1tLGMjKysqd5cLYB5dWlttiHyLeRO0hQghJhiDf7X5e+W2jVfM0kSyzWsnmcIy3feh/4LUP3YV/8NoX45f+y1P4oy/dwAf/1kvQqZ1MbrPUnveBPaUts0YMKPoNBoPELSG74HIRBBFmXio9b7VaUz22Jl55AEJKrivSK1Bn3UP9zWMdOueHlknLxfVsFEB04iTNZNV1dobDYTLvKwHEzkiWpfvoOXW+2na7jXq9jvX1dbRarQRAqL1cegDJc4GWcpapgWiDt+nMGrK1ZoHC61mzGAh/s+ZR/LVWFa9/+Cp+64+fwr9+7BkAwA++7C/hSqee7OP1vlYDsaxJTfexDYeCKEed8qU9LF04in6k+TrBctZ6QVbITANp/axidBrzsK6aAqwHauo+WTDa3t5GpVLBxsZGMi0C80QGgwE2NjYwGAwSYZn1b58BjebRbSGbabVayXun00nW2dG1aIq00vNA7I0GTmoCRYOIMg/eSIKH9qbaQDyqzO+8JkvTvXAu39Ou19YNt/35N/8V/NYfP5Vs9/Nvfjgpo5ZLGxAjAHaEKX/nyFTqJUycogvDhsAlMre2tjAcDpP/+fBTG6hWq+h2u1hbW8P6+jrW19fR7XaBWhvtdivRB4b7QLs9PW0i31XktQCv9W2v2Y570QiQTbzS1fNqtdrUvbHuC4Gz3+/jq1/9Knq9HiqVCp577rlERCU76fV6ePbZZ7GxsYGtra3EzbPPB+fLYZ1Vq1U0m03UajWsr6+jVqvhnnvuSbSPTqeTgEzRVvj4+LRe2OuBPWo3b7Pn1HKpL8reSecJVTqutDzNv+fxT2teXb3/P35xapt/+akvn3AR7DIOuk6tV24VftPuV1q59DyapckXl3cYhyre96nr+PUnNrC0tIRf+aOn8aMf/T/o7exn1pdlJFkvG95WHcLTPrx75h0TmE4uJKhub29ja2sLm5ub2NjYwObmJjY3N6dcPB1caOuPdU/WwbrS+iPYMiw+a67ZeVkp84EoRdPPgD+mgTZvFuKJcF5D5zY6cbHur8exYUlPZD2v9UZjPPqFv8A7v/PF+Nk3fiPe/x+exKNPPo93v+brsdqsuCKpFWJ1DlPeC000Y4+o199utxOqzlnWG41Gss/y8jJarVbyXqlUcPXqVayvr+PKlSu4++670Ww28eoHIh753E18/POTBKi3v/J+XGkfLzhl69+aZRk0NkZGiuhKqBtxeHiIer2OEELCOvji80l2xkQ5TdLjte7s7GBjYwPD4RDLy8u4efNmcjyefzQa4dq1axgOh0mY2zJUFUo7nU6SW9LpdFCtVrGysoJarYaVlZUERDQcXLSVtrCUPhiemq7vuu+8bFblW23Cc714HH3ZbWxi0q2wtVYN/+4nvhOrzUkD/7k3PTwFHnpuT+tgj6fhZc2JAI7HV5Bt8XV4eJhQZ4rKbMzLy8sJvWajYH5Eu91OWMl7/voVPPK5m0k5f+YND+UGDxvNsIDOcmmKvQqavGaPfWgZPGbD33m91H36/X6SV0KAZWIdXTzVPyzT0nAwk9E4QI8rBKYtTl60lRLGtTfLNqiy0JTm6S9pOkzab97Dlpd5nEXzWWvVkn2Xlpaw3q65jUm/e41Or4FshMdkD6luCUFkf38fjUYjWSYCmIBPt9tNxoFUKhWsra0lIEL6/Wv//drUuf/5H3wJP/emh5NypNWR1Z3YkLUOCR52gmOdfYzr2tiZ54HjqIsuuG0bP/fh8RiB4n8EEGohdm4U26HSXaG+0W63sbq6OrWguTJ2nmveDN2z3AASQngvgHcBiAA+B+CdAF4I4GMA7gLwBIC/G2Pcm3GcmXTeU9CLNm30aWzDM08XyOuG2XOex2yvlMaWsvbVezM8CGgkmkoFe6GaiH1suPV6PWE07EW73W7izxNQyD6q1Sp24jL+21e28CPfdg/+0fd+Az7w6Ffwh1+8jne/Zoz1ti8KetdhE8J0GysAKxCQbaXpHgCmxFfqJ1598fwctq/nZ64Mk+68Z0LdGIJFu91OgIT1SMao1859i7ZcABJCuA/AewC8NMY4CiH8LoAfAvAmAL8cY/xYCOFfAfgxAB/McbxU8LA9Pz8Xad55z8IK9ME4zTXMApKz9DRp0Qmv8VmXYHvvEP/0T3bxzXcHvPVFwCefreHzGw28/Z4RqtXp4fsxHs+yrgBCPYF+OzWG1WoFv/HDD+NKe6JDvO9vfD1+4jUvwVrreFCd9yzYc2rZ7W86H4ku20DjKFYtp56D43w0GUy1LRtNs2XgvhRu7b20jJzRH/vSEbc0nqcM8ABO58JUADRDCGMALQBfA/A6AD9y9P9HAfxjzAAQG9f3qFeZPp1ttOdlBWkgkudYadtog/K0I7ut17i8XAhNgdfoQnM54lvuXsKnnj7Ao08vAVjCq6/uYbVZRYyTXlHpOCM8THzSaA5dGfakIQSsNo5ngl9eXsZqo5L0qh4btfkctrz2u2oQmhVKtqShZps0puN+NH/DJp+p6dAAgg/HJtlng+ChkS9GVzgQjy/NxrVRJNZ70ZYLtmKMzwL4AICnMAGOHiYuy2aMkVztGQD32X1DCD8eQng8hPD4jRs3phA7ywc3x5i7f+exIM8/Pe3LO17WNXlinZdy7bl3FkzOwl7sOWk/8OD0o/KW+8dYXp7WRVTcs0lh+rJrwGjZ8tSPTYSzZWajtwlvzHexdaGN2AqTPI/myNjFoez90ikgbKjY3kNlH8o2CLTKiCxg2OcsT1u61ZbXhVkH8FYADwDYBPB7AN6QZ98Y44cAfAgAXv7yl0cr/mgvkKMceU55ZvNAZB7Hpnm+PH/TBDDd39OQvOPaBLK08uj5tAHaAXm/86Vpv/+Tz9bwvVf3pkZ/ai9qXQHVGXTZCO8as/IZFCBsMpjOgm7Bg6OCOaCPjEPBg6Iw64/HG41GSeLX1tbW1FKVKtyqdsJ31SjsfVXG1mg0sLq6mqSnq04UQkjEadaVnbuE7aloy+s4fQ+Ar8QYn48xjgF8HMB3AVgLIbDl3w/g2ayDeEJPFmKmuTiX3dJcEb7ndYXy/pfFeg4PD9HfO8T/fv4Qr7034le/4wDf/YJ9/OnmMkYH0w1+VmRNtS/+pu88lt1HzWMctrweANKV0fN4CX56fk3dtxqINyDOqzvL5GxHoMxH0/rJ5GyZymAZWZbXaXoKwKtCCC0AIwCvB/A4gE8DeBsmkZh3APjErANpr6Smfj1wuujF7WSeH8zfbYOydZK2jxXmvAfMCqZMjooxJpEBbSQcPbu7u4vlgwO895v2UccB9vYO8cZ7DvDq9QPUwz6007PJgErPrVBoQcYDm6zy6zgeXeBJB6x5jVq1DoaoyUhijFPMZnt7G71eD/1+HxsbG8kgQdVRtEF7LMO6g6wHisiqcXASorW1tWQqgEajMRV1YT1oCvxZ9blbYbkAJMb4WAjhEQCfAbAP4LOYuCX/HsDHQgj/7Oi3j8w6VtkXXKblYQ5aN9bN0N+8//KcX3tFbZA22crOl1rH/lSP3lyOODiY9uXTXKXTakVpZbeamcc4rC6ijVg1B61ruhgEGJ3PleN6NH9EZ1GzZdR3z4308mlsirqmqhPsGC7nMfV68tTfvCy3bBtj/AUAv2B+/jKAbz/NCT2qeFnYxXksTX9RFnIal4Tv+hCn0Xw2CCsQ2oiGpc8KADosIW3sj+eS2kbO/b1ePS3qYgfIqdiYxQJUr1BmTFbD9HOdYU2jMHo8ey8s2AET8KjX60mGbq1WQ6fTSbSP9fX1JPdDx7jo/bO5LmW7+aWsTGcv9k4HkKzr93p2zw+2vR8/a4/lNT7LPpSFqNBtgYRlU/CwwGFdFt1Hr8tjC3a7rLJbALGNi9ehGZsKyjYqqElnBBBNQEvL57B1bt0bRlo4zoWTAK2traHZbOLKlSuoVqtJ2r+OxdHQsJ6j7I64tLEwnqX5/JfFsrSNs9SJBQ19YD32wf/TmIWCAx9cbVzKDrRhaCIW3+1cGjymll3LQxfCCppZwqQFEh1Mp+XV7S2Q2bq3Oou3GpzWcdo9tXXEOuGwfKbzE0A4KbKOklbw02u1elmZbaaU+UC8EB5tFk2/DACTJhCnCSTNE3wAAA+7SURBVKZenXigoTTd69VtT6aage1RaV5qt8dA1A3ImhLAlo2Ng2FK635YBuExJboWOpcJz2fBwIaSFag0oY5sxuZ/zAIR795R86jX68n0g/fcc08CHmtra6jX6+h2uwngxXicQUsxm78BSLQRfW6y2s68rPSlLfMIqpcBNNSsf6+fbS/G37QxneYcnjai5rkcMcapZCo2As91IoDosWaFR7U81s1io8/SbnThKhtO9ai9LTeZiO2QCGQe+0ir91nuJ8FDZ2HTZDE7dYCCdRbD1Ou0gF6klQ4gQDaIlKkwz8O868xzjQoi/G5Zg2YsagPUY2iD5nKNdEH4+/7+8Zq1ygj0GlSrsLkd9jwWRLgfG+Z4PE4awtLSUgIKKiCyJ9Z5R7e3t09ERrwGpq6RArEFFWocg8EA29vbybwh7P1VV7H3Q8/J49Id4XyljUYDV69eTUK13W430TuYwGYB0+oeFijtpFBFW2kA4tF4r3db2ElLc4E8P1zfrbagDVypPYCE7lufm8dT7cK6KGn6gpo2dG0cAJK5RIDjtV+UfdiIkdVytHFrpESBw/bwvB7LbuzSDVr/eo1p7I7so9VqJZMDcUY2Ow+sxzKzWAjrz4taFWWlrQtjP3vb6bv9fLuad832oc97HKtNsLfVBCke3/agygaA48mAtPfX3k/Lat0EW2aPStv7OOs6qYkwWYuJbYPBAMPhMJkVXofIk2l45+NoVg2DqjA6Ho+xubmJ3d1dbGxsoN/vo9/vJzPPpy1qredgfRCUOQx/ZWUF9957b7LwNtdz0VwPHs9jHrauua26irw3RVspDOQ0vrzuc1nMUuhZGkcWyFrwSWtAnvDHBmeHiGvkxRMNVeiz58pz7bPM6iLqwuiEzqPRKGEjHiPT7zpjOdmMzSPh8psEKK5vo6Nv08BPwV8HxzG6wkhLt9tNJmPiAlM2y9SyDL2nBPY0hlc0iFwIDWRhE/MYSJZm4pkNv9I8Kq+/a9SErgmAE+nafMjzgoYtiyfEevvwXdfxZYo9G/ZwOJwCm6zjUhRW0ZXMgsP0+/1+suIe5y1VdkPwpFnQJQshMHBW9m63i263m6xWx+kKdQY0jZBpZMq6Z2rKPLh90VY4gJRBsy66WRchC0TyuHTeeCMVKRUENPLhuTC6vRX00q7DnjfPNfNdXxRN2Tj29/eThk2hUxmUxw5U0CQQEUg4xeHW1hZ2d3exubmJvb09bG1tJQKqrjHMOk9z2RhaJbsg81hZWcHq6irq9XoyEbVd2pLXqwCnESALIrYjSLsn87YLxUCy/ObLZll+q4JIXsD1hD3vP2USms6tDd0CkEYF0iIZ9vh53BuvTPxdQ6o2vKpZp6oDeDP7W5FW3RZdopNgohMupw2Pt9fMF4GBCz1ROK3Vai6wKSioYKvXrQBir80Thou2CwUgapcZPPKY15vPEqCtFpLWI2tOR4xxaiZ2NjK6Ll5Ku6aFW9Ywy49nGbgthVIbxuVLGQi/033hKvc2XV7PoQ2QDISzo29tbaHf7+PatWtJ2jrX+LWjbS2gq7jJOuT8HXfdddfUspNcC9iKo+q6HBwcJGxH5xmxmbU0zuhGIygWbbeNC3MWwe52tCxGchpmklZPHsvjQ20nB9IQYR6Xg9srlc5iKJqfoT2s9r5sxN7gPi9HRc+pPTiBUCdW5oszpWtYOO15U4bHa9IQOHM/NEyr6fR2OUsFCgrF3tq5Foz39qYnc1KwKdIKBRDrw83a1tplB4/TWloPP8ssM7EgolEK1UnsNpbJeGNsrH6i4UptjFomBQ9N4bYTIttZzhSM1E0hq9jf309mFOv1eonWsbGxMaU38Pict8M+iwrknDxacz04zoVaCDBZ6gEABoNBwrosSFIE9VLUFUTtfbcuXJFWmgtjKZn9z9qdAh55QtyWDWS5M1nnsZ81gqMUHUASnVHabHNFNOKS5npZRmLLQIFTX15EgtvaAXs8tuoouh4LQ7V0gxiBUVdB3RZv3JaeT5fwtC/rYpAJAZiailFZFoBEM7HXxMiR1rkCKFcOLNJKcWEUWe8UYDiNWTdlFtB622WBh93ehiPVtcgqo93f29cCmX5Oo9yWgVgRVfNX0kb+skESPBhh2dzcTIRTLgrOJS71/DyGBTwdcUyGopMhc25flpnHtgClDEST24DjLGDWnwK0TgytIOdNMVCElcJArBCY1UAWADNtHp0+7b559ZM0IVa3s8Ku/s7P9nxWI7H/23R1TWFXCq8N2YaLlYFQNB2NRuj3+9jZ2UkSxXSBqbR68lwGnYlewUMnZdYRxlpuq+kouADH+pOtMxW3uT2Pr0ypSLswURjLSu4U8DgNeM4CD90nLWpg9/UaeJpI6pU97aG17okn/qqrARxHKVTrYA9uBUbg5DrD2rvzGBRJmddBF4azqmuSmFevnmtIACF4NBqNqVG1AKaSurLq0OaCWDbH/wggtk71+i+9iGrNEwEVPMpA1ItiWdfv/Z4FvGn6g32o1bfW36wYmuYCsXFZgdR7t8dW4VYHtLGRWyZC/9+KrwQbBQ6OmxkOh+j1ekn0RfUEr548U72hXq8n+R4cNEcGYvM3vHuSljXsZRErgLCerd0RLozX456GacyTmZwlMnSa45zF3chzHo95pG0/y2XUz/rdZrDqMb1wata5bKhXG37afB/0/RVAeHy6Cna8DHNFlHWoW6Taglev1n2zrhkbsi7LQHBR90otq5HrYDzd3gMaHUQH5HdPb7WVMieq9zstj/iX1Qjm6fYUxYpOo1Hoe57jeD2hdVksVdYGnzYRjz2fF+bl//qfAgcTuNjIeUxdnFozRNmL8zsBotfrYTAYoN/v4/nnn8dwOMT169enksVUdOT18GVX1VNTcZOuDGdRX1tbO7GyHY9PsNJRvVrPGor2UtyVMWp5vftYpJXqwpzmwbcPrdfjZR3nNMDi+et5/juNpYFglv9ty3ArTRu3PdetejAtiGiDsFmu2sC937VMFBZ1akMdsUtgsUKsBQ47N4rOj6L1Yetf8zA81uHVo8dE9NgK2l6HkdYRX2oGch6bRc1PY1mVnPdY5wWSs4LAPN039cv1XNrAtOHTfUnLmAROJkFpRCINFPQ/hnF1dXsrupKlcDTt5uYm+v0+tre3cePGjWTErbIXsgeGYhVALANRBqF1w980n4PH1sbMbQC4c9BaN9C6TSyrgq3nGl16BlIm/fcebNqtbpTnuc60qEgRrpmeO4+YbVlEWh17Qql+tq4TP1M81fClgodqJwQJq31wmkI75SHZBTULFWR1MSeNcHiuml6bBQatJ1tmBWHWd9b90f/Jii5CoOG2YSDA/Hrf8zTQWW7IrbCzlO20ZciKpKQdXxt+2ncFCztRMXtjDdUygsIoyXA4dIFKIzKj0Qjj8Rg3b95Mhudvb29PTTpkQ78cp0INQwFD5+kgO9DwsT4v/G95eTm5Diu2aj3oAD0yCZ3oSM+p9ezdL70/ee7ZPOy2ApB5mNfzXhY7C5DZ61eNQI/rAYdlEcokvNnOyS7onlBA5YC3vb29ZISquowUIxmt4XD8Xq+XAAjZB10glouuCcOu1WoVjUYjAQ4AJ9LQ2TCtC8XyKICksQrWhU0E4/E5wbXqMFn3J6+QPm+77QDkNBU1y53JamDndUPOcyx1H856jtOcM29589a9ZR8WSOzAOAII3Q3Oy0EgAaZnnAcwtWYLZ063SWLqtqg4aceuqMui4inrwTIYrR/VdTSRS6M1ChR2MBx1J5u+bu+/fWYJ6vo9DXTmaRcKQIrUSOxNLLIMeew8/u15wIO/WTE1K2JmGYnqGzpUXRdrUl2CbgZdEUZQqGvEGJNZx9gAyVAomvJdl6Jkj6+RFbou9Xo9SUFXF8aGUpVxsKHzswq4IQTs7OycWCPH1pWnkZAV6bwoKurmvY93PICUYVk9/TyF1nmdYx76S15hOo+pT2/dH88d0t7bizx484RYUdeWU9mI1X7099Ncn56LGkZag7b3PKsDS9M/7Od5C+1pForscUMIzwP4fwDuBnC9sBPfGluUuTi7Hct9kcr84hjj1SJOVCiAJCcN4fEY4ysKP/E5bFHm4ux2LPftWOZbYcU7TQtb2MIujS0AZGELW9iZrSwA+VBJ5z2PLcpcnN2O5b4dy3xuK0UDWdjCFnY5bOHCLGxhCzuzLQBkYQtb2JmtUAAJIbwhhPDFEMKfhxB+ushz57UQwotCCJ8OIfxZCOFPQwg/efT7lRDCfw4hfOnofb3ssloLISyHED4bQvjk0fcHQgiPHdX374QQamWX0VoIYS2E8EgI4ckQwhdCCN9x0es6hPDeo2fj8yGE3w4hNG6Hup6HFQYgIYRlAL8G4I0AXgrgh0MILy3q/KewfQA/FWN8KYBXAXj3UTl/GsCjMcYHATx69P2i2U8C+IJ8/xcAfjnG+A0ANgD8WCmlyrZfBfCfYowPA/gWTMp/Yes6hHAfgPcAeEWM8ZsALAP4IdwedX3LrUgG8u0A/jzG+OUY4x6AjwF4a4Hnz2Uxxq/FGD9z9LmPyQN9HyZl/ejRZh8F8P3llNC3EML9AN4M4MNH3wOA1wF45GiTi1jmVQDfDeAjABBj3IsxbuKC1zUmQ0CaIYQKgBaAr+GC1/W8rEgAuQ/A0/L9maPfLqyFEL4OwLcCeAzAC2KMXzv66xqAF5RUrDT7FQDvA8DRWncB2Iwxcn2Bi1jfDwB4HsBvHrleHw4htHGB6zrG+CyADwB4ChPg6AF4Ahe/rudiCxE1xUIIHQD/FsDfjzFu6X9xEvu+MPHvEML3AXguxvhE2WU5pVUAvBzAB2OM3wpgAOOuXMC6XseEIT0A4F4AbQBvKLVQJVqRAPIsgBfJ9/uPfrtwFkKoYgIe/ybG+PGjn/8ihPDCo/9fCOC5ssrn2HcB+JshhP+LiWv4Oky0hbUjmg1czPp+BsAzMcbHjr4/ggmgXOS6/h4AX4kxPh9jHAP4OCb1f9Hrei5WJID8TwAPHqnVNUyEp98v8Py57Eg7+AiAL8QYf0n++n0A7zj6/A4Anyi6bGkWY/yZGOP9Mcavw6Re/zDG+KMAPg3gbUebXagyA0CM8RqAp0MI33j00+sB/BkucF1j4rq8KoTQOnpWWOYLXdfzsqKH878JE199GcBvxBjfX9jJc1oI4dUA/iuAz+FYT/hZTHSQ3wXwlzGZkuAHYow3SylkhoUQXgPgH8YYvy+E8BJMGMkVAJ8F8HdijLtlls9aCOFlmAi/NQBfBvBOTDq2C1vXIYR/AuAHMYnYfRbAuzDRPC50Xc/DFqnsC1vYws5sCxF1YQtb2JltASALW9jCzmwLAFnYwhZ2ZlsAyMIWtrAz2wJAFrawhZ3ZFgCysIUt7My2AJCFLWxhZ7b/D12Ch/q2/RUKAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo44bDuGf8q5"
      },
      "source": [
        "def submit(model, test_image ):\n",
        "    test_image = torch.from_numpy(test_image).type(torch.FloatTensor).to(DEVICE)\n",
        "    test_image /= 255.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(test_image)\n",
        "    prediction = prediction.cpu().numpy()\n",
        "    return prediction"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcJOESJf4GVK"
      },
      "source": [
        "pred = submit(best_model, test_image)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLtyDMRQ4ULg"
      },
      "source": [
        "IdLookupTable = pd.read_csv('IdLookupTable.csv')"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lZjji-A4YuO"
      },
      "source": [
        "RowID = list(IdLookupTable.RowId)\n",
        "ImageID = list(IdLookupTable.ImageId)\n",
        "FeatureHead = list(training_pd.columns.values)\n",
        "FeatureIndex = [FeatureHead.index(feature) for feature in IdLookupTable.FeatureName]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2Zt1nvTw4lk9",
        "outputId": "7f73c1ae-d07d-49fb-a997-8fb7b49f84f2"
      },
      "source": [
        "location = []\n",
        "for image_id, feature_id in zip(ImageID, FeatureIndex):\n",
        "    location.append(pred[image_id-1][feature_id])\n",
        "submission = pd.DataFrame({'RowID': RowID, 'Location': location})\n",
        "submission.head()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowID</th>\n",
              "      <th>Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>66.344727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>37.767342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>29.735804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>37.056568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60.648464</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowID   Location\n",
              "0      1  66.344727\n",
              "1      2  37.767342\n",
              "2      3  29.735804\n",
              "3      4  37.056568\n",
              "4      5  60.648464"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuctV2kl4nOg"
      },
      "source": [
        "def adjust_max(row):\n",
        "    if row.Location > 96.:\n",
        "        row.Location = 96.\n",
        "    return row\n",
        "\n",
        "submission = submission.apply(adjust_max, axis='columns')"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmScDCvO4-S3"
      },
      "source": [
        "submission.RowID = submission.RowID.astype('int')"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_9EZD-0I5Auu",
        "outputId": "375f44cc-11a0-4bac-ae21-8c807d6c00aa"
      },
      "source": [
        "submission.head()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RowID</th>\n",
              "      <th>Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>66.344727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>37.767342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>29.735804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>37.056568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>60.648464</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   RowID   Location\n",
              "0      1  66.344727\n",
              "1      2  37.767342\n",
              "2      3  29.735804\n",
              "3      4  37.056568\n",
              "4      5  60.648464"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxd-53Kc5CS1"
      },
      "source": [
        "submission.to_csv('submission_final.csv', index=None)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "D-kAMdC28Viy",
        "outputId": "4b31af7d-6203-467e-8b65-2bd39912e524"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e5843cab-25cb-4e0c-92b8-a2d10260f951\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e5843cab-25cb-4e0c-92b8-a2d10260f951\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 66 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhJYwLWL5D24",
        "outputId": "0688a8f8-3343-48a7-f81d-fbe6d2ca50ff"
      },
      "source": [
        "!kaggle competitions submit -c facial-keypoints-detection -f submission_final.csv -m \"FKD_improved\""
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "100% 629k/629k [00:01<00:00, 424kB/s]\n",
            "Successfully submitted to Facial Keypoints Detection"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unOmhmy077XY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
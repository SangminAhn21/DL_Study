{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FKD_hyperparameter_tuning",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1t9qbOmt01EYfhWnjzeSzdXhBhQg83f8Q",
      "authorship_tag": "ABX9TyNuHZho0+wH9G3WRFfRnz7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangminAhn21/DL_Study/blob/main/Kaggle/Facial_Keypoint_Detection/FKD_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHnN-GMgMpTX",
        "outputId": "37181c4a-ecd1-421c-be65-2b6db4a4b29c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JrCNHFvM75T",
        "outputId": "cadf46c8-28bb-4f78-d129-dd00b127d042"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLdIIW2lM_1g",
        "outputId": "ad1a9dd1-7e51-42e5-a02d-30b91940fbbf"
      },
      "source": [
        "%cd drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JWEf9yZUCxB",
        "outputId": "16361a2d-d703-43fa-e4f8-c85c603e75ef"
      },
      "source": [
        "pip install kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "UNgjUTgpUFOa",
        "outputId": "44f892bd-2dd1-44f4-d688-1d850ed298ee"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6c6fe990-c915-4412-a74a-098e84790d95\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6c6fe990-c915-4412-a74a-098e84790d95\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 66 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra7lY14CUFrO",
        "outputId": "05739c97-e518-403c-d969-2d0ef65ab14d"
      },
      "source": [
        "!kaggle competitions download -c facial-keypoints-detection"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "test.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "SampleSubmission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "training.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "IdLookupTable.csv: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpi5Y_5Uhdg",
        "outputId": "f4728eda-7a74-42a4-fb5a-e21fbd430afe"
      },
      "source": [
        "!unzip training.zip\n",
        "!unzip test.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  training.zip\n",
            "replace training.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Archive:  test.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx54U6VcQoo2"
      },
      "source": [
        "import models, utils"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1_OqSV6RD3E"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "training_pd = pd.read_csv('training.csv')\n",
        "test_pd = pd.read_csv('test.csv')\n",
        "\n",
        "training_pd = training_pd.fillna(method='ffill')\n",
        "\n",
        "training = training_pd.to_numpy()\n",
        "test = test_pd.to_numpy()\n",
        "\n",
        "train_image = training[:, -1]\n",
        "train_key = training[:, :-1].astype('float64')\n",
        "test_image = test[:, 1]\n",
        "\n",
        "train_image = np.array([np.array([int(pixel) for pixel in image.split()]).\\\n",
        "                        reshape(96, 96) for image in train_image])\n",
        "test_image = np.array([np.array([int(pixel) for pixel in image.split()]).\\\n",
        "                       reshape(96, 96) for image in test_image])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcAFuW3Sqifu",
        "outputId": "8906f427-20bd-40fb-d70d-06111d43070c"
      },
      "source": [
        "pip install ray"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray\n",
            "  Downloading ray-1.8.0-cp37-cp37m-manylinux2014_x86_64.whl (54.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.7 MB 34 kB/s \n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.0.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.41.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.3.2)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (2.6.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.2.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Collecting deprecated\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->redis>=3.5.0->ray) (1.13.3)\n",
            "Installing collected packages: deprecated, redis, ray\n",
            "Successfully installed deprecated-1.2.13 ray-1.8.0 redis-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWCw038zxCn_",
        "outputId": "47d2bdcd-cb7d-4693-9c99-741b0b97b774"
      },
      "source": [
        "pip install -U tensorboardx"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardx\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 25.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n",
            "Installing collected packages: tensorboardx\n",
            "Successfully installed tensorboardx-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOOM5Ccq9IsW"
      },
      "source": [
        "from utils import FaceDataset, RMSELoss\n",
        "from functools import partial\n",
        "from models import CNN\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from filelock import FileLock\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "\n",
        "def cnn_train(config, data, checkpoint_dir=None, data_dir=None):\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = torch.device('cuda')\n",
        "    else:\n",
        "        DEVICE = torch.device('cpu')\n",
        "    print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)\n",
        "\n",
        "    model = CNN(config['l1'], config['l2']).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'])\n",
        "    criterion = RMSELoss()\n",
        "    print(model)\n",
        "\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    dataset = FaceDataset(data[0], data[1])\n",
        "\n",
        "    lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\n",
        "    train_data, val_data = torch.utils.data.random_split(dataset, lengths)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_data,\n",
        "                          batch_size=config['batch_size'],\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "    val_loader = DataLoader(dataset=val_data,\n",
        "                        batch_size=config['batch_size'],\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)\n",
        "\n",
        "    # model.train()\n",
        "    for Epoch in range(20):\n",
        "        # running_loss = 0.0\n",
        "        # epoch_steps = 0\n",
        "        for batch_idx, (image, key) in enumerate(train_loader):\n",
        "            image = image.to(DEVICE)\n",
        "            key = key.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(image)\n",
        "            loss = criterion(output, key)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # running_loss += loss.item()\n",
        "            # epoch_steps += 1\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                    Epoch, batch_idx * len(image),\n",
        "                    len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
        "                    loss.item()))\n",
        "                # running_loss = 0.0\n",
        "\n",
        "        # model.eval()\n",
        "        val_loss = 0.0\n",
        "        for image, key in val_loader:\n",
        "            with torch.no_grad():\n",
        "                image = image.to(DEVICE)\n",
        "                key = key.to(DEVICE)\n",
        "                output = model(image)\n",
        "                val_loss += criterion(output, key).item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_loss *= config['batch_size']\n",
        "        print('\\n[EPOCH: {}], \\tVal Loss: {:.4f}\\n'.\n",
        "        format(Epoch, val_loss))\n",
        "\n",
        "        with tune.checkpoint_dir(Epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=val_loss)\n",
        "    print(\"Finished Training\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euECRmzGFaU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f6b409c-f129-461d-94b3-7176cc296bd8"
      },
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    data_dir = os.path.abspath(\"./data\")  # 특정 경로에 대해 절대 경로 얻기\n",
        "    config = {\n",
        "        'l1': tune.sample_from(lambda _: 2**np.random.randint(3, 8)),\n",
        "        'l2': tune.sample_from(lambda _: 2**np.random.randint(3, 8)),\n",
        "        'lr': tune.loguniform(1e-3, 1e-1),\n",
        "        'batch_size': tune.choice([8, 16, 32])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric='loss',\n",
        "        mode='min',\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    \n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"training_iteration\"])\n",
        "    \n",
        "    result = tune.run(\n",
        "        tune.with_parameters(partial(cnn_train, data_dir=data_dir), data=(train_image, train_key)),\n",
        "        resources_per_trial={'cpu': 1, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "\n",
        "    best_trained_model = CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = torch.device('cuda')\n",
        "    else:\n",
        "        DEVICE = torch.device('cpu')\n",
        "    best_trained_model.to(DEVICE)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    PATH = '/content/drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection/saved_model/'\n",
        "    torch.save(best_trained_model, PATH + str(best_trial.config) + 'val_loss: ' + str(best_trial.last_result['loss']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=20, gpus_per_trial=1)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:07 (running for 00:00:00.21)\n",
            "Memory usage on this node: 3.8/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (10 PENDING)\n",
            "+----------------------------------+----------+-------+--------------+------+------+------------+\n",
            "| Trial name                       | status   | loc   |   batch_size |   l1 |   l2 |         lr |\n",
            "|----------------------------------+----------+-------+--------------+------+------+------------|\n",
            "| tune_with_parameters_f6201_00000 | PENDING  |       |           16 |   64 |   32 | 0.0323551  |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |       |           64 |    8 |   16 | 0.0317033  |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |       |           16 |  128 |   64 | 0.00278453 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |       |           32 |   32 |    8 | 0.00966311 |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |       |           64 |  128 |   64 | 0.00207041 |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |       |            8 |    8 |   64 | 0.00112382 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |       |           64 |   32 |   16 | 0.00210753 |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |       |           16 |    8 |   32 | 0.0226108  |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |       |           32 |   32 |    8 | 0.00598872 |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |       |           16 |  128 |   16 | 0.00539106 |\n",
            "+----------------------------------+----------+-------+--------------+------+------+------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[1m\u001b[33m(scheduler +14m28s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 1.0, 'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:12 (running for 00:00:05.22)\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m   (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m   (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m   (fc3): Linear(in_features=32, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1570)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=1570)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.346024\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 5.985302\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 2.934462\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 5.582454\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-26-17\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.265287729357997\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 6.37751030921936\n",
            "  time_this_iter_s: 6.37751030921936\n",
            "  time_total_s: 6.37751030921936\n",
            "  timestamp: 1637306777\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 0], \tVal Loss: 3.2653\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 3.125282\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 4.548529\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:18 (running for 00:00:11.21)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.26529 |                    1 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 2.630565\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 3.774493\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 1], \tVal Loss: 5.0307\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 5.109639\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 3.275972\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 2.996475\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 3.285222\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-26-23\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.300050183559986\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 13.21760368347168\n",
            "  time_this_iter_s: 3.492060661315918\n",
            "  time_total_s: 13.21760368347168\n",
            "  timestamp: 1637306783\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:23 (running for 00:00:16.51)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: None | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.30005 |                    3 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 2], \tVal Loss: 3.3001\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 3.245633\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 4.167390\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 4.177038\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 2.725878\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 3], \tVal Loss: 3.8952\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 3.552910\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 4 [1600/5639(28%)]\tTrain Loss: 2.644384\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 4 [3200/5639(57%)]\tTrain Loss: 2.951181\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:29 (running for 00:00:21.77)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.89524 |                    4 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 4 [4800/5639(85%)]\tTrain Loss: 2.684077\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-26-30\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 5\n",
            "  loss: 3.5654832880547707\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 19.759028911590576\n",
            "  time_this_iter_s: 3.2933530807495117\n",
            "  time_total_s: 19.759028911590576\n",
            "  timestamp: 1637306790\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 4], \tVal Loss: 3.5655\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 3.346303\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 5 [1600/5639(28%)]\tTrain Loss: 4.284628\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 5 [3200/5639(57%)]\tTrain Loss: 2.578989\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 5 [4800/5639(85%)]\tTrain Loss: 3.257303\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 5], \tVal Loss: 3.2337\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 3.374401\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 6 [1600/5639(28%)]\tTrain Loss: 3.330929\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:34 (running for 00:00:27.46)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.23366 |                    6 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 6 [3200/5639(57%)]\tTrain Loss: 3.031158\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 6 [4800/5639(85%)]\tTrain Loss: 2.815270\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-26-37\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 7\n",
            "  loss: 3.1674761562482687\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 26.579911947250366\n",
            "  time_this_iter_s: 3.4240784645080566\n",
            "  time_total_s: 26.579911947250366\n",
            "  timestamp: 1637306797\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 6], \tVal Loss: 3.1675\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 3.009243\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 7 [1600/5639(28%)]\tTrain Loss: 5.905359\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 7 [3200/5639(57%)]\tTrain Loss: 3.516823\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 7 [4800/5639(85%)]\tTrain Loss: 3.923634\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:40 (running for 00:00:32.91)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: None | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.16748 |                    7 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 7], \tVal Loss: 3.5244\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 8 [0/5639(0%)]\tTrain Loss: 3.446854\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 8 [1600/5639(28%)]\tTrain Loss: 3.159437\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 8 [3200/5639(57%)]\tTrain Loss: 2.978883\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 8 [4800/5639(85%)]\tTrain Loss: 2.792574\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-26-44\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 9\n",
            "  loss: 3.395188920041348\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 33.56426000595093\n",
            "  time_this_iter_s: 3.532287120819092\n",
            "  time_total_s: 33.56426000595093\n",
            "  timestamp: 1637306804\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 8], \tVal Loss: 3.3952\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 9 [0/5639(0%)]\tTrain Loss: 3.157714\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 9 [1600/5639(28%)]\tTrain Loss: 3.241492\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 9 [3200/5639(57%)]\tTrain Loss: 3.152429\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:46 (running for 00:00:38.87)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.39519 |                    9 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 9 [4800/5639(85%)]\tTrain Loss: 3.616668\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 9], \tVal Loss: 3.1483\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 10 [0/5639(0%)]\tTrain Loss: 4.047760\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 10 [1600/5639(28%)]\tTrain Loss: 3.145873\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 10 [3200/5639(57%)]\tTrain Loss: 3.025080\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 10 [4800/5639(85%)]\tTrain Loss: 3.028458\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-26-51\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 11\n",
            "  loss: 3.079577025285004\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 40.49937963485718\n",
            "  time_this_iter_s: 3.5439062118530273\n",
            "  time_total_s: 40.49937963485718\n",
            "  timestamp: 1637306811\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 11\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 10], \tVal Loss: 3.0796\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 11 [0/5639(0%)]\tTrain Loss: 3.445404\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 11 [1600/5639(28%)]\tTrain Loss: 2.657229\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:52 (running for 00:00:44.80)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.07958 |                   11 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 11 [3200/5639(57%)]\tTrain Loss: 3.715557\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 11 [4800/5639(85%)]\tTrain Loss: 2.848857\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 11], \tVal Loss: 3.2047\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 12 [0/5639(0%)]\tTrain Loss: 2.760997\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 12 [1600/5639(28%)]\tTrain Loss: 3.542401\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 12 [3200/5639(57%)]\tTrain Loss: 3.094189\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 12 [4800/5639(85%)]\tTrain Loss: 7.309835\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:26:57 (running for 00:00:50.15)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.20475 |                   12 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-26-57\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 13\n",
            "  loss: 3.1215307628009334\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 47.17842149734497\n",
            "  time_this_iter_s: 3.329130172729492\n",
            "  time_total_s: 47.17842149734497\n",
            "  timestamp: 1637306817\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 13\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 12], \tVal Loss: 3.1215\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 13 [0/5639(0%)]\tTrain Loss: 2.943622\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 13 [1600/5639(28%)]\tTrain Loss: 3.422424\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 13 [3200/5639(57%)]\tTrain Loss: 2.832954\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 13 [4800/5639(85%)]\tTrain Loss: 2.696410\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 13], \tVal Loss: 3.2896\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 14 [0/5639(0%)]\tTrain Loss: 2.764858\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 14 [1600/5639(28%)]\tTrain Loss: 3.272130\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 14 [3200/5639(57%)]\tTrain Loss: 3.168376\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:03 (running for 00:00:55.80)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: None | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |   loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.2896 |                   14 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |        |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |        |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |        |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |        |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |        |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |        |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |        |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |        |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |        |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 14 [4800/5639(85%)]\tTrain Loss: 7.275588\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-27-04\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 15\n",
            "  loss: 3.0994277737664837\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 53.85019254684448\n",
            "  time_this_iter_s: 3.3529796600341797\n",
            "  time_total_s: 53.85019254684448\n",
            "  timestamp: 1637306824\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 15\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 14], \tVal Loss: 3.0994\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 15 [0/5639(0%)]\tTrain Loss: 3.184826\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 15 [1600/5639(28%)]\tTrain Loss: 2.620137\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 15 [3200/5639(57%)]\tTrain Loss: 3.349498\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 15 [4800/5639(85%)]\tTrain Loss: 3.189076\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 15], \tVal Loss: 3.0914\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 16 [0/5639(0%)]\tTrain Loss: 2.802334\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 16 [1600/5639(28%)]\tTrain Loss: 4.103874\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:08 (running for 00:01:01.50)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.09138 |                   16 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 16 [3200/5639(57%)]\tTrain Loss: 2.690990\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 16 [4800/5639(85%)]\tTrain Loss: 3.292672\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-27-11\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 17\n",
            "  loss: 3.152801221482297\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 60.65308356285095\n",
            "  time_this_iter_s: 3.446286678314209\n",
            "  time_total_s: 60.65308356285095\n",
            "  timestamp: 1637306831\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 17\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 16], \tVal Loss: 3.1528\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 17 [0/5639(0%)]\tTrain Loss: 3.755380\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 17 [1600/5639(28%)]\tTrain Loss: 3.304478\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 17 [3200/5639(57%)]\tTrain Loss: 2.910539\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 17 [4800/5639(85%)]\tTrain Loss: 2.892513\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:14 (running for 00:01:06.96)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |   loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.1528 |                   17 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |        |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |        |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |        |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |        |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |        |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |        |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |        |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |        |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |        |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 17], \tVal Loss: 3.3975\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 18 [0/5639(0%)]\tTrain Loss: 3.763737\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 18 [1600/5639(28%)]\tTrain Loss: 3.091994\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 18 [3200/5639(57%)]\tTrain Loss: 3.698248\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 18 [4800/5639(85%)]\tTrain Loss: 3.401062\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-27-18\n",
            "  done: false\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 19\n",
            "  loss: 3.148659301649594\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 67.37624382972717\n",
            "  time_this_iter_s: 3.348680257797241\n",
            "  time_total_s: 67.37624382972717\n",
            "  timestamp: 1637306838\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 19\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 18], \tVal Loss: 3.1487\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 19 [0/5639(0%)]\tTrain Loss: 2.387544\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 19 [1600/5639(28%)]\tTrain Loss: 2.855576\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 19 [3200/5639(57%)]\tTrain Loss: 4.317676\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:20 (running for 00:01:12.71)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (9 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | RUNNING  | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.14866 |                   19 |\n",
            "| tune_with_parameters_f6201_00001 | PENDING  |                 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING  |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING  |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING  |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING  |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING  |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING  |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING  |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING  |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m Train Epoch: 19 [4800/5639(85%)]\tTrain Loss: 3.585723\n",
            "Result for tune_with_parameters_f6201_00000:\n",
            "  date: 2021-11-19_07-27-21\n",
            "  done: true\n",
            "  experiment_id: ba0393095888428797db4ab915fd4ebd\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 20\n",
            "  loss: 3.1176741390363545\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1570\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 70.79909205436707\n",
            "  time_this_iter_s: 3.4228482246398926\n",
            "  time_total_s: 70.79909205436707\n",
            "  timestamp: 1637306841\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 20\n",
            "  trial_id: f6201_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m [EPOCH: 19], \tVal Loss: 3.1177\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1570)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:25 (running for 00:01:18.12)\n",
            "Memory usage on this node: 4.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (8 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00001 | RUNNING    | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  |         |                      |\n",
            "| tune_with_parameters_f6201_00002 | PENDING    |                 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m   (fc1): Linear(in_features=4608, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m   (fc2): Linear(in_features=8, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m   (fc3): Linear(in_features=16, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1569)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=1569)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.862083\n",
            "Result for tune_with_parameters_f6201_00001:\n",
            "  date: 2021-11-19_07-27-27\n",
            "  done: true\n",
            "  experiment_id: 1a3eb71451b24c7b8b6805acb1b01eb7\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 4.078365331338652\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1569\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.134280681610107\n",
            "  time_this_iter_s: 4.134280681610107\n",
            "  time_total_s: 4.134280681610107\n",
            "  timestamp: 1637306847\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m [EPOCH: 0], \tVal Loss: 4.0784\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=1569)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:30 (running for 00:01:23.39)\n",
            "Memory usage on this node: 4.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.6718265303483246\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m   (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m   (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2068)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=2068)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.836361\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 3.554539\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 3.025815\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 4.119226\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:35 (running for 00:01:28.41)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -5.030723950541612 | Iter 1.000: -3.6718265303483246\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 |         |                      |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 0], \tVal Loss: 3.1240\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-27-36\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.1240166657359887\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 6.3465516567230225\n",
            "  time_this_iter_s: 6.3465516567230225\n",
            "  time_total_s: 6.3465516567230225\n",
            "  timestamp: 1637306856\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 2.711570\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 3.862573\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 2.945088\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 3.110519\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 1], \tVal Loss: 3.5311\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 3.037732\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 2.634698\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:41 (running for 00:01:33.60)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 3.53109 |                    2 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 4.099458\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 3.112130\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-27-43\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 3\n",
            "  loss: 2.926197906927014\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 13.049360752105713\n",
            "  time_this_iter_s: 3.365964412689209\n",
            "  time_total_s: 13.049360752105713\n",
            "  timestamp: 1637306863\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 2], \tVal Loss: 2.9262\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 4.350762\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 2.427219\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 2.631902\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 2.844815\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:46 (running for 00:01:38.96)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.8952432781246538 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.9262  |                    3 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 3], \tVal Loss: 2.9522\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 2.465754\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 4 [1600/5639(28%)]\tTrain Loss: 1.971896\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 4 [3200/5639(57%)]\tTrain Loss: 2.895530\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 4 [4800/5639(85%)]\tTrain Loss: 2.334128\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-27-49\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 5\n",
            "  loss: 2.8023836271137212\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 19.649302005767822\n",
            "  time_this_iter_s: 3.29160475730896\n",
            "  time_total_s: 19.649302005767822\n",
            "  timestamp: 1637306869\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 4], \tVal Loss: 2.8024\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 2.815789\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 5 [1600/5639(28%)]\tTrain Loss: 2.653523\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 5 [3200/5639(57%)]\tTrain Loss: 2.587272\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:52 (running for 00:01:44.58)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.80238 |                    5 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 5 [4800/5639(85%)]\tTrain Loss: 2.832529\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 5], \tVal Loss: 2.6015\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 2.205298\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 6 [1600/5639(28%)]\tTrain Loss: 2.964437\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 6 [3200/5639(57%)]\tTrain Loss: 2.036421\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 6 [4800/5639(85%)]\tTrain Loss: 3.260645\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-27-56\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 7\n",
            "  loss: 2.6465444943583605\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 26.185920238494873\n",
            "  time_this_iter_s: 3.3020341396331787\n",
            "  time_total_s: 26.185920238494873\n",
            "  timestamp: 1637306876\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 6], \tVal Loss: 2.6465\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 3.087341\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 7 [1600/5639(28%)]\tTrain Loss: 2.915090\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:27:57 (running for 00:01:50.09)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.5243866426724915 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.64654 |                    7 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 7 [3200/5639(57%)]\tTrain Loss: 2.466542\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 7 [4800/5639(85%)]\tTrain Loss: 2.639344\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 7], \tVal Loss: 2.6409\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 8 [0/5639(0%)]\tTrain Loss: 2.822869\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 8 [1600/5639(28%)]\tTrain Loss: 2.103797\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 8 [3200/5639(57%)]\tTrain Loss: 3.239151\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 8 [4800/5639(85%)]\tTrain Loss: 2.701752\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:02 (running for 00:01:55.49)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.64092 |                    8 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-28-03\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 9\n",
            "  loss: 2.542677762998757\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 32.86536359786987\n",
            "  time_this_iter_s: 3.305349349975586\n",
            "  time_total_s: 32.86536359786987\n",
            "  timestamp: 1637306883\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 8], \tVal Loss: 2.5427\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 9 [0/5639(0%)]\tTrain Loss: 1.847975\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 9 [1600/5639(28%)]\tTrain Loss: 3.085095\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 9 [3200/5639(57%)]\tTrain Loss: 2.291980\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 9 [4800/5639(85%)]\tTrain Loss: 2.313155\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 9], \tVal Loss: 2.6642\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 10 [0/5639(0%)]\tTrain Loss: 4.256581\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 10 [1600/5639(28%)]\tTrain Loss: 2.596805\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 10 [3200/5639(57%)]\tTrain Loss: 1.999507\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:08 (running for 00:02:01.37)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.66423 |                   10 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 10 [4800/5639(85%)]\tTrain Loss: 2.357062\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-28-10\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 11\n",
            "  loss: 2.416733492858021\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 39.91268014907837\n",
            "  time_this_iter_s: 3.442737340927124\n",
            "  time_total_s: 39.91268014907837\n",
            "  timestamp: 1637306890\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 11\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 10], \tVal Loss: 2.4167\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 11 [0/5639(0%)]\tTrain Loss: 4.332008\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 11 [1600/5639(28%)]\tTrain Loss: 3.088845\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 11 [3200/5639(57%)]\tTrain Loss: 2.009331\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 11 [4800/5639(85%)]\tTrain Loss: 2.451396\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 11], \tVal Loss: 2.4362\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 12 [0/5639(0%)]\tTrain Loss: 2.107911\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 12 [1600/5639(28%)]\tTrain Loss: 2.650994\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:14 (running for 00:02:07.30)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.43618 |                   12 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 12 [3200/5639(57%)]\tTrain Loss: 2.407109\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 12 [4800/5639(85%)]\tTrain Loss: 1.921925\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-28-17\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 13\n",
            "  loss: 2.418408135488524\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 46.85832953453064\n",
            "  time_this_iter_s: 3.4594037532806396\n",
            "  time_total_s: 46.85832953453064\n",
            "  timestamp: 1637306897\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 13\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 12], \tVal Loss: 2.4184\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 13 [0/5639(0%)]\tTrain Loss: 1.762933\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 13 [1600/5639(28%)]\tTrain Loss: 1.959361\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 13 [3200/5639(57%)]\tTrain Loss: 2.468549\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 13 [4800/5639(85%)]\tTrain Loss: 2.500199\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:20 (running for 00:02:12.77)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.41841 |                   13 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 13], \tVal Loss: 2.3617\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 14 [0/5639(0%)]\tTrain Loss: 2.119889\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 14 [1600/5639(28%)]\tTrain Loss: 3.017179\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 14 [3200/5639(57%)]\tTrain Loss: 1.677840\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 14 [4800/5639(85%)]\tTrain Loss: 2.783741\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-28-24\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 15\n",
            "  loss: 2.3666195429808705\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 53.70745134353638\n",
            "  time_this_iter_s: 3.4614017009735107\n",
            "  time_total_s: 53.70745134353638\n",
            "  timestamp: 1637306904\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 15\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 14], \tVal Loss: 2.3666\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 15 [0/5639(0%)]\tTrain Loss: 2.279551\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 15 [1600/5639(28%)]\tTrain Loss: 1.730942\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 15 [3200/5639(57%)]\tTrain Loss: 1.556026\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:26 (running for 00:02:18.63)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -3.0913814233549943 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.36662 |                   15 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 15 [4800/5639(85%)]\tTrain Loss: 2.243459\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 15], \tVal Loss: 2.3417\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 16 [0/5639(0%)]\tTrain Loss: 1.825757\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 16 [1600/5639(28%)]\tTrain Loss: 1.878495\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 16 [3200/5639(57%)]\tTrain Loss: 1.799996\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 16 [4800/5639(85%)]\tTrain Loss: 2.595134\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-28-30\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 17\n",
            "  loss: 2.327890830344342\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 60.53204107284546\n",
            "  time_this_iter_s: 3.3822693824768066\n",
            "  time_total_s: 60.53204107284546\n",
            "  timestamp: 1637306910\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 17\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 16], \tVal Loss: 2.3279\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 17 [0/5639(0%)]\tTrain Loss: 1.789781\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 17 [1600/5639(28%)]\tTrain Loss: 1.631458\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:31 (running for 00:02:24.44)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.32789 |                   17 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 17 [3200/5639(57%)]\tTrain Loss: 2.256208\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 17 [4800/5639(85%)]\tTrain Loss: 2.646373\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 17], \tVal Loss: 2.3548\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 18 [0/5639(0%)]\tTrain Loss: 3.721719\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 18 [1600/5639(28%)]\tTrain Loss: 3.153993\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 18 [3200/5639(57%)]\tTrain Loss: 2.177885\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 18 [4800/5639(85%)]\tTrain Loss: 2.071745\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:37 (running for 00:02:29.78)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (7 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00002 | RUNNING    | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.35476 |                   18 |\n",
            "| tune_with_parameters_f6201_00003 | PENDING    |                 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-28-37\n",
            "  done: false\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 19\n",
            "  loss: 2.2722508829536165\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 67.21113634109497\n",
            "  time_this_iter_s: 3.3401389122009277\n",
            "  time_total_s: 67.21113634109497\n",
            "  timestamp: 1637306917\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 19\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 18], \tVal Loss: 2.2723\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 19 [0/5639(0%)]\tTrain Loss: 2.224791\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 19 [1600/5639(28%)]\tTrain Loss: 1.791566\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 19 [3200/5639(57%)]\tTrain Loss: 2.657329\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m Train Epoch: 19 [4800/5639(85%)]\tTrain Loss: 1.995580\n",
            "Result for tune_with_parameters_f6201_00002:\n",
            "  date: 2021-11-19_07-28-40\n",
            "  done: true\n",
            "  experiment_id: 323bc289942844ac8aaab5fa017ff804\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 20\n",
            "  loss: 2.3369139177579408\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2068\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 70.66000270843506\n",
            "  time_this_iter_s: 3.448866367340088\n",
            "  time_total_s: 70.66000270843506\n",
            "  timestamp: 1637306920\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 20\n",
            "  trial_id: f6201_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m [EPOCH: 19], \tVal Loss: 2.3369\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2068)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:43 (running for 00:02:35.61)\n",
            "Memory usage on this node: 4.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00003 | RUNNING    | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 |         |                      |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m   (fc1): Linear(in_features=4608, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m   (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m   (fc3): Linear(in_features=8, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2514)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=2514)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.874329\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 0 [3200/5639(56%)]\tTrain Loss: 2.967177\n",
            "Result for tune_with_parameters_f6201_00003:\n",
            "  date: 2021-11-19_07-28-48\n",
            "  done: false\n",
            "  experiment_id: 70e3cd2e9c364141ae66d084bfbaf2c0\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.197047765204247\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2514\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 5.032159328460693\n",
            "  time_this_iter_s: 5.032159328460693\n",
            "  time_total_s: 5.032159328460693\n",
            "  timestamp: 1637306928\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00003\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:48 (running for 00:02:41.28)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.423713470350766 | Iter 2.000: -4.2809074794147035 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00003 | RUNNING    | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.19705 |                    1 |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 0], \tVal Loss: 3.1970\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 3.352290\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 1 [3200/5639(56%)]\tTrain Loss: 3.522232\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 1], \tVal Loss: 3.3767\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 3.010441\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 2 [3200/5639(56%)]\tTrain Loss: 2.534558\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 2], \tVal Loss: 3.6459\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 3.856362\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 3 [3200/5639(56%)]\tTrain Loss: 3.511140\n",
            "Result for tune_with_parameters_f6201_00003:\n",
            "  date: 2021-11-19_07-28-54\n",
            "  done: false\n",
            "  experiment_id: 70e3cd2e9c364141ae66d084bfbaf2c0\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 4\n",
            "  loss: 3.256529013992201\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2514\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 10.96247911453247\n",
            "  time_this_iter_s: 1.9961788654327393\n",
            "  time_total_s: 10.96247911453247\n",
            "  timestamp: 1637306934\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: f6201_00003\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:28:54 (running for 00:02:47.21)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.531091008287795 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00003 | RUNNING    | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.25653 |                    4 |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 3], \tVal Loss: 3.2565\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 2.942490\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 4 [3200/5639(56%)]\tTrain Loss: 3.749843\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 4], \tVal Loss: 3.1654\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 2.729842\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 5 [3200/5639(56%)]\tTrain Loss: 2.577350\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 5], \tVal Loss: 3.1890\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 2.789782\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 6 [3200/5639(56%)]\tTrain Loss: 2.707125\n",
            "Result for tune_with_parameters_f6201_00003:\n",
            "  date: 2021-11-19_07-29-00\n",
            "  done: false\n",
            "  experiment_id: 70e3cd2e9c364141ae66d084bfbaf2c0\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 7\n",
            "  loss: 3.5843289152104805\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2514\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 16.89132595062256\n",
            "  time_this_iter_s: 1.9466209411621094\n",
            "  time_total_s: 16.89132595062256\n",
            "  timestamp: 1637306940\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: f6201_00003\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:00 (running for 00:02:53.14)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.08265448022396 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.531091008287795 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (6 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00003 | RUNNING    | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.58433 |                    7 |\n",
            "| tune_with_parameters_f6201_00004 | PENDING    |                 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 6], \tVal Loss: 3.5843\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 3.102646\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m Train Epoch: 7 [3200/5639(56%)]\tTrain Loss: 2.614424\n",
            "Result for tune_with_parameters_f6201_00003:\n",
            "  date: 2021-11-19_07-29-02\n",
            "  done: true\n",
            "  experiment_id: 70e3cd2e9c364141ae66d084bfbaf2c0\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 8\n",
            "  loss: 3.0945989838728667\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2514\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 18.796777486801147\n",
            "  time_this_iter_s: 1.9054515361785889\n",
            "  time_total_s: 18.796777486801147\n",
            "  timestamp: 1637306942\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 8\n",
            "  trial_id: f6201_00003\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m [EPOCH: 7], \tVal Loss: 3.0946\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2514)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:06 (running for 00:02:59.10)\n",
            "Memory usage on this node: 4.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.531091008287795 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (5 PENDING, 1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00004 | RUNNING    | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 |         |                      |\n",
            "| tune_with_parameters_f6201_00005 | PENDING    |                 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m   (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m   (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2710)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=2710)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 52.096420\n",
            "Result for tune_with_parameters_f6201_00004:\n",
            "  date: 2021-11-19_07-29-09\n",
            "  done: true\n",
            "  experiment_id: 59b5200faad44a30bcbcd623ede66f70\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.7278969730891234\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2710\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.112268447875977\n",
            "  time_this_iter_s: 4.112268447875977\n",
            "  time_total_s: 4.112268447875977\n",
            "  timestamp: 1637306949\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00004\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m [EPOCH: 0], \tVal Loss: 3.7279\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2710)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:12 (running for 00:03:04.75)\n",
            "Memory usage on this node: 4.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.531091008287795 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m   (fc1): Linear(in_features=4608, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m   (fc2): Linear(in_features=8, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2762)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=2762)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.869934\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [800/5639(14%)]\tTrain Loss: 3.480902\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 3.243412\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [2400/5639(43%)]\tTrain Loss: 2.916823\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:17 (running for 00:03:09.81)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.531091008287795 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 |         |                      |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 3.990840\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [4000/5639(71%)]\tTrain Loss: 2.835181\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 2.675276\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 0 [5600/5639(99%)]\tTrain Loss: 3.001434\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-29-20\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.082259675289722\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 9.080711364746094\n",
            "  time_this_iter_s: 9.080711364746094\n",
            "  time_total_s: 9.080711364746094\n",
            "  timestamp: 1637306960\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 0], \tVal Loss: 3.0823\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 2.701361\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [800/5639(14%)]\tTrain Loss: 2.930972\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 3.605263\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:22 (running for 00:03:15.40)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.531091008287795 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 3.08226 |                    1 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [2400/5639(43%)]\tTrain Loss: 2.681511\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 2.540577\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [4000/5639(71%)]\tTrain Loss: 3.003511\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 2.585923\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 1 [5600/5639(99%)]\tTrain Loss: 1.828955\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-29-26\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 2\n",
            "  loss: 3.023792183652837\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 15.040722608566284\n",
            "  time_this_iter_s: 5.96001124382019\n",
            "  time_total_s: 15.040722608566284\n",
            "  timestamp: 1637306966\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 1], \tVal Loss: 3.0238\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 3.024829\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [800/5639(14%)]\tTrain Loss: 2.080414\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 4.122966\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:28 (running for 00:03:21.38)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 3.02379 |                    2 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [2400/5639(43%)]\tTrain Loss: 3.939426\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 2.293804\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [4000/5639(71%)]\tTrain Loss: 3.039744\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 3.719656\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 2 [5600/5639(99%)]\tTrain Loss: 2.967771\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-29-32\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 3\n",
            "  loss: 2.965316722409945\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 21.096403121948242\n",
            "  time_this_iter_s: 6.055680513381958\n",
            "  time_total_s: 21.096403121948242\n",
            "  timestamp: 1637306972\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 2], \tVal Loss: 2.9653\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 3.109684\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [800/5639(14%)]\tTrain Loss: 3.652914\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:33 (running for 00:03:26.40)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.256529013992201 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.96532 |                    3 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 2.391973\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [2400/5639(43%)]\tTrain Loss: 2.750928\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 3.287045\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [4000/5639(71%)]\tTrain Loss: 3.769295\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 3.300197\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 3 [5600/5639(99%)]\tTrain Loss: 2.494409\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-29-38\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 4\n",
            "  loss: 2.94318859154451\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 27.11967134475708\n",
            "  time_this_iter_s: 6.023268222808838\n",
            "  time_total_s: 27.11967134475708\n",
            "  timestamp: 1637306978\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:38 (running for 00:03:31.42)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.94319 |                    4 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 3], \tVal Loss: 2.9432\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 3.107425\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [800/5639(14%)]\tTrain Loss: 3.347402\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [1600/5639(28%)]\tTrain Loss: 2.702343\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [2400/5639(43%)]\tTrain Loss: 2.959075\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [3200/5639(57%)]\tTrain Loss: 2.038989\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [4000/5639(71%)]\tTrain Loss: 3.565384\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [4800/5639(85%)]\tTrain Loss: 2.722476\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:43 (running for 00:03:36.45)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.94319 |                    4 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 4 [5600/5639(99%)]\tTrain Loss: 1.915844\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-29-44\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 5\n",
            "  loss: 2.906327498550956\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 33.164074420928955\n",
            "  time_this_iter_s: 6.044403076171875\n",
            "  time_total_s: 33.164074420928955\n",
            "  timestamp: 1637306984\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 4], \tVal Loss: 2.9063\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 3.357085\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [800/5639(14%)]\tTrain Loss: 3.743314\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [1600/5639(28%)]\tTrain Loss: 3.343271\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [2400/5639(43%)]\tTrain Loss: 2.382551\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [3200/5639(57%)]\tTrain Loss: 4.018915\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [4000/5639(71%)]\tTrain Loss: 2.770684\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:48 (running for 00:03:41.50)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.90633 |                    5 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [4800/5639(85%)]\tTrain Loss: 2.694416\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 5 [5600/5639(99%)]\tTrain Loss: 2.484322\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-29-51\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 6\n",
            "  loss: 2.9078259089314344\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 39.40137839317322\n",
            "  time_this_iter_s: 6.237303972244263\n",
            "  time_total_s: 39.40137839317322\n",
            "  timestamp: 1637306991\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 6\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 5], \tVal Loss: 2.9078\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 3.362571\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [800/5639(14%)]\tTrain Loss: 2.940712\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [1600/5639(28%)]\tTrain Loss: 3.106319\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [2400/5639(43%)]\tTrain Loss: 1.939106\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:54 (running for 00:03:46.72)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.90783 |                    6 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [3200/5639(57%)]\tTrain Loss: 2.918238\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [4000/5639(71%)]\tTrain Loss: 2.960852\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [4800/5639(85%)]\tTrain Loss: 2.368296\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 6 [5600/5639(99%)]\tTrain Loss: 2.870211\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-29-57\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 7\n",
            "  loss: 3.100148430276424\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 45.516392946243286\n",
            "  time_this_iter_s: 6.115014553070068\n",
            "  time_total_s: 45.516392946243286\n",
            "  timestamp: 1637306997\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 6], \tVal Loss: 3.1001\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 3.023061\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [800/5639(14%)]\tTrain Loss: 2.371356\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [1600/5639(28%)]\tTrain Loss: 2.212445\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:29:59 (running for 00:03:51.86)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.0945989838728667 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 3.10015 |                    7 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [2400/5639(43%)]\tTrain Loss: 2.377928\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [3200/5639(57%)]\tTrain Loss: 3.757741\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [4000/5639(71%)]\tTrain Loss: 2.749044\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [4800/5639(85%)]\tTrain Loss: 3.328828\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 7 [5600/5639(99%)]\tTrain Loss: 3.585219\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-03\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 8\n",
            "  loss: 3.028511691262536\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 51.530393838882446\n",
            "  time_this_iter_s: 6.01400089263916\n",
            "  time_total_s: 51.530393838882446\n",
            "  timestamp: 1637307003\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 8\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 7], \tVal Loss: 3.0285\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [0/5639(0%)]\tTrain Loss: 3.000899\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [800/5639(14%)]\tTrain Loss: 2.727394\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [1600/5639(28%)]\tTrain Loss: 3.044924\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:05 (running for 00:03:57.85)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 3.02851 |                    8 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [2400/5639(43%)]\tTrain Loss: 2.536423\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [3200/5639(57%)]\tTrain Loss: 2.261090\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [4000/5639(71%)]\tTrain Loss: 2.492384\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [4800/5639(85%)]\tTrain Loss: 3.236546\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 8 [5600/5639(99%)]\tTrain Loss: 2.690011\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-09\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 9\n",
            "  loss: 2.888510906949956\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 57.810683250427246\n",
            "  time_this_iter_s: 6.2802894115448\n",
            "  time_total_s: 57.810683250427246\n",
            "  timestamp: 1637307009\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 8], \tVal Loss: 2.8885\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [0/5639(0%)]\tTrain Loss: 4.183070\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [800/5639(14%)]\tTrain Loss: 2.850497\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:10 (running for 00:04:03.12)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.88851 |                    9 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [1600/5639(28%)]\tTrain Loss: 3.170290\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [2400/5639(43%)]\tTrain Loss: 3.474578\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [3200/5639(57%)]\tTrain Loss: 3.168619\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [4000/5639(71%)]\tTrain Loss: 2.682279\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [4800/5639(85%)]\tTrain Loss: 2.487830\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 9 [5600/5639(99%)]\tTrain Loss: 2.715092\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:15 (running for 00:04:08.15)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.88851 |                    9 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-15\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 10\n",
            "  loss: 2.878352723223098\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 63.95540475845337\n",
            "  time_this_iter_s: 6.144721508026123\n",
            "  time_total_s: 63.95540475845337\n",
            "  timestamp: 1637307015\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 10\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 9], \tVal Loss: 2.8784\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [0/5639(0%)]\tTrain Loss: 2.474751\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [800/5639(14%)]\tTrain Loss: 3.388659\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [1600/5639(28%)]\tTrain Loss: 2.697330\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [2400/5639(43%)]\tTrain Loss: 3.650009\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [3200/5639(57%)]\tTrain Loss: 3.501767\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [4000/5639(71%)]\tTrain Loss: 2.218490\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [4800/5639(85%)]\tTrain Loss: 2.855489\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:20 (running for 00:04:13.30)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.87835 |                   10 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 10 [5600/5639(99%)]\tTrain Loss: 2.510030\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-21\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 11\n",
            "  loss: 2.8775380114291575\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 70.11401438713074\n",
            "  time_this_iter_s: 6.158609628677368\n",
            "  time_total_s: 70.11401438713074\n",
            "  timestamp: 1637307021\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 11\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 10], \tVal Loss: 2.8775\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [0/5639(0%)]\tTrain Loss: 2.810525\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [800/5639(14%)]\tTrain Loss: 2.725283\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [1600/5639(28%)]\tTrain Loss: 3.269686\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [2400/5639(43%)]\tTrain Loss: 2.333444\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [3200/5639(57%)]\tTrain Loss: 1.969867\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [4000/5639(71%)]\tTrain Loss: 2.632232\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:25 (running for 00:04:18.43)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.87754 |                   11 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [4800/5639(85%)]\tTrain Loss: 2.803525\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 11 [5600/5639(99%)]\tTrain Loss: 2.622780\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-28\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 12\n",
            "  loss: 2.871595933251347\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 76.41680145263672\n",
            "  time_this_iter_s: 6.3027870655059814\n",
            "  time_total_s: 76.41680145263672\n",
            "  timestamp: 1637307028\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 12\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 11], \tVal Loss: 2.8716\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [0/5639(0%)]\tTrain Loss: 2.936957\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [800/5639(14%)]\tTrain Loss: 2.461146\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [1600/5639(28%)]\tTrain Loss: 2.365813\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [2400/5639(43%)]\tTrain Loss: 2.965658\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:31 (running for 00:04:23.76)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.8716  |                   12 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [3200/5639(57%)]\tTrain Loss: 2.303470\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [4000/5639(71%)]\tTrain Loss: 2.703745\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [4800/5639(85%)]\tTrain Loss: 3.623482\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 12 [5600/5639(99%)]\tTrain Loss: 1.788208\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-34\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 13\n",
            "  loss: 2.935671215869011\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 82.43046498298645\n",
            "  time_this_iter_s: 6.0136635303497314\n",
            "  time_total_s: 82.43046498298645\n",
            "  timestamp: 1637307034\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 13\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 12], \tVal Loss: 2.9357\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [0/5639(0%)]\tTrain Loss: 2.142721\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [800/5639(14%)]\tTrain Loss: 7.267066\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [1600/5639(28%)]\tTrain Loss: 10.015125\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [2400/5639(43%)]\tTrain Loss: 2.532926\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:37 (running for 00:04:29.74)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.93567 |                   13 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [3200/5639(57%)]\tTrain Loss: 3.698032\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [4000/5639(71%)]\tTrain Loss: 4.261452\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [4800/5639(85%)]\tTrain Loss: 2.494790\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 13 [5600/5639(99%)]\tTrain Loss: 2.442649\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-40\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 14\n",
            "  loss: 2.8403418710045782\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 88.58951163291931\n",
            "  time_this_iter_s: 6.159046649932861\n",
            "  time_total_s: 88.58951163291931\n",
            "  timestamp: 1637307040\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 14\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 13], \tVal Loss: 2.8403\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [0/5639(0%)]\tTrain Loss: 2.035173\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [800/5639(14%)]\tTrain Loss: 2.111403\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [1600/5639(28%)]\tTrain Loss: 2.206631\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:42 (running for 00:04:34.91)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.84034 |                   14 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [2400/5639(43%)]\tTrain Loss: 3.171075\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [3200/5639(57%)]\tTrain Loss: 2.856065\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [4000/5639(71%)]\tTrain Loss: 3.428511\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [4800/5639(85%)]\tTrain Loss: 2.683700\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 14 [5600/5639(99%)]\tTrain Loss: 3.533177\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-46\n",
            "  done: false\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 15\n",
            "  loss: 3.07453617677621\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 94.58719539642334\n",
            "  time_this_iter_s: 5.997683763504028\n",
            "  time_total_s: 94.58719539642334\n",
            "  timestamp: 1637307046\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 15\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 14], \tVal Loss: 3.0745\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [0/5639(0%)]\tTrain Loss: 2.806899\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [800/5639(14%)]\tTrain Loss: 3.056712\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [1600/5639(28%)]\tTrain Loss: 2.812172\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:48 (running for 00:04:40.90)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 16.000: -2.716549545991505 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (4 PENDING, 1 RUNNING, 5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00005 | RUNNING    | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 3.07454 |                   15 |\n",
            "| tune_with_parameters_f6201_00006 | PENDING    |                 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [2400/5639(43%)]\tTrain Loss: 3.473338\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [3200/5639(57%)]\tTrain Loss: 2.654402\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [4000/5639(71%)]\tTrain Loss: 3.093506\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [4800/5639(85%)]\tTrain Loss: 2.655641\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m Train Epoch: 15 [5600/5639(99%)]\tTrain Loss: 2.237588\n",
            "Result for tune_with_parameters_f6201_00005:\n",
            "  date: 2021-11-19_07-30-52\n",
            "  done: true\n",
            "  experiment_id: 63c77845b8b046d08c237e04b8f12b96\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 16\n",
            "  loss: 2.835878148315646\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2762\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 100.59777021408081\n",
            "  time_this_iter_s: 6.010574817657471\n",
            "  time_total_s: 100.59777021408081\n",
            "  timestamp: 1637307052\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 16\n",
            "  trial_id: f6201_00005\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m [EPOCH: 15], \tVal Loss: 2.8359\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=2762)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:53 (running for 00:04:45.93)\n",
            "Memory usage on this node: 3.9/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00006 | RUNNING    | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m   (fc1): Linear(in_features=4608, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m   (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m   (fc3): Linear(in_features=16, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=3126)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=3126)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.921406\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:30:58 (running for 00:04:51.48)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=6\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (3 PENDING, 1 RUNNING, 6 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00006 | RUNNING    | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 |         |                      |\n",
            "| tune_with_parameters_f6201_00007 | PENDING    |                 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m [EPOCH: 0], \tVal Loss: 3.4439\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3126)\u001b[0m \n",
            "Result for tune_with_parameters_f6201_00006:\n",
            "  date: 2021-11-19_07-30-59\n",
            "  done: true\n",
            "  experiment_id: 259932e5c2164a80893ed152e65d8750\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.4438554912594195\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3126\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.27872896194458\n",
            "  time_this_iter_s: 4.27872896194458\n",
            "  time_total_s: 4.27872896194458\n",
            "  timestamp: 1637307059\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00006\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:04 (running for 00:04:56.83)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=7\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (2 PENDING, 1 RUNNING, 7 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00007 | RUNNING    | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  |         |                      |\n",
            "| tune_with_parameters_f6201_00008 | PENDING    |                 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m   (fc1): Linear(in_features=4608, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m   (fc2): Linear(in_features=8, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m   (fc3): Linear(in_features=32, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=3178)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=3178)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.200623\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 3.315352\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 3.259130\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 3.044051\n",
            "Result for tune_with_parameters_f6201_00007:\n",
            "  date: 2021-11-19_07-31-07\n",
            "  done: true\n",
            "  experiment_id: 3dbfc4b7765047fba9e338dcf79265cc\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.348859289182839\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3178\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 6.172968626022339\n",
            "  time_this_iter_s: 6.172968626022339\n",
            "  time_total_s: 6.172968626022339\n",
            "  timestamp: 1637307067\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00007\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m [EPOCH: 0], \tVal Loss: 3.3489\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3178)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[1m\u001b[33m(scheduler +19m29s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:09 (running for 00:05:02.53)\n",
            "Memory usage on this node: 4.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.3070735092704178\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00008 | RUNNING    | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 |         |                      |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m   (fc1): Linear(in_features=4608, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m   (fc2): Linear(in_features=32, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m   (fc3): Linear(in_features=8, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=3230)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=3230)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.524769\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 0 [3200/5639(56%)]\tTrain Loss: 3.304287\n",
            "Result for tune_with_parameters_f6201_00008:\n",
            "  date: 2021-11-19_07-31-15\n",
            "  done: false\n",
            "  experiment_id: c6f5f9585bb44832b2ab6e43a1240150\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.1073670137013103\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3230\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.886317253112793\n",
            "  time_this_iter_s: 4.886317253112793\n",
            "  time_total_s: 4.886317253112793\n",
            "  timestamp: 1637307075\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00008\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:15 (running for 00:05:07.93)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=8\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.10435633828454 | Iter 2.000: -3.4538943906202384 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00008 | RUNNING    | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.10737 |                    1 |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m [EPOCH: 0], \tVal Loss: 3.1074\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 4.036116\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 1 [3200/5639(56%)]\tTrain Loss: 2.795337\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m [EPOCH: 1], \tVal Loss: 3.0223\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 3.103366\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 2 [3200/5639(56%)]\tTrain Loss: 2.604791\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m [EPOCH: 2], \tVal Loss: 2.9956\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 2.738809\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m Train Epoch: 3 [3200/5639(56%)]\tTrain Loss: 2.966669\n",
            "Result for tune_with_parameters_f6201_00008:\n",
            "  date: 2021-11-19_07-31-21\n",
            "  done: true\n",
            "  experiment_id: c6f5f9585bb44832b2ab6e43a1240150\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 4\n",
            "  loss: 3.1204218276003575\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3230\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 10.639582872390747\n",
            "  time_this_iter_s: 1.9058406352996826\n",
            "  time_total_s: 10.639582872390747\n",
            "  timestamp: 1637307081\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: f6201_00008\n",
            "  \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:21 (running for 00:05:13.68)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.1204218276003575 | Iter 2.000: -3.376697772952682 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 PENDING, 1 RUNNING, 8 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00008 | RUNNING    | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "| tune_with_parameters_f6201_00009 | PENDING    |                 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m [EPOCH: 3], \tVal Loss: 3.1204\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3230)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:26 (running for 00:05:18.72)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.1204218276003575 | Iter 2.000: -3.376697772952682 | Iter 1.000: -3.265287729357997\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 |         |                      |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m   (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m   (fc2): Linear(in_features=128, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m   (fc3): Linear(in_features=16, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=3342)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=3342)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.851734\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 3.470809\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 3.304157\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 2.886860\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-31-30\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 1\n",
            "  loss: 2.8633132664024408\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 6.38401985168457\n",
            "  time_this_iter_s: 6.38401985168457\n",
            "  time_total_s: 6.38401985168457\n",
            "  timestamp: 1637307090\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 0], \tVal Loss: 2.8633\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 2.662274\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 2.787992\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 2.839904\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:32 (running for 00:05:24.65)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.1204218276003575 | Iter 2.000: -3.376697772952682 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.86331 |                    1 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 2.458180\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 1], \tVal Loss: 2.9310\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 2.424362\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 2.826378\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 2.500540\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 2.472299\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-31-36\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.0464750330498878\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 13.12175440788269\n",
            "  time_this_iter_s: 3.3234598636627197\n",
            "  time_total_s: 13.12175440788269\n",
            "  timestamp: 1637307096\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 2], \tVal Loss: 3.0465\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 2.975856\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 3.382926\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:37 (running for 00:05:30.39)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.1204218276003575 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 3.04648 |                    3 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 2.837307\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 3.041358\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 3], \tVal Loss: 2.8791\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 2.815500\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 4 [1600/5639(28%)]\tTrain Loss: 5.264989\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 4 [3200/5639(57%)]\tTrain Loss: 2.752557\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 4 [4800/5639(85%)]\tTrain Loss: 3.163856\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:43 (running for 00:05:35.84)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.87912 |                    4 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-31-43\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 5\n",
            "  loss: 3.3373044710632755\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 19.939287662506104\n",
            "  time_this_iter_s: 3.386653423309326\n",
            "  time_total_s: 19.939287662506104\n",
            "  timestamp: 1637307103\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 4], \tVal Loss: 3.3373\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 2.940782\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 5 [1600/5639(28%)]\tTrain Loss: 2.034468\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 5 [3200/5639(57%)]\tTrain Loss: 2.330061\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 5 [4800/5639(85%)]\tTrain Loss: 2.336867\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 5], \tVal Loss: 2.6287\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 2.205724\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 6 [1600/5639(28%)]\tTrain Loss: 2.403028\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 6 [3200/5639(57%)]\tTrain Loss: 2.178774\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:49 (running for 00:05:41.56)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.061555337567701 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.62874 |                    6 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 6 [4800/5639(85%)]\tTrain Loss: 2.865383\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-31-50\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 7\n",
            "  loss: 2.6110510886983667\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 26.75685214996338\n",
            "  time_this_iter_s: 3.4581732749938965\n",
            "  time_total_s: 26.75685214996338\n",
            "  timestamp: 1637307110\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 6], \tVal Loss: 2.6111\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 2.441308\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 7 [1600/5639(28%)]\tTrain Loss: 2.901646\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 7 [3200/5639(57%)]\tTrain Loss: 2.865359\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 7 [4800/5639(85%)]\tTrain Loss: 2.491713\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 7], \tVal Loss: 2.9210\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 8 [0/5639(0%)]\tTrain Loss: 2.309008\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 8 [1600/5639(28%)]\tTrain Loss: 2.671338\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:31:54 (running for 00:05:47.36)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.92103 |                    8 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 8 [3200/5639(57%)]\tTrain Loss: 2.275835\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 8 [4800/5639(85%)]\tTrain Loss: 5.926147\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 8], \tVal Loss: 2.5022\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-31-57\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 9\n",
            "  loss: 2.5022226523000297\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 33.45851469039917\n",
            "  time_this_iter_s: 3.3498449325561523\n",
            "  time_total_s: 33.45851469039917\n",
            "  timestamp: 1637307117\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 9 [0/5639(0%)]\tTrain Loss: 2.651797\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 9 [1600/5639(28%)]\tTrain Loss: 2.666577\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 9 [3200/5639(57%)]\tTrain Loss: 2.459049\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 9 [4800/5639(85%)]\tTrain Loss: 2.959927\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:00 (running for 00:05:52.72)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.50222 |                    9 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 9], \tVal Loss: 2.7380\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 10 [0/5639(0%)]\tTrain Loss: 2.374722\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 10 [1600/5639(28%)]\tTrain Loss: 2.660224\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 10 [3200/5639(57%)]\tTrain Loss: 2.002949\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 10 [4800/5639(85%)]\tTrain Loss: 3.560543\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-32-03\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 11\n",
            "  loss: 2.4371333480726745\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 40.1856427192688\n",
            "  time_this_iter_s: 3.280015468597412\n",
            "  time_total_s: 40.1856427192688\n",
            "  timestamp: 1637307123\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 11\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 10], \tVal Loss: 2.4371\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 11 [0/5639(0%)]\tTrain Loss: 2.609586\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 11 [1600/5639(28%)]\tTrain Loss: 1.946514\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 11 [3200/5639(57%)]\tTrain Loss: 2.270899\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:05 (running for 00:05:58.45)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.43713 |                   11 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 11 [4800/5639(85%)]\tTrain Loss: 2.254271\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 11], \tVal Loss: 2.5926\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 12 [0/5639(0%)]\tTrain Loss: 2.257838\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 12 [1600/5639(28%)]\tTrain Loss: 2.417403\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 12 [3200/5639(57%)]\tTrain Loss: 2.793406\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 12 [4800/5639(85%)]\tTrain Loss: 2.102180\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-32-10\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 13\n",
            "  loss: 2.494646415980995\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 46.776859283447266\n",
            "  time_this_iter_s: 3.329948902130127\n",
            "  time_total_s: 46.776859283447266\n",
            "  timestamp: 1637307130\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 13\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 12], \tVal Loss: 2.4946\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 13 [0/5639(0%)]\tTrain Loss: 3.167937\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 13 [1600/5639(28%)]\tTrain Loss: 2.235950\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:11 (running for 00:06:04.04)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.49465 |                   13 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 13 [3200/5639(57%)]\tTrain Loss: 1.791136\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 13 [4800/5639(85%)]\tTrain Loss: 2.164150\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 13], \tVal Loss: 2.2659\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 14 [0/5639(0%)]\tTrain Loss: 1.969646\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 14 [1600/5639(28%)]\tTrain Loss: 2.322960\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 14 [3200/5639(57%)]\tTrain Loss: 2.330315\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 14 [4800/5639(85%)]\tTrain Loss: 2.478171\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:16 (running for 00:06:09.35)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.835878148315646 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.26595 |                   14 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-32-17\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 15\n",
            "  loss: 2.8164917533279312\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 53.43550729751587\n",
            "  time_this_iter_s: 3.360168933868408\n",
            "  time_total_s: 53.43550729751587\n",
            "  timestamp: 1637307137\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 15\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 14], \tVal Loss: 2.8165\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 15 [0/5639(0%)]\tTrain Loss: 2.378314\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 15 [1600/5639(28%)]\tTrain Loss: 2.473670\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 15 [3200/5639(57%)]\tTrain Loss: 2.247166\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 15 [4800/5639(85%)]\tTrain Loss: 1.686901\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 15], \tVal Loss: 2.3471\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 16 [0/5639(0%)]\tTrain Loss: 1.970069\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 16 [1600/5639(28%)]\tTrain Loss: 2.388715\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 16 [3200/5639(57%)]\tTrain Loss: 2.097763\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:22 (running for 00:06:15.08)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.5914811218883975 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.34708 |                   16 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 16 [4800/5639(85%)]\tTrain Loss: 2.388179\n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-32-23\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 17\n",
            "  loss: 2.309898556377871\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 60.28043270111084\n",
            "  time_this_iter_s: 3.4567818641662598\n",
            "  time_total_s: 60.28043270111084\n",
            "  timestamp: 1637307143\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 17\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 16], \tVal Loss: 2.3099\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 17 [0/5639(0%)]\tTrain Loss: 2.226730\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 17 [1600/5639(28%)]\tTrain Loss: 1.823331\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 17 [3200/5639(57%)]\tTrain Loss: 2.085522\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 17 [4800/5639(85%)]\tTrain Loss: 2.853176\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 17], \tVal Loss: 2.4068\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:27 (running for 00:06:20.09)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.5914811218883975 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.40679 |                   18 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 18 [0/5639(0%)]\tTrain Loss: 3.872400\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 18 [1600/5639(28%)]\tTrain Loss: 1.550693\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 18 [3200/5639(57%)]\tTrain Loss: 1.672859\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 18 [4800/5639(85%)]\tTrain Loss: 2.193985\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 18], \tVal Loss: 2.2698\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-32-30\n",
            "  done: false\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 19\n",
            "  loss: 2.2697904667955764\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 67.2320306301117\n",
            "  time_this_iter_s: 3.403493642807007\n",
            "  time_total_s: 67.2320306301117\n",
            "  timestamp: 1637307150\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 19\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 19 [0/5639(0%)]\tTrain Loss: 1.652256\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 19 [1600/5639(28%)]\tTrain Loss: 2.234102\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 19 [3200/5639(57%)]\tTrain Loss: 1.716374\n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:32 (running for 00:06:25.50)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=9\n",
            "Bracket: Iter 16.000: -2.5914811218883975 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00009 | RUNNING    | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.26979 |                   19 |\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m Train Epoch: 19 [4800/5639(85%)]\tTrain Loss: 3.008251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-19 07:32:34,478\tINFO tune.py:630 -- Total run time: 387.04 seconds (386.87 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for tune_with_parameters_f6201_00009:\n",
            "  date: 2021-11-19_07-32-34\n",
            "  done: true\n",
            "  experiment_id: 5de7d7968e874b1eb01832bdc7c8ac0c\n",
            "  hostname: ecc602ac4891\n",
            "  iterations_since_restore: 20\n",
            "  loss: 2.247820578230188\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 3342\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 70.5871012210846\n",
            "  time_this_iter_s: 3.3550705909729004\n",
            "  time_total_s: 70.5871012210846\n",
            "  timestamp: 1637307154\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 20\n",
            "  trial_id: f6201_00009\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m [EPOCH: 19], \tVal Loss: 2.2478\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=3342)\u001b[0m \n",
            "== Status ==\n",
            "Current time: 2021-11-19 07:32:34 (running for 00:06:26.91)\n",
            "Memory usage on this node: 5.0/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=10\n",
            "Bracket: Iter 16.000: -2.5914811218883975 | Iter 8.000: -3.028511691262536 | Iter 4.000: -3.036302745088618 | Iter 2.000: -3.2002449783027593 | Iter 1.000: -3.231167747281122\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-19_07-26-07\n",
            "Number of trials: 10/10 (10 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_f6201_00000 | TERMINATED | 172.28.0.2:1570 |           16 |   64 |   32 | 0.0323551  | 3.11767 |                   20 |\n",
            "| tune_with_parameters_f6201_00001 | TERMINATED | 172.28.0.2:1569 |           64 |    8 |   16 | 0.0317033  | 4.07837 |                    1 |\n",
            "| tune_with_parameters_f6201_00002 | TERMINATED | 172.28.0.2:2068 |           16 |  128 |   64 | 0.00278453 | 2.33691 |                   20 |\n",
            "| tune_with_parameters_f6201_00003 | TERMINATED | 172.28.0.2:2514 |           32 |   32 |    8 | 0.00966311 | 3.0946  |                    8 |\n",
            "| tune_with_parameters_f6201_00004 | TERMINATED | 172.28.0.2:2710 |           64 |  128 |   64 | 0.00207041 | 3.7279  |                    1 |\n",
            "| tune_with_parameters_f6201_00005 | TERMINATED | 172.28.0.2:2762 |            8 |    8 |   64 | 0.00112382 | 2.83588 |                   16 |\n",
            "| tune_with_parameters_f6201_00006 | TERMINATED | 172.28.0.2:3126 |           64 |   32 |   16 | 0.00210753 | 3.44386 |                    1 |\n",
            "| tune_with_parameters_f6201_00007 | TERMINATED | 172.28.0.2:3178 |           16 |    8 |   32 | 0.0226108  | 3.34886 |                    1 |\n",
            "| tune_with_parameters_f6201_00008 | TERMINATED | 172.28.0.2:3230 |           32 |   32 |    8 | 0.00598872 | 3.12042 |                    4 |\n",
            "| tune_with_parameters_f6201_00009 | TERMINATED | 172.28.0.2:3342 |           16 |  128 |   16 | 0.00539106 | 2.24782 |                   20 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Best trial config: {'l1': 128, 'l2': 16, 'lr': 0.005391064065783462, 'batch_size': 16}\n",
            "Best trial final validation loss: 2.247820578230188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxuUXGdJq9ys"
      },
      "source": [
        "best_model = torch.load(\"/content/drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection/saved_model/{'l1': 128, 'l2': 16, 'lr': 0.005391064065783462, 'batch_size': 16}val_loss: 2.247820578230188\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcHo5oqA6h76"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ZuI84jyOhN"
      },
      "source": [
        "def predict(model, test_image, idx, plot=False):\n",
        "    image_pred = torch.from_numpy(test_image[idx]).type(torch.FloatTensor).to(DEVICE)\n",
        "    image_pred /= 255.\n",
        "    with torch.no_grad():\n",
        "        prediction = model(image_pred)\n",
        "    image_pred = image_pred.cpu().numpy()*255.\n",
        "    prediction = prediction.cpu().numpy().reshape(30)\n",
        "    # answer = val_data[idx][1]\n",
        "\n",
        "    if plot:\n",
        "        fig, axis = plt.subplots()\n",
        "        prediction_plot(image_pred, prediction, axis,\\\n",
        "                        'prediction plot for {}th image in val_data'.\n",
        "                        format(idx))\n",
        "\n",
        "\n",
        "def prediction_plot(image, keypoint, axis, title, answer=None):\n",
        "    image = image.reshape(96, 96)\n",
        "    axis.imshow(image, cmap='gray')\n",
        "    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n",
        "    # axis.scatter(answer[0::2], answer[1::2], marker='x', color='r', s=20)\n",
        "    plt.title(title)\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiMTAe58xOdI"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "DYEPmKqt3k68",
        "outputId": "010cfb11-503a-4154-f81d-0b8cc2c95881"
      },
      "source": [
        "predict(model=best_model, test_image=train_image, idx=1, plot=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dyy4vXy4HQQM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
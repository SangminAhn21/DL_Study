{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FKD_hyperparameter_tuning",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1t9qbOmt01EYfhWnjzeSzdXhBhQg83f8Q",
      "authorship_tag": "ABX9TyNwrK6/qXke/r3KtR756dzE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangminAhn21/DL_Study/blob/main/Kaggle/Facial_Keypoint_Detection/FKD_hyperparameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHnN-GMgMpTX",
        "outputId": "307c12a8-7e40-41b7-a14e-dd7914fb3dc4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JrCNHFvM75T",
        "outputId": "73f9a1de-2429-4109-c4fb-4b8df0c33c56"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLdIIW2lM_1g",
        "outputId": "7ffa927e-dbf0-4562-bbe1-44c27b4587c3"
      },
      "source": [
        "%cd drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Facial_Keypoint_Detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JWEf9yZUCxB",
        "outputId": "709b20ee-818f-4e87-ba71-7a7b821d6aa5"
      },
      "source": [
        "pip install kaggle"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "UNgjUTgpUFOa",
        "outputId": "048da0ae-33dd-48fc-aa95-3505c18d1bd2"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a94ea484-f00c-4cd5-a42b-d42610022d2a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a94ea484-f00c-4cd5-a42b-d42610022d2a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 66 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra7lY14CUFrO",
        "outputId": "1e7873fe-36ae-4628-8dda-e5631bd36b33"
      },
      "source": [
        "!kaggle competitions download -c facial-keypoints-detection"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "IdLookupTable.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "SampleSubmission.csv: Skipping, found more recently modified local copy (use --force to force download)\n",
            "training.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpi5Y_5Uhdg",
        "outputId": "28e90309-5dfb-4968-966c-cd7a0e9a7402"
      },
      "source": [
        "!unzip training.zip\n",
        "!unzip test.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  training.zip\n",
            "replace training.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "Archive:  test.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx54U6VcQoo2"
      },
      "source": [
        "import models, utils"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1_OqSV6RD3E"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "training_pd = pd.read_csv('training.csv')\n",
        "test_pd = pd.read_csv('test.csv')\n",
        "\n",
        "training_pd = training_pd.fillna(method='ffill')\n",
        "\n",
        "training = training_pd.to_numpy()\n",
        "test = test_pd.to_numpy()\n",
        "\n",
        "train_image = training[:, -1]\n",
        "train_key = training[:, :-1].astype('float64')\n",
        "test_image = test[:, 1]\n",
        "\n",
        "train_image = np.array([np.array([int(pixel) for pixel in image.split()]).\\\n",
        "                        reshape(96, 96) for image in train_image])\n",
        "test_image = np.array([np.array([int(pixel) for pixel in image.split()]).\\\n",
        "                       reshape(96, 96) for image in test_image])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcAFuW3Sqifu",
        "outputId": "cb1d1497-5c0a-4fe7-a419-892c5b2662eb"
      },
      "source": [
        "pip install ray"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.19.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray) (1.41.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.2.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: redis>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ray) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.3.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (2.6.0)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray) (1.2.13)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated->redis>=3.5.0->ray) (1.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWCw038zxCn_",
        "outputId": "7d1d4943-2ce3-488e-f39e-af00c6e4e7dc"
      },
      "source": [
        "pip install -U tensorboardx"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardx in /usr/local/lib/python3.7/dist-packages (2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardx) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardx) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOOM5Ccq9IsW"
      },
      "source": [
        "from utils import FaceDataset, RMSELoss\n",
        "from functools import partial\n",
        "from models import CNN\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from filelock import FileLock\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "\n",
        "def cnn_train(config, data, checkpoint_dir=None, data_dir=None):\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = torch.device('cuda')\n",
        "    else:\n",
        "        DEVICE = torch.device('cpu')\n",
        "    print('Using PyTorch version:', torch.__version__, ' Device: ', DEVICE)\n",
        "\n",
        "    model = CNN(config['l1'], config['l2']).to(DEVICE)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'])\n",
        "    criterion = RMSELoss()\n",
        "    print(model)\n",
        "\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        model.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    dataset = FaceDataset(data[0], data[1])\n",
        "\n",
        "    lengths = [int(len(dataset)*0.8), len(dataset) - int(len(dataset)*0.8)]\n",
        "    train_data, val_data = torch.utils.data.random_split(dataset, lengths)\n",
        "\n",
        "    train_loader = DataLoader(dataset=train_data,\n",
        "                          batch_size=config['batch_size'],\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "    val_loader = DataLoader(dataset=val_data,\n",
        "                        batch_size=config['batch_size'],\n",
        "                        shuffle=True,\n",
        "                        num_workers=2)\n",
        "\n",
        "    # model.train()\n",
        "    for Epoch in range(10):\n",
        "        # running_loss = 0.0\n",
        "        # epoch_steps = 0\n",
        "        for batch_idx, (image, key) in enumerate(train_loader):\n",
        "            image = image.to(DEVICE)\n",
        "            key = key.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(image)\n",
        "            loss = criterion(output, key)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # running_loss += loss.item()\n",
        "            # epoch_steps += 1\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(\"Train Epoch: {} [{}/{}({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
        "                    Epoch, batch_idx * len(image),\n",
        "                    len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
        "                    loss.item()))\n",
        "                # running_loss = 0.0\n",
        "\n",
        "        # model.eval()\n",
        "        val_loss = 0.0\n",
        "        for image, key in val_loader:\n",
        "            with torch.no_grad():\n",
        "                image = image.to(DEVICE)\n",
        "                key = key.to(DEVICE)\n",
        "                output = model(image)\n",
        "                val_loss += criterion(output, key).item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_loss *= config['batch_size']\n",
        "        print('\\n[EPOCH: {}], \\tVal Loss: {:.4f}\\n'.\n",
        "        format(Epoch, val_loss))\n",
        "\n",
        "        with tune.checkpoint_dir(Epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((model.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=val_loss)\n",
        "    print(\"Finished Training\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euECRmzGFaU7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf2bd761-8683-4d9b-be67-89db615dcb80"
      },
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    data_dir = os.path.abspath(\"./data\")  # 특정 경로에 대해 절대 경로 얻기\n",
        "    config = {\n",
        "        'l1': tune.sample_from(lambda _: 2**np.random.randint(3, 8)),\n",
        "        'l2': tune.sample_from(lambda _: 2**np.random.randint(3, 8)),\n",
        "        'lr': tune.loguniform(1e-3, 1e-1),\n",
        "        'batch_size': tune.choice([8, 16, 32, 64])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric='loss',\n",
        "        mode='min',\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    \n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"training_iteration\"])\n",
        "    \n",
        "    result = tune.run(\n",
        "        tune.with_parameters(partial(cnn_train, data_dir=data_dir), data=(train_image, train_key)),\n",
        "        resources_per_trial={'cpu': 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "\n",
        "    best_trained_model = CNN(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    if torch.cuda.is_available():\n",
        "        DEVICE = torch.device('cuda')\n",
        "    else:\n",
        "        DEVICE = torch.device('cpu')\n",
        "    best_trained_model.to(DEVICE)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=5, max_num_epochs=10, gpus_per_trial=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2021-11-18 10:44:47 (running for 00:00:00.28)\n",
            "Memory usage on this node: 4.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (5 PENDING)\n",
            "+----------------------------------+----------+-------+--------------+------+------+------------+\n",
            "| Trial name                       | status   | loc   |   batch_size |   l1 |   l2 |         lr |\n",
            "|----------------------------------+----------+-------+--------------+------+------+------------|\n",
            "| tune_with_parameters_8c63e_00000 | PENDING  |       |           16 |   64 |   32 | 0.0517826  |\n",
            "| tune_with_parameters_8c63e_00001 | PENDING  |       |            8 |  128 |   32 | 0.00157046 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING  |       |           64 |   64 |    8 | 0.00563198 |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING  |       |           16 |   32 |   64 | 0.0257101  |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING  |       |            8 |   16 |   32 | 0.00634202 |\n",
            "+----------------------------------+----------+-------+--------------+------+------+------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:44:52 (running for 00:00:05.36)\n",
            "Memory usage on this node: 5.3/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------|\n",
            "| tune_with_parameters_8c63e_00000 | RUNNING  | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  |\n",
            "| tune_with_parameters_8c63e_00001 | PENDING  |                 |            8 |  128 |   32 | 0.00157046 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING  |                 |           64 |   64 |    8 | 0.00563198 |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING  |                 |           16 |   32 |   64 | 0.0257101  |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING  |                 |            8 |   16 |   32 | 0.00634202 |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m   (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m   (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m   (fc3): Linear(in_features=32, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=8583)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=8583)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.543255\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 4.666120\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 3.797408\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 2.971472\n",
            "Result for tune_with_parameters_8c63e_00000:\n",
            "  date: 2021-11-18_10-44-56\n",
            "  done: false\n",
            "  experiment_id: 651f6b983d7c426192cc6e551635445a\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.5893895494176986\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8583\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 6.173115968704224\n",
            "  time_this_iter_s: 6.173115968704224\n",
            "  time_total_s: 6.173115968704224\n",
            "  timestamp: 1637232296\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 8c63e_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 0], \tVal Loss: 3.5894\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 3.530412\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 3.902304\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 5.609354\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 5.728297\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:44:58 (running for 00:00:11.31)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00000 | RUNNING  | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 3.58939 |                    1 |\n",
            "| tune_with_parameters_8c63e_00001 | PENDING  |                 |            8 |  128 |   32 | 0.00157046 |         |                      |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING  |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING  |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING  |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 1], \tVal Loss: 3.2247\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 3.398839\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 3.325742\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 2.375696\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 3.231542\n",
            "Result for tune_with_parameters_8c63e_00000:\n",
            "  date: 2021-11-18_10-45-02\n",
            "  done: false\n",
            "  experiment_id: 651f6b983d7c426192cc6e551635445a\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.3459084531094168\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8583\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 12.18898606300354\n",
            "  time_this_iter_s: 2.988485097885132\n",
            "  time_total_s: 12.18898606300354\n",
            "  timestamp: 1637232302\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 8c63e_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 2], \tVal Loss: 3.3459\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 3.283486\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 2.744297\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 4.994035\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:04 (running for 00:00:17.07)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -3.2247190624264115 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00000 | RUNNING  | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 3.34591 |                    3 |\n",
            "| tune_with_parameters_8c63e_00001 | PENDING  |                 |            8 |  128 |   32 | 0.00157046 |         |                      |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING  |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING  |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING  |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 5.436459\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 3], \tVal Loss: 3.2250\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 5.306270\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 4 [1600/5639(28%)]\tTrain Loss: 2.973304\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 4 [3200/5639(57%)]\tTrain Loss: 3.662938\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 4 [4800/5639(85%)]\tTrain Loss: 3.051292\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 4], \tVal Loss: 3.0570\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "Result for tune_with_parameters_8c63e_00000:\n",
            "  date: 2021-11-18_10-45-08\n",
            "  done: false\n",
            "  experiment_id: 651f6b983d7c426192cc6e551635445a\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 5\n",
            "  loss: 3.05699411486903\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8583\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 18.133393049240112\n",
            "  time_this_iter_s: 2.964707851409912\n",
            "  time_total_s: 18.133393049240112\n",
            "  timestamp: 1637232308\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: 8c63e_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 3.227154\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 5 [1600/5639(28%)]\tTrain Loss: 4.956942\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 5 [3200/5639(57%)]\tTrain Loss: 2.613861\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:10 (running for 00:00:23.03)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.2247190624264115 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00000 | RUNNING  | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 3.05699 |                    5 |\n",
            "| tune_with_parameters_8c63e_00001 | PENDING  |                 |            8 |  128 |   32 | 0.00157046 |         |                      |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING  |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING  |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING  |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 5 [4800/5639(85%)]\tTrain Loss: 3.047682\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 5], \tVal Loss: 3.0908\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 2.872693\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 6 [1600/5639(28%)]\tTrain Loss: 3.102272\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 6 [3200/5639(57%)]\tTrain Loss: 3.051126\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 6 [4800/5639(85%)]\tTrain Loss: 4.603146\n",
            "Result for tune_with_parameters_8c63e_00000:\n",
            "  date: 2021-11-18_10-45-14\n",
            "  done: false\n",
            "  experiment_id: 651f6b983d7c426192cc6e551635445a\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 7\n",
            "  loss: 3.3455096522121566\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8583\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 24.06761932373047\n",
            "  time_this_iter_s: 2.9797208309173584\n",
            "  time_total_s: 24.06761932373047\n",
            "  timestamp: 1637232314\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: 8c63e_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 6], \tVal Loss: 3.3455\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 3.748199\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 7 [1600/5639(28%)]\tTrain Loss: 3.405211\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 7 [3200/5639(57%)]\tTrain Loss: 4.771221\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:16 (running for 00:00:28.95)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.2247190624264115 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00000 | RUNNING  | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 3.34551 |                    7 |\n",
            "| tune_with_parameters_8c63e_00001 | PENDING  |                 |            8 |  128 |   32 | 0.00157046 |         |                      |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING  |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING  |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING  |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 7 [4800/5639(85%)]\tTrain Loss: 2.620931\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 7], \tVal Loss: 3.1275\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 8 [0/5639(0%)]\tTrain Loss: 3.111007\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 8 [1600/5639(28%)]\tTrain Loss: 3.152706\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 8 [3200/5639(57%)]\tTrain Loss: 2.949090\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 8 [4800/5639(85%)]\tTrain Loss: 3.206834\n",
            "Result for tune_with_parameters_8c63e_00000:\n",
            "  date: 2021-11-18_10-45-20\n",
            "  done: false\n",
            "  experiment_id: 651f6b983d7c426192cc6e551635445a\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 9\n",
            "  loss: 4.197917816486765\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8583\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 30.025683403015137\n",
            "  time_this_iter_s: 2.995318651199341\n",
            "  time_total_s: 30.025683403015137\n",
            "  timestamp: 1637232320\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: 8c63e_00000\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 8], \tVal Loss: 4.1979\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 9 [0/5639(0%)]\tTrain Loss: 4.152231\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 9 [1600/5639(28%)]\tTrain Loss: 2.866450\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 9 [3200/5639(57%)]\tTrain Loss: 2.971317\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:22 (running for 00:00:34.92)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.2247190624264115 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00000 | RUNNING  | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.19792 |                    9 |\n",
            "| tune_with_parameters_8c63e_00001 | PENDING  |                 |            8 |  128 |   32 | 0.00157046 |         |                      |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING  |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING  |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING  |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "+----------------------------------+----------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m Train Epoch: 9 [4800/5639(85%)]\tTrain Loss: 5.249395\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m [EPOCH: 9], \tVal Loss: 4.3876\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8583)\u001b[0m \n",
            "Result for tune_with_parameters_8c63e_00000:\n",
            "  date: 2021-11-18_10-45-22\n",
            "  done: true\n",
            "  experiment_id: 651f6b983d7c426192cc6e551635445a\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 10\n",
            "  loss: 4.387597234198387\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8583\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 33.005094051361084\n",
            "  time_this_iter_s: 2.9794106483459473\n",
            "  time_total_s: 33.005094051361084\n",
            "  timestamp: 1637232322\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 10\n",
            "  trial_id: 8c63e_00000\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:28 (running for 00:00:40.92)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.2247190624264115 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |   loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+--------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 |        |                      |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |        |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |        |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |        |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876 |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m   (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m   (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m   (fc3): Linear(in_features=32, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=8822)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=8822)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 52.024601\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [800/5639(14%)]\tTrain Loss: 3.074728\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 3.071284\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [2400/5639(43%)]\tTrain Loss: 4.458999\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 2.783240\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [4000/5639(71%)]\tTrain Loss: 3.383051\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 2.657067\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 0 [5600/5639(99%)]\tTrain Loss: 2.570134\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:33 (running for 00:00:45.96)\n",
            "Memory usage on this node: 5.6/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.2247190624264115 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |   loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+--------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 |        |                      |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |        |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |        |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |        |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876 |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+--------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 0], \tVal Loss: 3.0351\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-45-33\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.0350876943439458\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 8.290400505065918\n",
            "  time_this_iter_s: 8.290400505065918\n",
            "  time_total_s: 8.290400505065918\n",
            "  timestamp: 1637232333\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 3.236718\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [800/5639(14%)]\tTrain Loss: 2.049862\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 3.122256\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [2400/5639(43%)]\tTrain Loss: 2.827008\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 2.366056\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [4000/5639(71%)]\tTrain Loss: 2.521281\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 2.919267\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 1 [5600/5639(99%)]\tTrain Loss: 2.622761\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:38 (running for 00:00:51.71)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.2247190624264115 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 3.03509 |                    1 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-45-39\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 2\n",
            "  loss: 2.9238791445468335\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 13.539680242538452\n",
            "  time_this_iter_s: 5.249279737472534\n",
            "  time_total_s: 13.539680242538452\n",
            "  timestamp: 1637232339\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 1], \tVal Loss: 2.9239\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 3.311847\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [800/5639(14%)]\tTrain Loss: 3.116393\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 2.224157\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [2400/5639(43%)]\tTrain Loss: 2.377794\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 1.905315\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [4000/5639(71%)]\tTrain Loss: 2.459441\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 2.673980\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 2 [5600/5639(99%)]\tTrain Loss: 2.844262\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:44 (running for 00:00:56.98)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.92388 |                    2 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-45-44\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 3\n",
            "  loss: 2.9271139280170413\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 18.763376712799072\n",
            "  time_this_iter_s: 5.22369647026062\n",
            "  time_total_s: 18.763376712799072\n",
            "  timestamp: 1637232344\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 2], \tVal Loss: 2.9271\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 3.635453\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [800/5639(14%)]\tTrain Loss: 3.065648\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 3.102770\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [2400/5639(43%)]\tTrain Loss: 2.247204\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 2.836141\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [4000/5639(71%)]\tTrain Loss: 2.779894\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 2.363647\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 3 [5600/5639(99%)]\tTrain Loss: 3.377240\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:49 (running for 00:01:02.19)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -3.2250398730555325 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.92711 |                    3 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-45-49\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 4\n",
            "  loss: 2.643945374725558\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 24.09924054145813\n",
            "  time_this_iter_s: 5.335863828659058\n",
            "  time_total_s: 24.09924054145813\n",
            "  timestamp: 1637232349\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 3], \tVal Loss: 2.6439\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [0/5639(0%)]\tTrain Loss: 1.913306\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [800/5639(14%)]\tTrain Loss: 2.207934\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [1600/5639(28%)]\tTrain Loss: 1.781929\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [2400/5639(43%)]\tTrain Loss: 3.310374\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [3200/5639(57%)]\tTrain Loss: 2.434192\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [4000/5639(71%)]\tTrain Loss: 2.463402\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [4800/5639(85%)]\tTrain Loss: 2.787265\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 4 [5600/5639(99%)]\tTrain Loss: 2.490839\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:54 (running for 00:01:07.57)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.64395 |                    4 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 4], \tVal Loss: 2.6655\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-45-54\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 5\n",
            "  loss: 2.6655355216763543\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 29.37484359741211\n",
            "  time_this_iter_s: 5.2756030559539795\n",
            "  time_total_s: 29.37484359741211\n",
            "  timestamp: 1637232354\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 5\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [0/5639(0%)]\tTrain Loss: 2.240402\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [800/5639(14%)]\tTrain Loss: 2.553808\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [1600/5639(28%)]\tTrain Loss: 2.243748\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [2400/5639(43%)]\tTrain Loss: 3.982366\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [3200/5639(57%)]\tTrain Loss: 3.107385\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [4000/5639(71%)]\tTrain Loss: 1.916415\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [4800/5639(85%)]\tTrain Loss: 5.095100\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 5 [5600/5639(99%)]\tTrain Loss: 2.718857\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:45:59 (running for 00:01:12.80)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.66554 |                    5 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 5], \tVal Loss: 2.7063\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-46-00\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 6\n",
            "  loss: 2.706279521969193\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 34.663480281829834\n",
            "  time_this_iter_s: 5.288636684417725\n",
            "  time_total_s: 34.663480281829834\n",
            "  timestamp: 1637232360\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 6\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [0/5639(0%)]\tTrain Loss: 2.677395\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [800/5639(14%)]\tTrain Loss: 1.867791\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [1600/5639(28%)]\tTrain Loss: 2.615226\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [2400/5639(43%)]\tTrain Loss: 1.929766\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [3200/5639(57%)]\tTrain Loss: 1.901579\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [4000/5639(71%)]\tTrain Loss: 2.004703\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [4800/5639(85%)]\tTrain Loss: 2.831280\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 6 [5600/5639(99%)]\tTrain Loss: 1.870878\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:05 (running for 00:01:18.11)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.70628 |                    6 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-46-05\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 7\n",
            "  loss: 2.542959196347717\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 39.89696025848389\n",
            "  time_this_iter_s: 5.233479976654053\n",
            "  time_total_s: 39.89696025848389\n",
            "  timestamp: 1637232365\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 7\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 6], \tVal Loss: 2.5430\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [0/5639(0%)]\tTrain Loss: 1.575720\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [800/5639(14%)]\tTrain Loss: 1.712816\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [1600/5639(28%)]\tTrain Loss: 1.724697\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [2400/5639(43%)]\tTrain Loss: 2.243823\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [3200/5639(57%)]\tTrain Loss: 1.533126\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [4000/5639(71%)]\tTrain Loss: 2.571437\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [4800/5639(85%)]\tTrain Loss: 2.522292\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 7 [5600/5639(99%)]\tTrain Loss: 2.313634\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:10 (running for 00:01:23.33)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -3.1274511107316254 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.54296 |                    7 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-46-10\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 8\n",
            "  loss: 2.5218659225085105\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 45.235289335250854\n",
            "  time_this_iter_s: 5.338329076766968\n",
            "  time_total_s: 45.235289335250854\n",
            "  timestamp: 1637232370\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 8\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 7], \tVal Loss: 2.5219\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [0/5639(0%)]\tTrain Loss: 2.248290\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [800/5639(14%)]\tTrain Loss: 3.014213\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [1600/5639(28%)]\tTrain Loss: 2.331790\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [2400/5639(43%)]\tTrain Loss: 2.277493\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [3200/5639(57%)]\tTrain Loss: 2.205276\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [4000/5639(71%)]\tTrain Loss: 2.458809\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [4800/5639(85%)]\tTrain Loss: 2.377023\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 8 [5600/5639(99%)]\tTrain Loss: 1.998610\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:15 (running for 00:01:28.69)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.52187 |                    8 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-46-16\n",
            "  done: false\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 9\n",
            "  loss: 2.422070192107072\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 50.57227921485901\n",
            "  time_this_iter_s: 5.336989879608154\n",
            "  time_total_s: 50.57227921485901\n",
            "  timestamp: 1637232376\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 9\n",
            "  trial_id: 8c63e_00001\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 8], \tVal Loss: 2.4221\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [0/5639(0%)]\tTrain Loss: 2.040066\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [800/5639(14%)]\tTrain Loss: 2.141094\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [1600/5639(28%)]\tTrain Loss: 2.515424\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [2400/5639(43%)]\tTrain Loss: 2.209021\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [3200/5639(57%)]\tTrain Loss: 1.504228\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [4000/5639(71%)]\tTrain Loss: 1.590792\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [4800/5639(85%)]\tTrain Loss: 2.651287\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m Train Epoch: 9 [5600/5639(99%)]\tTrain Loss: 1.868094\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:21 (running for 00:01:34.00)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (3 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00001 | RUNNING    | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.42207 |                    9 |\n",
            "| tune_with_parameters_8c63e_00002 | PENDING    |                 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m [EPOCH: 9], \tVal Loss: 2.3605\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=8822)\u001b[0m \n",
            "Result for tune_with_parameters_8c63e_00001:\n",
            "  date: 2021-11-18_10-46-21\n",
            "  done: true\n",
            "  experiment_id: c891530e23f441db8a915336f464e220\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 10\n",
            "  loss: 2.3604819196335813\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 8822\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 55.88161301612854\n",
            "  time_this_iter_s: 5.309333801269531\n",
            "  time_total_s: 55.88161301612854\n",
            "  timestamp: 1637232381\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 10\n",
            "  trial_id: 8c63e_00001\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[1m\u001b[36m(scheduler +9m6s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
            "\u001b[2m\u001b[1m\u001b[33m(scheduler +9m6s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'GPU': 1.0, 'CPU': 2.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:26 (running for 00:01:39.34)\n",
            "Memory usage on this node: 5.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.312238621880822\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (2 PENDING, 1 RUNNING, 2 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00002 | RUNNING    | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 |         |                      |\n",
            "| tune_with_parameters_8c63e_00003 | PENDING    |                 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m   (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m   (fc2): Linear(in_features=64, out_features=8, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m   (fc3): Linear(in_features=8, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9066)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9066)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.946308\n",
            "Result for tune_with_parameters_8c63e_00002:\n",
            "  date: 2021-11-18_10-46-28\n",
            "  done: true\n",
            "  experiment_id: 6590e89c0c924260a2d3e1c8c2b65687\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 1\n",
            "  loss: 4.38415901779283\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9066\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 4.256778240203857\n",
            "  time_this_iter_s: 4.256778240203857\n",
            "  time_total_s: 4.256778240203857\n",
            "  timestamp: 1637232388\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 8c63e_00002\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m [EPOCH: 0], \tVal Loss: 4.3842\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9066)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:32 (running for 00:01:45.11)\n",
            "Memory usage on this node: 5.1/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (1 PENDING, 1 RUNNING, 3 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00003 | RUNNING    | 172.28.0.2:9118 |           16 |   32 |   64 | 0.0257101  |         |                      |\n",
            "| tune_with_parameters_8c63e_00004 | PENDING    |                 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "| tune_with_parameters_8c63e_00002 | TERMINATED | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 | 4.38416 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m   (fc1): Linear(in_features=4608, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m   (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m   (fc3): Linear(in_features=64, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9118)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9118)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 51.915077\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 2.770913\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 2.837118\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 3.083371\n",
            "Result for tune_with_parameters_8c63e_00003:\n",
            "  date: 2021-11-18_10-46-36\n",
            "  done: true\n",
            "  experiment_id: f21a0934f73742b1af626fb9d91a2fca\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 1\n",
            "  loss: 5.121710481034948\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9118\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 5.925897836685181\n",
            "  time_this_iter_s: 5.925897836685181\n",
            "  time_total_s: 5.925897836685181\n",
            "  timestamp: 1637232396\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 8c63e_00003\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m [EPOCH: 0], \tVal Loss: 5.1217\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9118)\u001b[0m \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[33m(raylet)\u001b[0m /usr/local/lib/python3.7/dist-packages/redis/connection.py:72: UserWarning: redis-py works best with hiredis. Please consider installing\n",
            "\u001b[2m\u001b[33m(raylet)\u001b[0m   warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:37 (running for 00:01:50.53)\n",
            "Memory usage on this node: 4.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.986774283605264\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00004 | RUNNING    | 172.28.0.2:9168 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "| tune_with_parameters_8c63e_00002 | TERMINATED | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 | 4.38416 |                    1 |\n",
            "| tune_with_parameters_8c63e_00003 | TERMINATED | 172.28.0.2:9118 |           16 |   32 |   64 | 0.0257101  | 5.12171 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Using PyTorch version: 1.10.0+cu111  Device:  cuda\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m CNN(\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m   (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m   (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m   (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m   (fc1): Linear(in_features=4608, out_features=16, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m   (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m   (fc3): Linear(in_features=32, out_features=30, bias=True)\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=9168)\u001b[0m /content/drive/My Drive/Colab Notebooks/Facial_Keypoint_Detection/utils.py:11: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n",
            "\u001b[2m\u001b[36m(pid=9168)\u001b[0m   self.x_data = (torch.from_numpy(x)/255.).type('torch.FloatTensor')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [0/5639(0%)]\tTrain Loss: 52.201576\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [800/5639(14%)]\tTrain Loss: 3.379310\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:43 (running for 00:01:56.02)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.986774283605264\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00004 | RUNNING    | 172.28.0.2:9168 |            8 |   16 |   32 | 0.00634202 |         |                      |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "| tune_with_parameters_8c63e_00002 | TERMINATED | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 | 4.38416 |                    1 |\n",
            "| tune_with_parameters_8c63e_00003 | TERMINATED | 172.28.0.2:9118 |           16 |   32 |   64 | 0.0257101  | 5.12171 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [1600/5639(28%)]\tTrain Loss: 3.213176\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [2400/5639(43%)]\tTrain Loss: 2.806046\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [3200/5639(57%)]\tTrain Loss: 3.244871\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [4000/5639(71%)]\tTrain Loss: 2.607712\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [4800/5639(85%)]\tTrain Loss: 2.830641\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 0 [5600/5639(99%)]\tTrain Loss: 3.203065\n",
            "Result for tune_with_parameters_8c63e_00004:\n",
            "  date: 2021-11-18_10-46-47\n",
            "  done: false\n",
            "  experiment_id: 27de2ccae2924eb7a4482cddefa6a75d\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 1\n",
            "  loss: 3.3134926478068034\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9168\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 8.275307893753052\n",
            "  time_this_iter_s: 8.275307893753052\n",
            "  time_total_s: 8.275307893753052\n",
            "  timestamp: 1637232407\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: 8c63e_00004\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m [EPOCH: 0], \tVal Loss: 3.3135\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [0/5639(0%)]\tTrain Loss: 2.663979\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [800/5639(14%)]\tTrain Loss: 2.928774\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:48 (running for 00:02:01.31)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0742991034866227 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00004 | RUNNING    | 172.28.0.2:9168 |            8 |   16 |   32 | 0.00634202 | 3.31349 |                    1 |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "| tune_with_parameters_8c63e_00002 | TERMINATED | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 | 4.38416 |                    1 |\n",
            "| tune_with_parameters_8c63e_00003 | TERMINATED | 172.28.0.2:9118 |           16 |   32 |   64 | 0.0257101  | 5.12171 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [1600/5639(28%)]\tTrain Loss: 3.407837\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [2400/5639(43%)]\tTrain Loss: 2.651776\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [3200/5639(57%)]\tTrain Loss: 2.650678\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [4000/5639(71%)]\tTrain Loss: 2.826862\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [4800/5639(85%)]\tTrain Loss: 2.712023\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 1 [5600/5639(99%)]\tTrain Loss: 3.611159\n",
            "Result for tune_with_parameters_8c63e_00004:\n",
            "  date: 2021-11-18_10-46-52\n",
            "  done: false\n",
            "  experiment_id: 27de2ccae2924eb7a4482cddefa6a75d\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 2\n",
            "  loss: 3.0665208978855865\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9168\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 13.634115219116211\n",
            "  time_this_iter_s: 5.358807325363159\n",
            "  time_total_s: 13.634115219116211\n",
            "  timestamp: 1637232412\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: 8c63e_00004\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m [EPOCH: 1], \tVal Loss: 3.0665\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [0/5639(0%)]\tTrain Loss: 3.915492\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [800/5639(14%)]\tTrain Loss: 2.617151\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:53 (running for 00:02:06.66)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0665208978855865 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00004 | RUNNING    | 172.28.0.2:9168 |            8 |   16 |   32 | 0.00634202 | 3.06652 |                    2 |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "| tune_with_parameters_8c63e_00002 | TERMINATED | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 | 4.38416 |                    1 |\n",
            "| tune_with_parameters_8c63e_00003 | TERMINATED | 172.28.0.2:9118 |           16 |   32 |   64 | 0.0257101  | 5.12171 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [1600/5639(28%)]\tTrain Loss: 2.787859\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [2400/5639(43%)]\tTrain Loss: 5.662400\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [3200/5639(57%)]\tTrain Loss: 2.767082\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [4000/5639(71%)]\tTrain Loss: 2.798296\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [4800/5639(85%)]\tTrain Loss: 2.801558\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 2 [5600/5639(99%)]\tTrain Loss: 3.053669\n",
            "Result for tune_with_parameters_8c63e_00004:\n",
            "  date: 2021-11-18_10-46-58\n",
            "  done: false\n",
            "  experiment_id: 27de2ccae2924eb7a4482cddefa6a75d\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 3\n",
            "  loss: 3.4980359977018747\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9168\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 18.93893575668335\n",
            "  time_this_iter_s: 5.304820537567139\n",
            "  time_total_s: 18.93893575668335\n",
            "  timestamp: 1637232418\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: 8c63e_00004\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m [EPOCH: 2], \tVal Loss: 3.4980\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [0/5639(0%)]\tTrain Loss: 4.183034\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [800/5639(14%)]\tTrain Loss: 2.918143\n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:46:59 (running for 00:02:11.97)\n",
            "Memory usage on this node: 5.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=4\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -2.934492623890545 | Iter 2.000: -3.0665208978855865 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (1 RUNNING, 4 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00004 | RUNNING    | 172.28.0.2:9168 |            8 |   16 |   32 | 0.00634202 | 3.49804 |                    3 |\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "| tune_with_parameters_8c63e_00002 | TERMINATED | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 | 4.38416 |                    1 |\n",
            "| tune_with_parameters_8c63e_00003 | TERMINATED | 172.28.0.2:9118 |           16 |   32 |   64 | 0.0257101  | 5.12171 |                    1 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [1600/5639(28%)]\tTrain Loss: 2.310168\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [2400/5639(43%)]\tTrain Loss: 3.474170\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [3200/5639(57%)]\tTrain Loss: 3.793836\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [4000/5639(71%)]\tTrain Loss: 2.667040\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [4800/5639(85%)]\tTrain Loss: 2.612080\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m Train Epoch: 3 [5600/5639(99%)]\tTrain Loss: 3.685275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-11-18 10:47:03,606\tINFO tune.py:630 -- Total run time: 136.50 seconds (136.34 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for tune_with_parameters_8c63e_00004:\n",
            "  date: 2021-11-18_10-47-03\n",
            "  done: true\n",
            "  experiment_id: 27de2ccae2924eb7a4482cddefa6a75d\n",
            "  hostname: f3af5fe45eb5\n",
            "  iterations_since_restore: 4\n",
            "  loss: 3.0232863054207875\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 9168\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 24.310259103775024\n",
            "  time_this_iter_s: 5.371323347091675\n",
            "  time_total_s: 24.310259103775024\n",
            "  timestamp: 1637232423\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 4\n",
            "  trial_id: 8c63e_00004\n",
            "  \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m [EPOCH: 3], \tVal Loss: 3.0233\n",
            "\u001b[2m\u001b[36m(ImplicitFunc pid=9168)\u001b[0m \n",
            "== Status ==\n",
            "Current time: 2021-11-18 10:47:03 (running for 00:02:16.36)\n",
            "Memory usage on this node: 5.5/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=5\n",
            "Bracket: Iter 8.000: -2.824658516620068 | Iter 4.000: -3.0232863054207875 | Iter 2.000: -3.0665208978855865 | Iter 1.000: -3.5893895494176986\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/6.14 GiB heap, 0.0/3.07 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/tune_with_parameters_2021-11-18_10-44-47\n",
            "Number of trials: 5/5 (5 TERMINATED)\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "| Trial name                       | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   training_iteration |\n",
            "|----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------|\n",
            "| tune_with_parameters_8c63e_00000 | TERMINATED | 172.28.0.2:8583 |           16 |   64 |   32 | 0.0517826  | 4.3876  |                   10 |\n",
            "| tune_with_parameters_8c63e_00001 | TERMINATED | 172.28.0.2:8822 |            8 |  128 |   32 | 0.00157046 | 2.36048 |                   10 |\n",
            "| tune_with_parameters_8c63e_00002 | TERMINATED | 172.28.0.2:9066 |           64 |   64 |    8 | 0.00563198 | 4.38416 |                    1 |\n",
            "| tune_with_parameters_8c63e_00003 | TERMINATED | 172.28.0.2:9118 |           16 |   32 |   64 | 0.0257101  | 5.12171 |                    1 |\n",
            "| tune_with_parameters_8c63e_00004 | TERMINATED | 172.28.0.2:9168 |            8 |   16 |   32 | 0.00634202 | 3.02329 |                    4 |\n",
            "+----------------------------------+------------+-----------------+--------------+------+------+------------+---------+----------------------+\n",
            "\n",
            "\n",
            "Best trial config: {'l1': 128, 'l2': 32, 'lr': 0.0015704600243220549, 'batch_size': 8}\n",
            "Best trial final validation loss: 2.3604819196335813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxuUXGdJq9ys"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}